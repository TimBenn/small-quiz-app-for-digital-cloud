[
    "{\"question\":\"A Developer will be launching several Docker containers on a new Amazon ECS cluster using the EC2 Launch Type. The containers will all run a web service on port 80. What is the EASIEST way the Developer can configure the task definition to ensure the web services run correctly and there are no port conflicts on the host instances?\",\"choices\":[\"Specify port 80 for the container port and a unique port number for the host port\",\"Specify a unique port number for the container port and port 80 for the host port\",\"Specify port 80 for the container port and port 0 for the host port\",\"Leave both the container port and host port configuration blank\"],\"answer\":\"Specify port 80 for the container port and port 0 for the host port\",\"reason\":\"Explanation: Port mappings allow containers to access ports on the host container instance to send or receive traffic. Port mappings are specified as part of the container definition. The container port is the port number on the container that is bound to the user-specified or automatically assigned host port. The host port is the port number on the container instance to reserve for your container. As we cannot have multiple services bound to the same host port, we need to ensure that each container port mapping uses a different host port. The easiest way to do this is to set the host port number to 0 and ECS will automatically assign an available port. We also need to assign port 80 to the container port so that the web service is able to run. \\n CORRECT : \\\"Specify port 80 for the container port and port 0 for the host port\\\" is the correct answer. \\n INCORRECT : \\\"Specify port 80 for the container port and a unique port number for the host port\\\" is incorrect as this is more difficult to manage as you have to manually assign the port number. \\n INCORRECT : \\\"Specify a unique port number for the container port and port 80 for the host port\\\" is incorrect as the web service on the container needs to run on pot 80 and you can only bind one container to port 80 on the host so this would not allow more than one container to work. \\n INCORRECT : \\\"Leave both the container port and host port configuration blank\\\" is incorrect as this would mean that ECS would dynamically assign both the container and host port. As the web service must run on port 80 this would not work correctly. References: https://docs.aws.amazon.com/AmazonECS/latest/APIReference/API_PortMapping.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-ecs-and-eks/\"}",
    "{\"question\":\"A Developer is deploying an AWS Lambda update using AWS CodeDeploy. In the appspec. yaml file, which of the following is a valid structure for the order of hooks that should be specified?\",\"choices\":[\"BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic\",\"BeforeInstall > AfterInstall > ApplicationStart > ValidateService\",\"BeforeAllowTraffic > AfterAllowTraffic\",\"BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic\"],\"answer\":\"BeforeAllowTraffic > AfterAllowTraffic\",\"reason\":\"Explanation: The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment. The following code snippet shows a valid example of the structure of hooks for an AWS Lambda deployment:Therefore, in this scenario a valid structure for the order of hooks that should be specified in the appspec. yml file is: BeforeAllowTraffic > AfterAllowTrafficCORRECT: \\\"BeforeAllowTraffic > AfterAllowTraffic\\\" is the correct answer. \\n INCORRECT : \\\"BeforeInstall > AfterInstall > ApplicationStart > ValidateService\\\" is incorrect as this would be valid for Amazon EC2. \\n INCORRECT : \\\"BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic\\\" is incorrect as this would be valid for Amazon ECS. \\n INCORRECT : \\\"BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic\\\" is incorrect as this is a partial listing of hooks for Amazon EC2 but is incomplete. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-developer-tools/\"}",
    "{\"question\":\"A serverless application uses an AWS Lambda function to process Amazon S3 events. The Lambda function executes 20 times per second and takes 20 seconds to complete each execution. How many concurrent executions will the Lambda function require?\",\"choices\":[\"5\",\"400\",\"40\",\"20\"],\"answer\":\"400\",\"reason\":\"Explanation: Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency. To calculate the concurrency requirements for the Lambda function simply multiply the number of executions per second (20) by the time it takes to complete the execution (20). Therefore, for this scenario the calculation is 20 x 20 = 400. \\n CORRECT : \\\"400\\\" is the correct answer. \\n INCORRECT : \\\"5\\\" is incorrect. Please use the formula above to calculate concurrency requirements. \\n INCORRECT : \\\"40\\\" is incorrect. Please use the formula above to calculate concurrency requirements. \\n INCORRECT : \\\"20\\\" is incorrect. Please use the formula above to calculate concurrency requirements. References: https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-lambda/\"}",
    "{\"question\":\"A Developer is setting up a code update to Amazon ECS using AWS CodeDeploy. The Developer needs to complete the code update quickly. Which of the following deployment types should the Developer use?\",\"choices\":[\"In-place\",\"Canary\",\"Blue/green\",\"Linear\"],\"answer\":\"Blue/green\",\"reason\":\"Explanation: CodeDeploy provides two deployment type options – in-place and blue/green. Note that AWS Lambda and Amazon ECS deployments cannot use an in-place deployment type. The Blue/green deployment type on an Amazon ECS compute platform works like this:Traffic is shifted from the task set with the original version of an application in an Amazon ECS service to a replacement task set in the same service. You can set the traffic shifting to linear or canary through the deployment configuration. The protocol and port of a specified load balancer listener is used to reroute production traffic. During a deployment, a test listener can be used to serve traffic to the replacement task set while validation tests are run. \\n CORRECT : \\\"Blue/green\\\" is the correct answer. \\n INCORRECT : \\\"Canary\\\" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments. \\n INCORRECT : \\\"Linear\\\" is incorrect as this is a traffic shifting option, not a deployment type. Traffic is shifted in two increments. \\n INCORRECT : \\\"In-place\\\" is incorrect as AWS Lambda and Amazon ECS deployments cannot use an in-place deployment type. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/deployments.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-developer-tools/\"}",
    "{\"question\":\"An application on-premises uses Linux servers and a relational database using PostgreSQL. The company will be migrating the application to AWS and require a managed service that will take care of capacity provisioning, load balancing, and auto-scaling. Which combination of services should the Developer use? (Select TWO. )\",\"choices\":[\"AWS Elastic Beanstalk\",\"Amazon EC2 with Auto Scaling\",\"Amazon EC2 with PostgreSQL\",\"Amazon RDS with PostrgreSQL\",\"AWS Lambda with CloudWatch Events\"],\"answer\":[\"AWS Elastic Beanstalk\",\"Amazon RDS with PostrgreSQL\"],\"reason\":\"Explanation: The company require a managed service therefore the Developer should choose to use Elastic Beanstalk for the compute layer and Amazon RDS with the PostgreSQL engine for the database layer. AWS Elastic Beanstalk will handle all capacity provisioning, load balancing, and auto-scaling for the web front-end and Amazon RDS provides push-button scaling for the backend. \\n CORRECT : \\\"AWS Elastic Beanstalk\\\" is a correct answer. \\n CORRECT : \\\"Amazon RDS with PostrgreSQL\\\" is also a correct answer. \\n INCORRECT : \\\"Amazon EC2 with Auto Scaling\\\" is incorrect as though these services will be used to provide the automatic scalability required for the solution, they still need to be managed. The questions asks for a managed solution and Elastic Beanstalk will manage this for you. Also, there is no mention of a load balancer so connections cannot be distributed to instances. \\n INCORRECT : \\\"Amazon EC2 with PostgreSQL\\\" is incorrect as the question asks for a managed service and therefore the database should be run on Amazon RDS. \\n INCORRECT : \\\"AWS Lambda with CloudWatch Events\\\" is incorrect as there is no mention of refactoring application code to run on AWS Lambda. References: https://aws. amazon. com/elasticbeanstalk/https://aws. amazon. com/rds/postgresql/Save time with our AWS cheat sheets:https://digitalcloud.training/aws-elastic-beanstalk/ https://digitalcloud.training/amazon-rds/\"}",
    "{\"question\":\"A company runs many microservices applications that use Docker containers. The company are planning to migrate the containers to Amazon ECS. The workloads are highly variable and therefore the company prefers to be charged per running task. Which solution is the BEST fit for the company’s requirements?\",\"choices\":[\"Amazon ECS with the EC2 launch type\",\"Amazon ECS with the Fargate launch type\",\"An Amazon ECS Service with Auto Scaling\",\"An Amazon ECS Cluster with Auto Scaling\"],\"answer\":\"Amazon ECS with the Fargate launch type\",\"reason\":\"Explanation: The key requirement is that the company should be charged per running task. Therefore, the best answer is to use Amazon ECS with the Fargate launch type as with this model AWS charge you for running tasks rather than running container instances. The Fargate launch type allows you to run your containerized applications without the need to provision and manage the backend infrastructure. You just register your task definition and Fargate launches the container for you. The Fargate Launch Type is a serverless infrastructure managed by AWS. \\n CORRECT : \\\"Amazon ECS with the Fargate launch type\\\" is the correct answer. \\n INCORRECT : \\\"Amazon ECS with the EC2 launch type\\\" is incorrect as with this launch type you pay for running container instances (EC2 instances). \\n INCORRECT : \\\"An Amazon ECS Service with Auto Scaling\\\" is incorrect as this does not specify the launch type. You can run an ECS Service on the Fargate or EC2 launch types. \\n INCORRECT : \\\"An Amazon ECS Cluster with Auto Scaling\\\" is incorrect as this does not specify the launch type. You can run an ECS Cluster on the Fargate or EC2 launch types. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/AWS_Fargate.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-ecs-and-eks/\"}",
    "{\"question\":\"A Developer is creating a new web application that will be deployed using AWS Elastic Beanstalk from the AWS Management Console. The Developer is about to create a source bundle which will be uploaded using the console. Which of the following are valid requirements for creating the source bundle? (Select TWO. )\",\"choices\":[\"Must include the cron. yaml file. \",\"Must include a parent folder or top-level directory. \",\"Must not include a parent folder or top-level directory. \",\"Must not exceed 500 MB. \",\"Must consist of one or more ZIP files. \"],\"answer\":[\"Must not include a parent folder or top-level directory. \",\"Must not exceed 500 MB. \"],\"reason\":\"Explanation: When you use the AWS Elastic Beanstalk console to deploy a new application or an application version, you'll need to upload a source bundle. Your source bundle must meet the following requirements:Consist of a single ZIP file or WAR file (you can include multiple WAR files inside your ZIP file)Not exceed 500 MBNot include a parent folder or top-level directory (subdirectories are fine)If you want to deploy a worker application that processes periodic background tasks, your application source bundle must also include a cron. yaml file, but in other cases it is not required. \\n CORRECT : \\\"Must not include a parent folder or top-level directory\\\" is a correct answer. \\n CORRECT : \\\"Must not exceed 500 MB\\\" is also a correct answer. \\n INCORRECT : \\\"Must include the cron. yaml file\\\" is incorrect. As mentioned above, this is not required in all cases. \\n INCORRECT : \\\"Must include a parent folder or top-level directory\\\" is incorrect. A parent folder or top-level directory must NOT be included. \\n INCORRECT : \\\"Must consist of one or more ZIP files\\\" is incorrect. You bundle into a single ZIP or WAR file. References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/applications-sourcebundle.html Save time with our AWS cheat sheets:https://digitalcloud.training/certification-training/aws-Developer-associate/aws-compute/elastic-beanstalk/\"}",
    "{\"question\":\"A Developer is designing a fault-tolerant application that will use Amazon EC2 instances and an Elastic Load Balancer. The Developer needs to ensure that if an EC2 instance fails session data is not lost. How can this be achieved?\",\"choices\":[\"Enable Sticky Sessions on the Elastic Load Balancer\",\"Use an EC2 Auto Scaling group to automatically launch new instances\",\"Use Amazon DynamoDB to perform scalable session handling\",\"Use Amazon SQS to save session data\"],\"answer\":\"Use Amazon DynamoDB to perform scalable session handling\",\"reason\":\"Explanation: For this scenario the key requirement is to ensure the data is not lost. Therefore, the data must be stored in a durable data store outside of the EC2 instances. Amazon DynamoDB is a suitable solution for storing session data. DynamoDB has a session handling capability for multiple languages as in the below example for PHP:“The DynamoDB Session Handler is a custom session handler for PHP that allows developers to use Amazon DynamoDB as a session store. Using DynamoDB for session storage alleviates issues that occur with session handling in a distributed web application by moving sessions off of the local file system and into a shared location. DynamoDB is fast, scalable, easy to setup, and handles replication of your data automatically. ”Therefore, the best answer is to use DynamoDB to store the session data. \\n CORRECT : \\\"Use Amazon DynamoDB to perform scalable session handling\\\" is the correct answer. \\n INCORRECT : \\\"Enable Sticky Sessions on the Elastic Load Balancer\\\" is incorrect. Sticky sessions attempts to direct a user that has reconnected to the application to the same EC2 instance that they connected to previously. However, this does not ensure that the session data is going to be available. \\n INCORRECT : \\\"Use an EC2 Auto Scaling group to automatically launch new instances\\\" is incorrect as this does not provide a solution for storing the session data. \\n INCORRECT : \\\"Use Amazon SQS to save session data\\\" is incorrect as Amazon SQS is not suitable for storing session data. References: https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-dynamodb/\"}",
    "{\"question\":\"A company has a large Amazon DynamoDB table which they scan periodically so they can analyze several attributes. The scans are consuming a lot of provisioned throughput. What technique can a Developer use to minimize the impact of the scan on the table's provisioned throughput?\",\"choices\":[\"Set a smaller page size for the scan\",\"Use parallel scans\",\"Define a range key on the table\",\"Prewarm the table by updating all items\"],\"answer\":\"Set a smaller page size for the scan\",\"reason\":\"Explanation: In general, Scan operations are less efficient than other operations in DynamoDB. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set. If possible, you should avoid using a Scan operation on a large table or index with a filter that removes many results. Also, as a table or index grows, the Scan operation slows. The Scan operation examines every item for the requested values and can use up the provisioned throughput for a large table or index in a single operation. The following diagram illustrates the impact of a sudden spike of capacity unit usage by Query and Scan operations, and its impact on your other requests against the same table. Instead of using a large Scan operation, you can use the following techniques to minimize the impact of a scan on a table's provisioned throughput. Reduce page sizeBecause a Scan operation reads an entire page (by default, 1 MB), you can reduce the impact of the scan operation by setting a smaller page size. The Scan operation provides a Limit parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a \\\"pause\\\" between each request. Isolate scan operationsDynamoDB is designed for easy scalability. As a result, an application can create tables for distinct purposes, possibly even duplicating content across several tables. You want to perform scans on a table that is not taking \\\"mission-critical\\\" traffic. Some applications handle this load by rotating traffic hourly between two tables—one for critical traffic, and one for bookkeeping. Other applications can do this by performing every write on two tables: a \\\"mission-critical\\\" table, and a \\\"shadow\\\" table. Therefore, the best option to reduce the impact of the scan on the table's provisioned throughput is to set a smaller page size for the scan. \\n CORRECT : \\\"Set a smaller page size for the scan\\\" is the correct answer. \\n INCORRECT : \\\"Use parallel scans\\\" is incorrect as this will return results faster but place more burden on the table’s provisioned throughput. \\n INCORRECT : \\\"Define a range key on the table\\\" is incorrect. A range key is a composite key that includes the hash key and another attribute. This is of limited use in this scenario as the table is being scanned to analyze multiple attributes. \\n INCORRECT : \\\"Prewarm the table by updating all items\\\" is incorrect as updating all items would incur significant costs in terms of provisioned throughput and would not be advantageous. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-dynamodb/\"}",
    "{\"question\":\"A Developer needs to scan a full DynamoDB 50GB table within non-peak hours. About half of the strongly consistent RCUs are typically used during non-peak hours and the scan duration must be minimized. How can the Developer optimize the scan execution time without impacting production workloads?\",\"choices\":[\"Use sequential scans\",\"Use parallel scans while limiting the rate\",\"Increase the RCUs during the scan operation\",\"Change to eventually consistent RCUs during the scan operation\"],\"answer\":\"Use parallel scans while limiting the rate\",\"reason\":\"Explanation: Performing a scan on a table consumes a lot of RCUs. A Scan operation always scans the entire table or secondary index. It then filters out values to provide the result you want, essentially adding the extra step of removing data from the result set. To reduce the amount of RCUs used by the scan so it doesn’t affect production workloads whilst minimizing the execution time, there are a couple of recommendations the Developer can follow. Firstly, the Limit parameter can be used to reduce the page size. The Scan operation provides a Limit parameter that you can use to set the page size for your request. Each Query or Scan request that has a smaller page size uses fewer read operations and creates a \\\"pause\\\" between each request. Secondly, the Developer can configure parallel scans. With parallel scans the Developer can maximize usage of the available throughput and have the scans distributed across the table’s partitions. A parallel scan can be the right choice if the following conditions are met:The table size is 20 GB or larger. The table's provisioned read throughput is not being fully used. Sequential Scan operations are too slow. Therefore, to optimize the scan operation the Developer should use parallel scans while limiting the rate as this will ensure that the scan operation does not affect the performance of production workloads and still have it complete in the minimum time. \\n CORRECT : \\\"Use parallel scans while limiting the rate\\\" is the correct answer. \\n INCORRECT : \\\"Use sequential scans\\\" is incorrect as this is slower than parallel scans and the Developer needs to minimize scan execution time. \\n INCORRECT : \\\"Increase the RCUs during the scan operation\\\" is incorrect as the table is only using half of the RCUs during non-peak hours so there are RCUs available. You could increase RCUs and perform the scan faster, but this would be more expensive. The better solution is to use parallel scans with the limit parameter. \\n INCORRECT : \\\"Change to eventually consistent RCUs during the scan operation\\\" is incorrect as this does not provide a solution for preventing impact to the production workloads. The limit parameter should be used to ensure the tables RCUs are not fully used. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/bp-query-scan.html#QueryAndScanGuidelines. ParallelScanSave time with our AWS cheat sheets:https://digitalcloud.training/amazon-dynamodb/\"}",
    "{\"question\":\"An e-commerce web application that shares session state on-premises is being migrated to AWS. The application must be fault tolerant, natively highly scalable, and any service interruption should not affect the user experience. What is the best option to store the session state?\",\"choices\":[\"Store the session state in Amazon ElastiCache\",\"Store the session state in Amazon CloudFront\",\"Store the session state in Amazon S3\",\"Enable session stickiness using elastic load balancers\"],\"answer\":\"Store the session state in Amazon ElastiCache\",\"reason\":\"Explanation: There are various ways to manage user sessions including storing those sessions locally to the node responding to the HTTP request or designating a layer in your architecture which can store those sessions in a scalable and robust manner. Common approaches used include utilizing Sticky sessions or using a Distributed Cache for your session management. In this scenario, a distributed cache is suitable for storing session state data. ElastiCache can perform this role and with the Redis engine replication is also supported. Therefore, the solution is fault-tolerant and natively highly scalable. \\n CORRECT : \\\"Store the session state in Amazon ElastiCache\\\" is the correct answer. \\n INCORRECT : \\\"Store the session state in Amazon CloudFront\\\" is incorrect as CloudFront is not suitable for storing session state data, it is used for caching content for better global performance. \\n INCORRECT : \\\"Store the session state in Amazon S3\\\" is incorrect as though you can store session data in Amazon S3 and replicate the data to another bucket, this would result in a service interruption if the S3 bucket was not accessible. \\n INCORRECT : \\\"Enable session stickiness using elastic load balancers\\\" is incorrect as this feature directs sessions from a specific client to a specific EC2 instances. Therefore, if the instance fails the user must be redirected to another EC2 instance and the session state data would be lost. References: https://aws. amazon. com/caching/session-management/Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-elasticache/\"}",
    "{\"question\":\"A Developer is creating a serverless application that uses an Amazon DynamoDB table. The application must make idempotent, all-or-nothing operations for multiple groups of write actions. Which solution will meet these requirements?\",\"choices\":[\"Update the items in the table using the BatchWriteltem operation and configure idempotency at the table level. \",\"Create an Amazon SQS FIFO queue and use the SendMessageBatch operation to group the changes. \",\"Update the items in the table using the TransactWriteltems operation to group the changes. \",\"Enable DynamoDB streams and capture new images. Update the items in the table using the BatchWriteltem. \"],\"answer\":\"Update the items in the table using the TransactWriteltems operation to group the changes. \",\"reason\":\"Explanation: TransactWriteItems is a synchronous and idempotent write operation that groups up to 25 write actions in a single all-or-nothing operation. These actions can target up to 25 distinct items in one or more DynamoDB tables within the same AWS account and in the same Region. The aggregate size of the items in the transaction cannot exceed 4 MB. The actions are completed atomically so that either all of them succeed or none of them succeeds. A TransactWriteItems operation differs from a BatchWriteItem operation in that all the actions it contains must be completed successfully, or no changes are made at all. With a BatchWriteItem operation, it is possible that only some of the actions in the batch succeed while the others do not. \\n CORRECT : \\\"Update the items in the table using the TransactWriteltems operation to group the changes\\\" is the correct answer. \\n INCORRECT : \\\"Update the items in the table using the BatchWriteltem operation and configure idempotency at the table level\\\" is incorrect. As explained above, the TransactWriteItems operation must be used. \\n INCORRECT : \\\"Enable DynamoDB streams and capture new images. Update the items in the table using the BatchWriteltem\\\" is incorrect. DynamoDB streams will not assist with making idempotent write operations. \\n INCORRECT : \\\"Create an Amazon SQS FIFO queue and use the SendMessageBatch operation to group the changes\\\" is incorrect. Amazon SQS should not be used as it does not assist and this solution is supposed to use a DynamoDB tableReferences:https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_TransactWriteItems.html Save time with our AWS cheat sheets:https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-associate/aws-database-dva/amazon-dynamodb/\"}",
    "{\"question\":\"A Development team would use a GitHub repository and would like to migrate their application code to AWS CodeCommit. What needs to be created before they can migrate a cloned repository to CodeCommit over HTTPS?\",\"choices\":[\"A GitHub secure authentication token\",\"A public and private SSH key file\",\"A set of Git credentials generated with IAM\",\"An Amazon EC2 IAM role with CodeCommit permissions\"],\"answer\":\"A set of Git credentials generated with IAM\",\"reason\":\"Explanation: AWS CodeCommit is a managed version control service that hosts private Git repositories in the AWS cloud. To use CodeCommit, you configure your Git client to communicate with CodeCommit repositories. As part of this configuration, you provide IAM credentials that CodeCommit can use to authenticate you. IAM supports CodeCommit with three types of credentials:Git credentials, an IAM -generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS. SSH keys, a locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH. AWS access keys, which you can use with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS. In this scenario the Development team need to connect to CodeCommit using HTTPS so they need either AWS access keys to use the AWS CLI or Git credentials generated by IAM. Access keys are not offered as an answer choice so the best answer is that they need to create a set of Git credentials generated with IAMCORRECT: \\\"A set of Git credentials generated with IAM\\\" is the correct answer. \\n INCORRECT : \\\"A GitHub secure authentication token\\\" is incorrect as they need to authenticate to AWS CodeCommit, not GitHub (they have already accessed and cloned the repository). \\n INCORRECT : \\\"A public and private SSH key file\\\" is incorrect as these are used to communicate with CodeCommit repositories using SSH, not HTTPS. \\n INCORRECT : \\\"An Amazon EC2 IAM role with CodeCommit permissions\\\" is incorrect as you need the Git credentials generated through IAM to connect to CodeCommit. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-developer-tools/\"}",
    "{\"question\":\"A company has implemented AWS CodePipeline to automate its release pipelines. The Development team is writing an AWS Lambda function that will send notifications for state changes of each of the actions in the stages. Which steps must be taken to associate the Lambda function with the event source?\",\"choices\":[\"Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source\",\"Create an event trigger and specify the Lambda function from the CodePipeline console\",\"Create an Amazon CloudWatch alarm that monitors status changes in CodePipeline and triggers the Lambda function\",\"Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source\"],\"answer\":\"Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source\",\"reason\":\"Explanation: Amazon CloudWatch Events help you to respond to state changes in your AWS resources. When your resources change state, they automatically send events into an event stream. You can create rules that match selected events in the stream and route them to your AWS Lambda function to take action. AWS CodePipeline can be configured as an event source in CloudWatch Events and can then send notifications using as service such as Amazon SNS. Therefore, the best answer is to create an Amazon CloudWatch Events rule that uses CodePipeline as an event source. \\n CORRECT : \\\"Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source\\\" is the correct answer. \\n INCORRECT : \\\"Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source\\\" is incorrect as CodePipeline cannot be configured as a trigger for Lambda. \\n INCORRECT : \\\"Create an event trigger and specify the Lambda function from the CodePipeline console\\\" is incorrect as CodePipeline cannot be configured as a trigger for Lambda. \\n INCORRECT : \\\"Create an Amazon CloudWatch alarm that monitors status changes in CodePipeline and triggers the Lambda function\\\" is incorrect as CloudWatch Events is used for monitoring state changes. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/events/WhatIsCloudWatchEvents.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cloudwatch/\"}",
    "{\"question\":\"A company is deploying a microservices application on AWS Fargate using Amazon ECS. The application has environment variables that must be passed to a container for the application to initialize. How should the environment variables be passed to the container?\",\"choices\":[\"Use advanced container definition parameters and define environment variables under the environment parameter within the service definition. \",\"Use advanced container definition parameters and define environment variables under the environment parameter within the task definition. \",\"Use standard container definition parameters and define environment variables under the secrets parameter within the task definition. \",\"Use standard container definition parameters and define environment variables under the WorkingDirectory parameter within the service definition. \"],\"answer\":\"Use advanced container definition parameters and define environment variables under the environment parameter within the task definition. \",\"reason\":\"Explanation: When you register a task definition, you must specify a list of container definitions that are passed to the Docker daemon on a container instance. The developer should use advanced container definition parameters and define environment variables to pass to the container. \\n CORRECT : \\\"Use advanced container definition parameters and define environment variables under the environment parameter within the task definition\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Use advanced container definition parameters and define environment variables under the environment parameter within the service definition\\\" is incorrect. The task definition is the correct place to define the environment variables to pass to the container. \\n INCORRECT : \\\"Use standard container definition parameters and define environment variables under the secrets parameter within the task definition\\\" is incorrect. Advanced container definition parameters must be used to pass the environment variables to the container. The environment parameter should also be used. \\n INCORRECT : \\\"Use standard container definition parameters and define environment variables under the WorkingDirectory parameter within the service definition\\\" is incorrect. Advanced container definition parameters must be used to pass the environment variables to the container. The environment parameter should also be used. References: https://docs.aws.amazon.com/AmazonECS/latest/userguide/task_definition_parameters.html#container_definition_environmentSave time with our AWS cheat sheets:https://digitalcloud.training/amazon-ecs-and-eks/\"}",
    "{\"question\":\"A developer has deployed an application on AWS Lambda. The application uses Python and must generate and then upload a file to an Amazon S3 bucket. The developer must implement the upload functionality with the least possible change to the application code. Which solution BEST meets these requirements?\",\"choices\":[\"Make an HTTP request directly to the S3 API to upload the file. \",\"Include the AWS SDK for Python in the Lambda function code. \",\"Use the AWS SDK for Python that is installed in the Lambda execution environment. \",\"Use the AWS CLI that is installed in the Lambda execution environment. \"],\"answer\":\"Use the AWS SDK for Python that is installed in the Lambda execution environment. \",\"reason\":\"Explanation: The best practice for Lambda development is to bundle all dependencies used by your Lambda function, including the AWS SDK. However, since this question specifically requests that the least possible changes are made to the application code, the developer can instead use the SDK for Python that is installed in the Lambda environment to upload the file to Amazon S3. \\n CORRECT : \\\"Use the AWS SDK for Python that is installed in the Lambda execution environment\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Include the AWS SDK for Python in the Lambda function code\\\" is incorrect. This is the best practice for deployment. However, in this case the developer must minimize changes to code and including the SDK as a dependency in the code would require potential updates to existing Python code. \\n INCORRECT : \\\"Make an HTTP request directly to the S3 API to upload the file\\\" is incorrect. AWS supports uploads to S3 using the console, AWS SDKs, REST API, and the AWS CLI. \\n INCORRECT : \\\"Use the AWS CLI that is installed in the Lambda execution environment\\\" is incorrect. The AWS CLI is not installed in the Lambda execution environment. References: https://aws. amazon. com/blogs/compute/upcoming-changes-to-the-python-sdk-in-aws-lambda/Save time with our AWS cheat sheets:https://digitalcloud.training/aws-lambda/\"}",
    "{\"question\":\"A company is deploying a static website hosted from an Amazon S3 bucket. The website must support encryption in-transit for website visitors. Which combination of actions must the Developer take to meet this requirement? (Select TWO. )\",\"choices\":[\"Create an AWS WAF WebACL with a secure listener. \",\"Create an Amazon CloudFront distribution. Set the S3 bucket as an origin. \",\"Configure an Amazon CloudFront distribution with an AWS WAF WebACL. \",\"Configure the S3 bucket with an SSL/TLS certificate. \",\"Configure an Amazon CloudFront distribution with an SSL/TLS certificate. \"],\"answer\":[\"Create an Amazon CloudFront distribution. Set the S3 bucket as an origin. \",\"Configure an Amazon CloudFront distribution with an SSL/TLS certificate. \"],\"reason\":\"Explanation: Amazon S3 static websites use the HTTP protocol only and you cannot enable HTTPS. To enable HTTPS connections to your S3 static website, use an Amazon CloudFront distribution that is configured with an SSL/TLS certificate. This will ensure that connections between clients and the CloudFront distribution are encrypted in-transit as per the requirements. \\n CORRECT : \\\"Create an Amazon CloudFront distribution. Set the S3 bucket as an origin\\\" is a correct answer. \\n CORRECT : \\\"Configure an Amazon CloudFront distribution with an SSL/TLS certificate\\\" is also a correct answer. \\n INCORRECT : \\\"Create an AWS WAF WebACL with a secure listener\\\" is incorrect. You cannot configure a secure listener on a WebACL. \\n INCORRECT : \\\"Configure an Amazon CloudFront distribution with an AWS WAF WebACL\\\" is incorrect. This will not enable encrypted connections. \\n INCORRECT : \\\"Configure the S3 bucket with an SSL/TLS certificate\\\" is incorrect. You cannot manually add SSL/TLS certificates to Amazon S3, and it is not possible to directly configure an S3 bucket that is configured as a static website to accept encrypted connections. References: https://aws. amazon. com/premiumsupport/knowledge-center/cloudfront-serve-static-website/Save time with our AWS cheat sheets:https://digitalcloud.training/certification-training/aws-Developer-associate/aws-networking-and-content-delivery/amazon-cloudfront/\"}",
    "{\"question\":\"An organization is hosting a website on an Amazon EC2 instance in a public subnet. The website should allow public access for HTTPS traffic on TCP port 443 but should only accept SSH traffic on TCP port 22 from a corporate address range accessible over a VPN. Which security group configuration will support both requirements?\",\"choices\":[\"Allow traffic to port 22 from 0. 0. 0. 0/0 and allow traffic to port 443 from 192. 168. 0. 0/16. \",\"Allow traffic to both port 443 and port 22 from the VPC CIDR block. \",\"Allow traffic to both port 443 and port 22 from 0. 0. 0. 0/0 and 192. 168. 0. 0/16. \",\"Allow traffic to port 443 from 0. 0. 0. 0/0 and allow traffic to port 22 from 192. 168. 0. 0/16. \"],\"answer\":\"Allow traffic to port 443 from 0. 0. 0. 0/0 and allow traffic to port 22 from 192. 168. 0. 0/16. \",\"reason\":\"Explanation: Allowing traffic from 0. 0. 0. 0/0 to port 443 will allow any traffic from the internet to access the website.  Limiting the IP address to 192. 168. 0. 0/16 for port 22 will only allow local organizational traffic. \\n CORRECT : \\\"Allow traffic to port 443 from 0. 0. 0. 0/0 and allow traffic to port 22 from 192. 168. 0. 0/16\\\" is the correct answer (as explained above. )\\n INCORRECT : “Allow traffic to port 22 from 0. 0. 0. 0/0 and allow traffic to port 443 from 192. 168. 0. 0/16\\\" is incorrect. This will allow traffic from the Internet to port 22 and allow traffic to port 443 from the corporate address block only (192. 168. 0. 0/16). \\n INCORRECT : \\\"Allow traffic to both port 443 and port 22 from the VPC CIDR block\\\" is incorrect.  This would not satisfy either requirement as internet-based users will not be able to access the website and corporate users will not be able to manage the instance via SSH. \\n INCORRECT : \\\"Allow traffic to both port 443 and port 22 from 0. 0. 0. 0/0 and 192. 168. 0. 0/16\\\" is incorrect. This does not satisfy the requirement to restrict access to port 22 to only the corporate address block. References: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-ec2/\"}",
    "{\"question\":\"An application uses an Amazon RDS database. The company requires that the performance of database reads is improved, and they want to add a caching layer in front of the database. The cached data must be encrypted, and the solution must be highly available. Which solution will meet these requirements?\",\"choices\":[\"Amazon CloudFront with multiple origins. \",\"Amazon ElastiCache for Redis in cluster mode. \",\"Amazon DynamoDB Accelerator (DAX). \",\"Amazon ElastiCache for Memcached. \"],\"answer\":\"Amazon ElastiCache for Redis in cluster mode. \",\"reason\":\"Explanation: Amazon ElastiCache is an in-memory database cache that can be used in front of Amazon RDS. The key to answering this question is to know the differences between ElastiCache Memcached and ElastiCache Redis. To support both encryption and high availability we must use ElastiCache Redis with cluster mode enabled. You can see the differences between the different engines and configuration options for ElastiCache in the table below:\\n CORRECT : \\\"Amazon ElastiCache for Redis in cluster mode\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Amazon ElastiCache for Memcached\\\" is incorrect. The Memcached engine does not support encryption or high availability. \\n INCORRECT : \\\"Amazon CloudFront with multiple origins\\\" is incorrect. You cannot configure an Amazon RDS as an origin for Amazon RDS. Also, what would the second origin be anyway? There’s only one database!\\n INCORRECT : \\\"Amazon DynamoDB Accelerator (DAX)\\\" is incorrect. DynamoDB DAX can be used to increase the performance of DynamoDB tables and offload read requests. It cannot be used in front of an Amazon RDS database. References: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Replication. Redis-RedisCluster.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-elasticache/\"}",
    "{\"question\":\"An ecommerce company manages a storefront that uses an Amazon API Gateway API which exposes an AWS Lambda function. The Lambda functions processes orders and stores the orders in an Amazon RDS for MySQL database. The number of transactions increases sporadically during marketing campaigns, and then goes close to zero during quite times. How can a developer increase the elasticity of the system MOST cost-effectively?\",\"choices\":[\"Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average CPU utilization. \",\"Create an Amazon SNS topic. Publish transactions to the topic configure an SQS queue as a destination. Configure Lambda to process transactions from the queue. \",\"Create an Amazon SQS queue. Publish transactions to the queue and set the queue to invoke the Lambda function. Set the reserved concurrency of the Lambda function to be equal to the max number of database connections. \",\"Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average connections of Aurora Replicas. \"],\"answer\":\"Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average connections of Aurora Replicas. \",\"reason\":\"Explanation: The most efficient solution would be to use Aurora Auto Scaling and configure the scaling events to happen based on target metric. The metric to use is Average connections of Aurora Replicas which will create a policy based on the average number of connections to Aurora Replicas. This will ensure that the Aurora replicas scale based on actual numbers of connections to the replicas which will vary based on how busy the storefront is and how many transactions are being processed. \\n CORRECT : \\\"Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average connections of Aurora Replicas\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Migrate from Amazon RDS to Amazon Aurora MySQL. Use an Aurora Auto Scaling policy to scale read replicas based on average CPU utilization\\\" is incorrect. The better metric to use for this situation would be the number of connections to Aurora Replicas as that is the metric that has the closest correlation to the number of transactions being executed. \\n INCORRECT : \\\"Create an Amazon SNS topic. Publish transactions to the topic configure an SQS queue as a destination. Configure Lambda to process transactions from the queue\\\" is incorrect. This is highly inefficient. There is no need for an SNS topic in this situation. \\n INCORRECT : \\\"Create an Amazon SQS queue. Publish transactions to the queue and set the queue to invoke the Lambda function. Set the reserved concurrency of the Lambda function to be equal to the max number of database connections\\\" is incorrect. This would be less cost effective as you would be paying for the reserved concurrency at all times. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora. Integrating. AutoScaling.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-aurora/\"}",
    "{\"question\":\"An organization handles data that requires high availability in its relational database. The main headquarters for the organization is in Virginia with smaller offices located in California. The main headquarters uses the data more frequently than the smaller offices. How should the developer configure their databases to meet high availability standards?\",\"choices\":[\"Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in Virginia. \",\"Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in California. \",\"Create a DynamoDB database with the primary database in Virginia and specify the failover to the DynamoDB replica in another AZ in Virginia. \",\"Create an Athena database with the primary database in Virginia and specify the failover to the Athena replica in another AZ in Virginia. \"],\"answer\":\"Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in Virginia. \",\"reason\":\"Explanation: Aurora is a relational database that provides high availability by allowing customers to create up to 15 database replications in different Availability Zones. It also allows you to specify which Aurora replica can be promoted to the primary database should the primary database become unavailable. Selecting the AZ that is closest to the main headquarters should not negatively impact the smaller offices but changing the primary database to California could negatively impact the main headquarters. \\n CORRECT : \\\"Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in Virginia\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Create an Aurora database with the primary database in Virginia and specify the failover to the Aurora replica in another AZ in California\\\" is incorrect. It could create some latency issues for the main headquarters in Virginia. \\n INCORRECT : \\\"Create a DynamoDB database with the primary database in Virginia and specify the failover to the DynamoDB replica in another AZ in Virginia\\\" is incorrect. DynamoDB is not a relational database. \\n INCORRECT : \\\"Create an Athena database with the primary database in Virginia and specify the failover to the Athena replica in another AZ in Virginia\\\" is incorrect. Athena analyzes data but is not a database service. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora. Overview.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-aurora/\"}",
    "{\"question\":\"A developer is responsible for a business critical application that uses Amazon DynamoDB as its main data repository. This DynamoDB table holds millions of records and handles high volumes of requests. The developer must implement near-real time processing on the records as soon as they are inserted or modified in the DynamoDB table. What's the most efficient way to introduce this capability with MINIMUM modification to the existing application code?\",\"choices\":[\"Use Amazon SQS to queue incoming data and process it using Amazon EC2 instances. \",\"Use AWS Lambda triggered by DynamoDB Streams to process the documents. \",\"Modify the application code to add processing logic after each DynamoDB write operation. \",\"Set up an Amazon Kinesis Data Stream to process updates from the DynamoDB table. \"],\"answer\":\"Use AWS Lambda triggered by DynamoDB Streams to process the documents. \",\"reason\":\"Explanation: AWS Lambda can be triggered by DynamoDB Streams to automatically process changes to the DynamoDB table, ensuring near-real-time processing without substantial changes to the application code. \\n CORRECT : \\\"Use AWS Lambda triggered by DynamoDB Streams to process the documents\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Use Amazon SQS to queue incoming data and process it using Amazon EC2 instances\\\" is incorrect. Amazon SQS would require significant changes to the application code to integrate the queuing and processing mechanism. Moreover, it would not guarantee near-real-time processing. \\n INCORRECT : \\\"Modify the application code to add processing logic after each DynamoDB write operation\\\" is incorrect. Modifying the application code to include processing logic after each write operation would entail significant changes to the code and may also slow down write operations. \\n INCORRECT : \\\"Set up an Amazon Kinesis Data Stream to process updates from the DynamoDB table\\\" is incorrect. Amazon Kinesis Data Streams can be used to capture the changes, but consumers are required to perform the processing, and this is not mentioned in the solution. The simplest solution is to use DynamoDB streams with Lambda. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-dynamodb/\"}",
    "{\"question\":\"A critical application is hosted on AWS, exposed by an HTTP API through Amazon API Gateway. The API is integrated with an AWS Lambda function and the application data is housed in an Amazon RDS for PostgreSQL DB instance, featuring 2 vCPUs and 16 GB of RAM. The company has been receiving customer complaints about occasional HTTP 500 Internal Server Error responses from some API calls during unpredictable peak usage times. Amazon CloudWatch Logs has recorded \\\"connection limit exceeded\\\" errors. The company wants to ensure resilience in the application, with no unscheduled downtime for the database. Which solution would best fit these requirements?\",\"choices\":[\"Double the RAM and vCPUs of the RDS instance. \",\"Implement auto-scaling for the RDS instance based on connection count. \",\"Use AWS Lambda to create a connection pool for the RDS instance. \",\"Use Amazon RDS Proxy and update the Lambda function to connect to the proxy. \"],\"answer\":\"Use Amazon RDS Proxy and update the Lambda function to connect to the proxy. \",\"reason\":\"Explanation: Amazon RDS Proxy is designed to improve application scalability and resilience by pooling and sharing database connections, reducing the CPU and memory overhead on the database. It handles the burst in connections seamlessly and improves the application's ability to scale. \\n CORRECT : \\\"Use Amazon RDS Proxy and update the Lambda function to connect to the proxy\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Double the RAM and vCPUs of the RDS instance\\\" is incorrect. Merely augmenting the number of vCPUs and RAM for the RDS instance might not resolve the issue as it doesn't directly tackle the problem of too many connections. \\n INCORRECT : \\\"Implement auto-scaling for the RDS instance based on connection count\\\" is incorrect. Auto-scaling in response to connection count is not a feature provided by RDS. \\n INCORRECT : \\\"Use AWS Lambda to create a connection pool for the RDS instance\\\" is incorrect. The best solution is to use RDS proxy which is designed for creating a pool of connections. Lambda may not be the best solution for this problem as it could be costly and has a limitation in execution time. References: https://aws. amazon. com/blogs/compute/using-amazon-rds-proxy-with-aws-lambda/Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-rds/\"}",
    "{\"question\":\"A developer is using AWS CodeBuild to build an application into a Docker image. The buildspec file is used to run the application build. The developer needs to push the Docker image to an Amazon ECR repository only upon the successful completion of each build. \",\"choices\":[\"Add a post_build phase to the buildspec file that uses the commands block to push the Docker image. \",\"Add a post_build phase to the buildspec file that uses the finally block to push the Docker image. \",\"Add an install phase to the buildspec file that uses the commands block to push the Docker image. \",\"Add a post_build phase to the buildspec file that uses the artifacts sequence to find the build artifacts and push to Amazon ECR. \"],\"answer\":\"Add a post_build phase to the buildspec file that uses the commands block to push the Docker image. \",\"reason\":\"Explanation: The post_build phase is an optional sequence. It represents the commands, if any, that CodeBuild runs after the build. For example, you might use Maven to package the build artifacts into a JAR or WAR file, or you might push a Docker image into Amazon ECR. Then you might send a build notification through Amazon SNS. Here is an example of a buildspec file with a post_build phase that pushes a Docker image to Amazon ECR:\\n CORRECT : \\\"Add a post_build phase to the buildspec file that uses the commands block to push the Docker image\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Add a post_build phase to the buildspec file that uses the finally block to push the Docker image\\\" is incorrect. Commands specified in a finally block are run after commands in the commands block. The commands in a finally block are run even if a command in the commands block fails. This would not be ideal as this would push the image to ECR even if commands in previous sequences failed. \\n INCORRECT : \\\"Add an install phase to the buildspec file that uses the commands block to push the Docker image\\\" is incorrect. These are commands that are run during installation. The develop would want to push the image only after all installations have succeeded. Therefore, the post_build phase should be used. \\n INCORRECT : \\\"Add a post_build phase to the buildspec file that uses the artifacts sequence to find the build artifacts and push to Amazon ECR\\\" is incorrect. The artifacts sequence is not required if you are building and pushing a Docker image to Amazon ECR, or you are running unit tests on your source code, but not building it. References: https://docs.aws.amazon.com/codebuild/latest/userguide/sample-docker.htmlhttps://docs.aws.amazon.com/codebuild/latest/userguide/build-spec-ref.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-developer-tools/\"}",
    "{\"question\":\"A Developer is writing code to run in a cron job on an Amazon EC2 instance that sends status information about the application to Amazon CloudWatch. Which method should the Developer use?\",\"choices\":[\"Use the AWS CLI put-metric-alarm command. \",\"Use the AWS CLI put-metric-data command. \",\"Use the unified CloudWatch agent to publish custom metrics. \",\"Use the CloudWatch console with detailed monitoring. \"],\"answer\":\"Use the AWS CLI put-metric-data command. \",\"reason\":\"Explanation: The put-metric-data command publishes metric data points to Amazon CloudWatch. CloudWatch associates the data points with the specified metric. If the specified metric does not exist, CloudWatch creates the metric. \\n CORRECT : \\\"Use the AWS CLI put-metric-data command\\\" is the correct answer. \\n INCORRECT : \\\"Use the AWS CLI put-metric-alarm command\\\" is incorrect. This command creates or updates an alarm and associates it with the specified metric, metric math expression, or anomaly detection model. \\n INCORRECT : \\\"Use the unified CloudWatch agent to publish custom metrics\\\" is incorrect. It is not necessary to use the unified CloudWatch agent. In this case the Developer can use the AWS CLI with the cron job. \\n INCORRECT : \\\"Use the CloudWatch console with detailed monitoring\\\" is incorrect. You cannot collect custom metric data using the CloudWatch console with detailed monitoring. Detailed monitoring sends data at 1-minute rather than 5-minute frequencies but will not collect custom data. References: https://docs.aws.amazon.com/cli/latest/reference/cloudwatch/put-metric-data.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cloudwatch/\"}",
    "{\"question\":\"A review of Amazon CloudWatch metrics shows that there are a high number of reads taking place on a primary database built on Amazon Aurora with MySQL. What can a developer do to improve the read scaling of the database? (Select TWO. )\",\"choices\":[\"Create Aurora Replicas in same cluster as the primary database instance. \",\"Create a separate Aurora MySQL cluster and configure binlog replication. \",\"Create a duplicate Aurora database cluster to process read requests. \",\"Create a duplicate Aurora primary database to process read requests. \",\"Create Aurora Replicas in a global S3 bucket as the primary read source. \"],\"answer\":[\"Create Aurora Replicas in same cluster as the primary database instance. \",\"Create a separate Aurora MySQL cluster and configure binlog replication. \"],\"reason\":\"Explanation: Aurora Replicas can help improve read scaling because it synchronously updates data with the primary database (within 100 ms). Aurora Replicas are created in the same DB cluster within a Region. With Aurora MySQL you can also enable binlog replication to another Aurora DB cluster which can be in the same or a different Region. \\n CORRECT : \\\"Create Aurora Replicas in same cluster as the primary database instance\\\" is the correct answer (as explained above. )\\n CORRECT : \\\"Create a separate Aurora MySQL cluster and configure binlog replication\\\" is also a correct answer (as explained above. )\\n INCORRECT : \\\"Create a duplicate Aurora database cluster to process read requests\\\" is incorrect. A duplicate Aurora database cluster would be a separate database with read and write capability and would not help with read scaling. \\n INCORRECT : \\\"Create a duplicate Aurora primary database to process read requests\\\" is incorrect. A duplicate Aurora primary database would be for read and write requests and would not help with read scaling. \\n INCORRECT : \\\"Creating read replicas of Aurora in a S3 global bucket as the primary read source\\\" is incorrect. S3 is an object storage service. It cannot be used to host databases. References: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/Aurora. Managing. Performance.html#Aurora. Managing. Performance. ReadScalingSave time with our AWS cheat sheets:https://digitalcloud.training/amazon-aurora/\"}",
    "{\"question\":\"A developer is partitioning data using Athena to improve performance when performing queries. What are two things the analyst can do that would counter any benefit of using partitions? (Select TWO. )\",\"choices\":[\"Segmenting data too finely. \",\"Skewing data heavily to one partition value. \",\"Storing the data in S3. \",\"Creating partitions directly from data source. \",\"Using a Hive-style partition format. \"],\"answer\":[\"Segmenting data too finely. \",\"Skewing data heavily to one partition value. \"],\"reason\":\"Explanation: There is a cost associated with partitioning data. A higher number of partitions can also increase the overhead from retrieving and processing the partition metadata. Multiple smaller files can counter the benefit of using partitioning. If your data is heavily skewed to one partition value, and most queries use that value, then the overhead may wipe out the initial benefit. \\n CORRECT : \\\"Segmenting data too finely\\\" is a correct answer (as explained above. )\\n CORRECT : \\\"Skewing data heavily to one partition value\\\" is a correct answer (as explained above. )\\n INCORRECT : \\\"Storing the data in S3\\\" is incorrect. Data must be stored in S3 buckets. \\n INCORRECT : \\\"Creating partitions directly from data source \\\" is incorrect. Athena can pull data directly from the S3 source. \\n INCORRECT : \\\"Using a Hive-style partition format\\\" is incorrect. Athena is compatible with Hive-style partition formats. References: https://aws. amazon. com/blogs/big-data/top-10-performance-tuning-tips-for-amazon-athena/Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-athena/\"}",
    "{\"question\":\"A developer is running queries on Hive-compatible partitions in Athena using DDL but is facing time out issues. What is the most effective and efficient way to prevent this from continuing to happen?\",\"choices\":[\"Use the MSCK REPAIR TABLE command to update the metadata in the catalog. \",\"Use the ALTER TABLE ADD PARTITION command to update the column names. \",\"Export the data into DynamoDB to perform queries in a more flexible schema. \",\"Export the data into a JSON document to clean any errors and upload the cleaned data into S3. \"],\"answer\":\"Use the MSCK REPAIR TABLE command to update the metadata in the catalog. \",\"reason\":\"Explanation: The MSCK REPAIR TABLE command scans Amazon S3 for Hive compatible partitions that were added to the file system after the table was created. It compares the partitions in the table metadata and the partitions in S3. If new partitions are present in the S3 location that you specified when you created the table, it adds those partitions to the metadata and to the Athena table. MSK REPAIR TABLE can work better than DDL if have more than a few thousand partitions and DDL is facing timeout issues. \\n CORRECT : \\\"Use the MSCK REPAIR TABLE command to update the metadata in the catalog\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Use the ALTER TABLE ADD PARTITION command to update the column names\\\" is incorrect. This DDL command is used to add one or more partition columns. \\n INCORRECT : \\\"Export the data into DynamoDB to perform queries in a more flexible schema\\\" is incorrect. DynamoDB is a NoSQL table. \\n INCORRECT : \\\"Export the data into a JSON document to clean any errors and upload the cleaned data into S3\\\" is incorrect. This is not an efficient or effective way to reduce DDL time out issues. References: https://docs.aws.amazon.com/athena/latest/ug/msck-repair-table.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-athena/\"}",
    "{\"question\":\"An organization is launching a new service that will use an IoT device. How can secure communication protocols be established over the internet to ensure the security of the IoT devices during the launch?\",\"choices\":[\"Use AWS Certificate Manager (ACM) to provide TLS secured communications to IoT devices and deploy X. 509 certificates in the IoT environment. \",\"Use AWS Private Certificate Authority (CA) to provide TLS secured communications to the IoT devices and deploy X. 509 certificates in the IoT environment. \",\"Use IoT Greengrass to enable TLS secured communications to AWS from the IoT devices by issuing X. 509 certificates. \",\"Use IoT Core to provide TLS secured communications to AWS from the IoT devices by issuing X. 509 certificates. \"],\"answer\":\"Use AWS Certificate Manager (ACM) to provide TLS secured communications to IoT devices and deploy X. 509 certificates in the IoT environment. \",\"reason\":\"Explanation: AWS Certificate Manager (ACM) is used to provision X. 509 certificates for TLS/SSL secured communications. It can be used to create certificates for use with many AWS services and applications. It is compatible with IoT devices and applications such as IoT Core and IoT Greengrass. \\n CORRECT : \\\"Use AWS Certificate Manager (ACM) to provide TLS secured communications to IoT devices and deploy X. 509 certificates in the IoT environment\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Use AWS Private Certificate Authority (CA) to provide TLS secured communications to the IoT devices and deploy X. 509 certificates in the IoT environment\\\" is incorrect. AWS Private Certificate Authority cannot be used over the internet. \\n INCORRECT : \\\"Use IoT Greengrass to enable TLS secured communications to AWS from the IoT devices by issuing X. 509 certificates\\\" is incorrect. AWS IoT Greengrass is not a certificate authority. \\n INCORRECT : \\\"Use IoT Core to provide TLS secured communications to AWS from the IoT devices by issuing X. 509 certificates\\\" is incorrect. AWS IoT Core is not a certificate authority. References: https://docs.aws.amazon.com/iot/latest/developerguide/device-certs-your-own.html\"}",
    "{\"question\":\"A company is migrating to the AWS Cloud and needs to build a managed Public Key Infrastructure (PKI) using AWS services. The solution must support the following features:1. IAM integration. 2. Auditing with AWS CloudTrail. 3. Private certificates. 4. Subordinate certificate authorities (CAs). Which solution should the company use to meet these requirements?\",\"choices\":[\"AWS Certificate Manager. \",\"AWS Key Management Service. \",\"AWS Private Certificate Authority. \",\"AWS Secrets Manager. \"],\"answer\":\"AWS Private Certificate Authority. \",\"reason\":\"Explanation: An AWS Private CA hierarchy provides strong security and restrictive access controls for the most-trusted root CA at the top of the trust chain, while allowing more permissive access and bulk certificate issuance for subordinate CAs lower on the chain. With AWS Private CA, you can create private certificates to identify resources and protect data. You can create versatile certificate and CA configurations to identify and protect your resources, including servers, applications, users, devices, and containers. The service offers direct integration with AWS IAM, and you can control access to AWS Private CA with IAM policies. \\n CORRECT : \\\"AWS Private Certificate Authority\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"AWS Certificate Manager\\\" is incorrect. This service is used for issuing SSL/TLS certificates and is not suitable as a private CA for building a PKI with subordinate CAs. \\n INCORRECT : \\\"AWS Key Management Service\\\" is incorrect. This service is used to create and manage the encryption keys used for encrypting data at rest. \\n INCORRECT : \\\"AWS Secrets Manager\\\" is incorrect. This service is used for storing secret information such as database connections strings and passwords with API access. References: https://aws. amazon. com/private-ca/features/Save time with our AWS cheat sheets:https://digitalcloud.training/aws-certificate-manager/\"}",
    "{\"question\":\"An application must be refactored for the cloud. The application data is stored in an Amazon DynamoDB table and is processed by a Lambda function which prepares the data for analytics. The data processing currently takes place once a day, but the data analysts require it to be performed in near-real time. Which architecture pattern could be used to enable the data to be processed as it is received?\",\"choices\":[\"Use a microservices architecture. \",\"Use a fan-out architecture. \",\"Use an event-driven architecture. \",\"Use a scheduled architecture. \"],\"answer\":\"Use an event-driven architecture. \",\"reason\":\"Explanation: An event driven architecture will ensure that the records are processed as they are received. This can be achieved by creating a DynamoDB Stream for the existing table and then configuring the Lambda function to retrieve messages from the stream. This would be an event-driven architecture as the event (a record being written to the stream) causes the processing layer to do work. \\n CORRECT : \\\"Use an event-driven architecture\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Use a microservices architecture\\\" is incorrect. A microservices architecture is where you have many small individual components of an application that are loosely coupled. Though the architecture described may have components of a microservices architecture, this is not the defining characteristic that meets the requirement of near-real time processing. \\n INCORRECT : \\\"Use a fan-out architecture\\\" is incorrect. An example of a fan-out architecture is using SNS to send a single notification to many SQS queues. \\n INCORRECT : \\\"Use a scheduled architecture\\\" is incorrect. A scheduled architecture would not process events as they occur. It would only process them on a fixed schedule. References: https://aws. amazon. com/event-driven-architecture/\"}",
    "{\"question\":\"A Developer has created an Amazon S3 bucket and uploaded some objects that will be used for a publicly available static website. What steps MUST be performed to configure the bucket as a static website? (Select TWO. )\",\"choices\":[\"Upload an index and error document and enter the name of the index and error documents when enabling static website hosting\",\"Upload an index document and enter the name of the index document when enabling static website hosting\",\"Enable public access and grant everyone the s3:GetObject permissions\",\"Create an object access control list (ACL) granting READ permissions to the AllUsers group\",\"Upload a certificate from AWS Certificate Manager\"],\"answer\":[\"Upload an index document and enter the name of the index document when enabling static website hosting\",\"Enable public access and grant everyone the s3:GetObject permissions\"],\"reason\":\"Explanation: You can use Amazon S3 to host a static website. On a static website, individual webpages include static content. They might also contain client-side scripts. To host a static website on Amazon S3, you configure an Amazon S3 bucket for website hosting and then upload your website content to the bucket. When you configure a bucket as a static website, you enable static website hosting, set permissions, and add an index document. When you enable static website hosting for your bucket, you enter the name of the index document (for example, index.html). After you enable static website hosting for your bucket, you upload anhtml file with the index document name to your bucket. Note that an error document is optional. To provide permissions, it is necessary to disable “block public access” settings and then create a bucket policy that grants everyone the s3:GetObject permission. For example:{    \\\"Version\\\": \\\"2012-10-17\\\",    \\\"Statement\\\": [        {            \\\"Sid\\\": \\\"PublicReadGetObject\\\",            \\\"Effect\\\": \\\"Allow\\\",            \\\"Principal\\\": \\\"*\\\",            \\\"Action\\\": [                \\\"s3:GetObject\\\"           ],            \\\"Resource\\\": [                \\\"arn:aws:s3:::example. com/*\\\"            ]        }    ]}\\n CORRECT : \\\"Upload an index document and enter the name of the index document when enabling static website hosting\\\" is a correct answer. \\n CORRECT : \\\"Enable public access and grant everyone the s3:GetObject permissions\\\" is also a correct answer. \\n INCORRECT : \\\"Upload an index and error document and enter the name of the index and error documents when enabling static website hosting\\\" is incorrect as the error document is optional and the question specifically asks for the steps that MUST be completed. \\n INCORRECT : \\\"Create an object access control list (ACL) granting READ permissions to the AllUsers group\\\" is incorrect. This may be necessary if the bucket objects are not owned by the bucket owner but the question states that the Developer created the bucket and uploaded the objects and so must be the object owner. \\n INCORRECT : \\\"Upload a certificate from AWS Certificate Manager\\\" is incorrect as this is not supported or necessary for static websites on Amazon S3. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/WebsiteHosting.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-s3-and-glacier/\"}",
    "{\"question\":\"A new application will be hosted on the domain name dctlabs. com using an Amazon API Gateway REST API front end. The Developer needs to configure the API with a path to dctlabs. com/products that will be accessed using the HTTP GET verb. How MUST the Developer configure the API? (Select TWO. )\",\"choices\":[\"Create a /products method\",\"Create a /products resource\",\"Create a GET resource\",\"Create a GET method\",\"Create a /GET method\"],\"answer\":[\"Create a /products resource\",\"Create a GET method\"],\"reason\":\"Explanation: An API Gateway REST API is a collection of HTTP resources and methods that are integrated with backend HTTP endpoints, Lambda functions, or other AWS services. You can deploy this collection in one or more stages. Typically, API resources are organized in a resource tree according to the application logic. Each API resource can expose one or more API methods that have unique HTTP verbs supported by API Gateway. As you can see from the image above, the Developer would need to create a resource which in this case would be /products. The Developer would then create a GET method within the resource. \\n CORRECT : \\\"Create a /products resource\\\" is a correct answer. \\n CORRECT : \\\"Create a GET method\\\" is a correct answer. \\n INCORRECT : \\\"Create a /products method\\\" is incorrect as a resource should be created. \\n INCORRECT : \\\"Create a GET resource\\\" is incorrect as a method should be created. \\n INCORRECT : \\\"Create a /GET method\\\" is incorrect as a method is not preceded by a slash. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-basic-concept.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-api-gateway/\"}",
    "{\"question\":\"A static website that serves a collection of images runs from an Amazon S3 bucket in the us-east-1 region. The website is gaining in popularity and is now being viewed around the world. How can a Developer improve the performance of the website for global users?\",\"choices\":[\"Use cross region replication to replicate the bucket to several global regions\",\"Use Amazon S3 Transfer Acceleration to improve the performance of the website\",\"Use Amazon ElastiCache to cache the website content\",\"Use Amazon CloudFront to cache the website content\"],\"answer\":\"Use Amazon CloudFront to cache the website content\",\"reason\":\"Explanation: CloudFront is a web service that gives businesses and web application developers an easy and cost-effective way to distribute content with low latency and high data transfer speeds. CloudFront is a good choice for distribution of frequently accessed static content that benefits from edge delivery—like popular website images, videos, media files or software downloads. \\n CORRECT : \\\"Use Amazon CloudFront to cache the website content\\\" is the correct answer. \\n INCORRECT : \\\"Use Amazon ElastiCache to cache the website content\\\" is incorrect as ElastiCache is used for caching the contents of databases, not S3 buckets. \\n INCORRECT : \\\"Use cross region replication to replicate the bucket to several global regions\\\" is incorrect as though this would get the content closer to users it would not provide a mechanism for connecting to those copies. This could be achieved using Route 53 latency based routing however it would be easier to use CloudFront. \\n INCORRECT : \\\"Use Amazon S3 Transfer Acceleration to improve the performance of the website\\\" is incorrect as this service is used for improving the performance of uploads to Amazon S3. References: https://aws. amazon. com/blogs/networking-and-content-delivery/amazon-s3-amazon-cloudfront-a-match-made-in-the-cloud/https://aws. amazon. com/premiumsupport/knowledge-center/cloudfront-serve-static-website/ Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cloudfront/\"}",
    "{\"question\":\"Customers who use a REST API have reported performance issues. A Developer needs to measure the time between when API Gateway receives a request from a client and when it returns a response to the client. Which metric should the Developer monitor?\",\"choices\":[\"IntegrationLatency\",\"Latency\",\"CacheHitCount\",\"5XXError\"],\"answer\":\"Latency\",\"reason\":\"Explanation: The Latency metric measures the time between when API Gateway receives a request from a client and when it returns a response to the client. The latency includes the integration latency and other API Gateway overhead. \\n CORRECT : \\\"Latency\\\" is the correct answer. \\n INCORRECT : \\\"IntegrationLatency\\\" is incorrect. This measures the time between when API Gateway relays a request to the backend and when it receives a response from the backend. \\n INCORRECT : \\\"CacheHitCount\\\" is incorrect. This measures the number of requests served from the API cache in a given period. \\n INCORRECT : \\\"5XXError\\\" is incorrect. This measures the number of server-side errors captured in a given period. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/api-gateway-metrics-and-dimensions.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-api-gateway/https://digitalcloud.training/amazon-cloudwatch/\"}",
    "{\"question\":\"A Developer has deployed an application that runs on an Auto Scaling group of Amazon EC2 instances. The application data is stored in an Amazon DynamoDB table and records are constantly updated by all instances. An instance sometimes retrieves old data. The Developer wants to correct this by making sure the reads are strongly consistent. How can the Developer accomplish this?\",\"choices\":[\"Set ConsistentRead to true when calling GetItem\",\"Create a new DynamoDB Accelerator (DAX) table\",\"Set consistency to strong when calling UpdateTable\",\"Use the GetShardIterator command\"],\"answer\":\"Set ConsistentRead to true when calling GetItem\",\"reason\":\"Explanation: When you request a strongly consistent read, DynamoDB returns a response with the most up-to-date data, reflecting the updates from all prior write operations that were successful. The GetItem operation returns a set of attributes for the item with the given primary key. If there is no matching item, GetItem does not return any data and there will be no Item element in the response. GetItem provides an eventually consistent read by default. If your application requires a strongly consistent read, set ConsistentRead to true. Although a strongly consistent read might take more time than an eventually consistent read, it always returns the last updated value. Therefore, the Developer should set ConsistentRead to true when calling GetItem. \\n CORRECT : \\\"Set ConsistentRead to true when calling GetItem\\\" is the correct answer. \\n INCORRECT : \\\"Create a new DynamoDB Accelerator (DAX) table\\\" is incorrect as DAX is not used to enable strongly consistent reads. DAX is used for improving read performance as it caches data in an in-memory cache. \\n INCORRECT : \\\"Set consistency to strong when calling UpdateTable\\\" is incorrect as you cannot use this API action to configure consistency at a table level. \\n INCORRECT : \\\"Use the GetShardIterator command\\\" is incorrect as this is not related to DynamoDB, it is related to Amazon Kinesis. References: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_GetItem.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-dynamodb/\"}",
    "{\"question\":\"An organization has an account for each environment: Production, Testing, Development. A Developer with an IAM user in the Development account needs to launch resources in the Production and Testing accounts. What is the MOST efficient way to provide access?\",\"choices\":[\"Create a role with the required permissions in the Production and Testing accounts and have the Developer assume that role\",\"Create a separate IAM user in each account and have the Developer login separately to each account\",\"Create an IAM group in the Production and Testing accounts and add the Developer’s user from the Development account to the groups\",\"Create an IAM permissions policy in the Production and Testing accounts and reference the IAM user in the Development account\"],\"answer\":\"Create a role with the required permissions in the Production and Testing accounts and have the Developer assume that role\",\"reason\":\"Explanation: You can grant your IAM users’ permission to switch to roles within your AWS account or to roles defined in other AWS accounts that you own. This is known as cross-account access. In the image below a user in the Development account needs to access an S3 bucket in the Production account:The user is able to assume the role in the Production account and access the S3 bucket. This is more efficient than providing the user with multiple accounts. In this scenario the user requests to switch to the role through either the console or the API/CLI. \\n CORRECT : \\\"Create a role with the required permissions in the Production and Testing accounts and have the Developer assume that role\\\" is the correct answer. \\n INCORRECT : \\\"Create a separate IAM user in each account and have the Developer login separately to each account\\\" is incorrect as this is not the most efficient method of providing access. Cross-account access is preferred . \\n INCORRECT : \\\"Create an IAM group in the Production and Testing accounts and add the Developer’s user from the Development account to the groups\\\" is incorrect as you cannot add an IAM user from another AWS account to a group. \\n INCORRECT : \\\"Create an IAM permissions policy in the Production and Testing accounts and reference the IAM user in the Development account\\\" is incorrect as you cannot reference an IAM user from another AWS account in a permissions policy. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_common-scenarios_aws-accounts.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-iam/\"}",
    "{\"question\":\"An application running on Amazon EC2 generates a large number of small files (1KB each) containing personally identifiable information that must be converted to ciphertext. The data will be stored on a proprietary network-attached file system. What is the SAFEST way to encrypt the data using AWS KMS?\",\"choices\":[\"Create a data encryption key from a customer master key and encrypt the data with the data encryption key\",\"Encrypt the data directly with a customer managed customer master key\",\"Create a data encryption key from a customer master key and encrypt the data with the customer master key\",\"Encrypt the data directly with an AWS managed customer master key\"],\"answer\":\"Encrypt the data directly with a customer managed customer master key\",\"reason\":\"Explanation: With AWS KMS you can encrypt files directly with a customer master key (CMK). A CMK can encrypt up to 4KB (4096 bytes) of data in a single encrypt, decrypt, or reencrypt operation. As CMKs cannot be exported from KMS this is a very safe way to encrypt small amounts of data. Customer managed CMKs are CMKs in your AWS account that you create, own, and manage. You have full control over these CMKs, including establishing and maintaining their key policies, IAM policies, and grants, enabling and disabling them, rotating their cryptographic material, adding tags, creating aliases that refer to the CMK, and scheduling the CMKs for deletion. AWS managed CMKs are CMKs in your account that are created, managed, and used on your behalf by an AWS service that is integrated with AWS KMS. Some AWS services support only an AWS managed CMK. In this example the Amazon EC2 instance is saving files on a proprietary network-attached file system and this will not have support for AWS managed CMKs. Data keys are encryption keys that you can use to encrypt data, including large amounts of data and other data encryption keys. You can use AWS KMS CMKs to generate, encrypt, and decrypt data keys. However, AWS KMS does not store, manage, or track your data keys, or perform cryptographic operations with data keys. You must use and manage data keys outside of AWS KMS – this is potentially less secure as you need to manage the security of these keys. \\n CORRECT : \\\"Encrypt the data directly with a customer managed customer master key\\\" is the correct answer. \\n INCORRECT : \\\"Create a data encryption key from a customer master key and encrypt the data with the data encryption key\\\" is incorrect as this is not the most secure option here as you need to secure the data encryption key outside of KMS. It is also unwarranted as you can use a CMK directly to encrypt files up to 4KB in size. \\n INCORRECT : \\\"Create a data encryption key from a customer master key and encrypt the data with the customer master key\\\" is incorrect as the creation of the data encryption key is of no use here. It does not necessarily pose a security risk as the data key hasn’t been used (and you can use the CMK to encrypt the data), however this is not the correct process to follow. \\n INCORRECT : \\\"Encrypt the data directly with an AWS managed customer master key\\\" is incorrect as the network-attached file system is proprietary and therefore will not be supported by AWS managed CMKs. References: https://docs.aws.amazon.com/kms/latest/developerguide/concepts.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-kms/\"}",
    "{\"question\":\"A company is developing a new online game that will run on top of Amazon ECS. Four distinct Amazon ECS services will be part of the architecture, each requiring specific permissions to various AWS services. The company wants to optimize the use of the underlying Amazon EC2 instances by bin packing the containers based on memory reservation. Which configuration would allow the Development team to meet these requirements MOST securely?\",\"choices\":[\"Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances\",\"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS service to reference the associated IAM role\",\"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then, create an IAM group and configure the ECS cluster to reference that group\",\"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS task definition to reference the associated IAM role\"],\"answer\":\"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS task definition to reference the associated IAM role\",\"reason\":\"Explanation: With IAM roles for Amazon ECS tasks, you can specify an IAM role that can be used by the containers in a task. Applications must sign their AWS API requests with AWS credentials, and this feature provides a strategy for managing credentials for your applications to use, similar to the way that Amazon EC2 instance profiles provide credentials to EC2 instances. Instead of creating and distributing your AWS credentials to the containers or using the EC2 instance’s role, you can associate an IAM role with an ECS task definition or RunTask API operation. The applications in the task’s containers can then use the AWS SDK or CLI to make API requests to authorized AWS services. In this case each service requires access to different AWS services so following the principal of least privilege it is best to assign as a separate role to each task definition. \\n CORRECT : \\\"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS task definition to reference the associated IAM role\\\" is the correct answer. \\n INCORRECT : \\\"Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances\\\" is incorrect. It is a best practice to use IAM roles for tasks instead of assigning the roles to the container instances. \\n INCORRECT : \\\"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then configure each ECS service to reference the associated IAM role\\\" is incorrect as the reference should be made within the task definition. \\n INCORRECT : \\\"Create four distinct IAM roles, each containing the required permissions for the associated ECS services, then, create an IAM group and configure the ECS cluster to reference that group\\\" is incorrect as the reference should be made within the task definition. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-iam-roles.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-ecs-and-eks/\"}",
    "{\"question\":\"A Developer is creating a serverless application that will process sensitive data. The AWS Lambda function must encrypt all data that is written to /tmp storage at rest. How should the Developer encrypt this data?\",\"choices\":[\"Configure Lambda to use an AWS KMS customer managed customer master key (CMK). Use the CMK to generate a data key and encrypt all data prior to writing to /tmp storage. \",\"Attach the Lambda function to a VPC and encrypt Amazon EBS volumes at rest using the AWS managed CMK. Mount the EBS volume to /tmp. \",\"Enable default encryption on an Amazon S3 bucket using an AWS KMS customer managed customer master key (CMK). Mount the S3 bucket to /tmp. \",\"Enable secure connections over HTTPS for the AWS Lambda API endpoints using Transport Layer Security (TLS). \"],\"answer\":\"Configure Lambda to use an AWS KMS customer managed customer master key (CMK). Use the CMK to generate a data key and encrypt all data prior to writing to /tmp storage. \",\"reason\":\"Explanation: On a per-function basis, you can configure Lambda to use an encryption key that you create and manage in AWS Key Management Service. These are referred to as customer managed customer master keys (CMKs) or customer managed keys. If you don't configure a customer managed key, Lambda uses an AWS managed CMK named aws/lambda, which Lambda creates in your account. The CMK can be used to generate a data encryption key that can be used for encrypting all data uploaded to Lambda or generated by Lambda. \\n CORRECT : \\\"Configure Lambda to use an AWS KMS customer managed customer master key (CMK). Use the CMK to generate a data key and encrypt all data prior to writing to /tmp storage\\\" is the correct answer. \\n INCORRECT : \\\"Attach the Lambda function to a VPC and encrypt Amazon EBS volumes at rest using the AWS managed CMK. Mount the EBS volume to /tmp\\\" is incorrect. You cannot attach an EBS volume to a Lambda function. \\n INCORRECT : \\\"Enable default encryption on an Amazon S3 bucket using an AWS KMS customer managed customer master key (CMK). Mount the S3 bucket to /tmp\\\" is incorrect. You cannot mount an S3 bucket to a Lambda function. \\n INCORRECT : \\\"Enable secure connections over HTTPS for the AWS Lambda API endpoints using Transport Layer Security (TLS)\\\" is incorrect. The Lambda API endpoints are always encrypted using TLS and this is encryption in-transit not encryption at-rest. References: https://docs.aws.amazon.com/lambda/latest/dg/security-dataprotection.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-lambda/\"}",
    "{\"question\":\"An organization has a new containerized application that needs to be launched quickly. The team lead has stated to choose the option that would reduce the burden of infrastructure maintenance for the team. Which solution best meets this requirement?\",\"choices\":[\"Use AWS Copilot to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on Amazon Elastic Compute Cloud (EC2). \",\"Use AWS Fargate to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on Amazon Elastic Compute Cloud (EC2). \",\"Use Amazon Elastic Container Service (ECS) to provide a template that supports a quick launch of AWS Fargate applications on Amazon Elastic Compute Cloud (EC2). \",\"Use AWS Copilot to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on AWS Fargate. \"],\"answer\":\"Use AWS Copilot to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on AWS Fargate. \",\"reason\":\"Explanation: Copilot can run tasks and services on serverless infrastructures such as AWS Fargate. This can provide the support of containers using ECS and serverless services like Fargate does not require customers to manage. \\n CORRECT : “Use AWS Copilot to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on AWS Fargate” is the correct answer (as explained above. )\\n INCORRECT : \\\"Use AWS Copilot to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on Amazon Elastic Compute Cloud (EC2)\\\" is incorrect. EC2 is not a serverless service and would require additional maintenance and support from the development team. \\n INCORRECT : \\\"Use AWS Fargate to provide a template that supports a quick launch of Amazon Elastic Container Service (ECS) applications on Amazon Elastic Compute Cloud (EC2)\\\" is incorrect. EC2 is not a serverless service and would require additional maintenance and support from the development team. Fargate is a serverless compute engine. It would not provide a template to support a quick launch. \\n INCORRECT : \\\"Use Amazon Elastic Container Service (ECS) to provide a template that supports a quick launch of AWS Fargate applications on Amazon Elastic Compute Cloud (EC2)\\\" is incorrect. EC2 is not a serverless service and would require additional maintenance and support from the development team. ECS is a container orchestration service that would not provide a template. References: https://docs.aws.amazon.com/prescriptive-guidance/latest/patterns/deploy-a-clustered-application-to-amazon-ecs-by-using-aws-copilot.html\"}",
    "{\"question\":\"An organization is modernizing a software solution by migrating its database operations from Amazon EC2 instances to a serverless architecture. The software utilizes an Amazon RDS for PostgreSQL database and operates within a single VPC on AWS. Both the software and the database are currently deployed on a private subnet in the VPC. The organization needs to enable AWS Lambda functions to interact with the PostgreSQL database. Which solution would securely satisfy these needs?\",\"choices\":[\"Configure the Lambda functions to connect directly to the RDS instance by using its public IP address. \",\"Place the Lambda functions within the same VPC as the RDS instance and ensure they have an appropriate security group rule to access the RDS instance. \",\"Deploy an AWS AppRunner service in the VPC, which will host the Lambda functions and connect them to the RDS instance. \",\"Establish an AWS Direct Connect link between the Lambda functions and the RDS instance. \"],\"answer\":\"Place the Lambda functions within the same VPC as the RDS instance and ensure they have an appropriate security group rule to access the RDS instance. \",\"reason\":\"Explanation: Placing the Lambda functions within the same VPC as the RDS instance and ensuring they have an appropriate security group rule to access the RDS instance would allow the Lambda functions to interact with the PostgreSQL database without exposing the database to the public internet, keeping the operations secure and in line with the existing infrastructure. \\n CORRECT : \\\"Place the Lambda functions within the same VPC as the RDS instance and ensure they have an appropriate security group rule to access the RDS instance\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Configure the Lambda functions to connect directly to the RDS instance by using its public IP address\\\" is incorrect. Configuring the Lambda functions to connect directly to the RDS instance using its public IP address is insecure and goes against best practices for keeping resources in private subnets isolated. \\n INCORRECT : \\\"Deploy an AWS AppRunner service in the VPC, which will host the Lambda functions and connect them to the RDS instance\\\" is incorrect. AWS App Runner is primarily designed for building, deploying, and scaling containerized applications quickly, and is not suited to hosting Lambda functions or establishing connections with an RDS instance. \\n INCORRECT : \\\"Establish an AWS Direct Connect link between the Lambda functions and the RDS instance\\\" is incorrect. AWS Direct Connect is used for establishing a dedicated network connection from your premises to AWS and would not be applicable or necessary for Lambda functions trying to connect to an RDS instance within the same AWS environment. References: https://docs.aws.amazon.com/lambda/latest/dg/configuration-vpc.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-lambda/\"}",
    "{\"question\":\"A developer needs a long-term mountable storage solution for an Amazon Elastic Compute Cloud (EC2) instance using a compute optimized C6i instance type that meets heavily regulated compliance standards on data encryption for data at rest and in transit. What solution would provide this?\",\"choices\":[\"Attach an Amazon Elastic Block Store (EBS) volume to the instance and enable encryption during creation. \",\"Attach an Amazon Elastic Block Store (EBS) volume to the instance. Encryption is enabled automatically. \",\"Attach an AWS Simple Storage Service (S3) bucket to the instance and enable encryption during creation. \",\"Attach an AWS Simple Storage Service (S3) bucket to the instance. Encryption is enabled automatically. \"],\"answer\":\"Attach an Amazon Elastic Block Store (EBS) volume to the instance and enable encryption during creation. \",\"reason\":\"Explanation: Amazon EBS is permanent, mountable storage solution for Amazon EC2 instances. Amazon EBS volumes are not encrypted by default but can be encrypted for all current generation instance types and specific previous generation instance types. Amazon EBS volumes are not encrypted by default without additional configuration. \\n CORRECT : \\\"Attach an Amazon Elastic Block Store (EBS) volume to the instance and enable encryption during creation\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Attach an Amazon Elastic Block Store (EBS) volume to the instance. Encryption is enabled automatically\\\" is incorrect. Amazon EBS volumes can be configured to be encrypted upon creation or can be configured to encrypt new instances by default. AWS does not have Amazon EBS volumes encrypted by default. \\n INCORRECT : \\\"Attach an AWS Simple Storage Service (S3) bucket to the instance and enable encryption during creation\\\" is incorrect. Amazon S3 provides a block storage solution. It is not mountable to instances. In January 2023, AWS announced that new objects in AWS S3 would be encrypted by default. \\n INCORRECT :  Attach an AWS Simple Storage Service (S3) bucket to the instance. Encryption is enabled automatically\\\" is incorrect. Amazon S3 provide a block storage solution. It is not mountable to instances. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-ebs/\"}",
    "{\"question\":\"An organization needs to create a central file storage solution that scales on demand and can be used by multiple Amazon Elastic Compute Cloud (EC2) instances and AWS Lambda functions. Which storage solution will meet these requirements?\",\"choices\":[\"Create an Amazon ElastiCache cluster. \",\"Create an Amazon EBS Multi-Attach volume. \",\"Create an Amazon Elastic File System (EFS) file system. \",\"Create an AWS Storage Gateway volume gateway. \"],\"answer\":\"Create an Amazon Elastic File System (EFS) file system. \",\"reason\":\"Explanation: Amazon Elastic File System (EFS) provides a scalable storage solution that does not require provisioning. It can scale up and down based on files stored. It can be used as a file source for multiple computing services such as Amazon Elastic Compute Cloud (EC2), Amazon Elastic Container Service (ECS), AWS Lambda. \\n CORRECT : \\\"Create an Amazon Elastic File System (EFS) file system\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Create an Amazon ElastiCache cluster\\\" is incorrect. Amazon ElastiCache is a database caching service; it cannot be used for file storage. \\n INCORRECT : \\\"Create an Amazon EBS Multi-Attach volume\\\" is incorrect. Amazon EBS Multi-Attach Provisioned IOPS SSD can be used to attach a volume to multiple EC2 instances but not to other computing services such as Lambda. \\n INCORRECT : \\\"Create an AWS Storage Gateway volume gateway\\\" is incorrect. Amazon Storage Gateway is used for hybrid infrastructure set-ups that allows on-premises infrastructure to use AWS cloud storage. References: https://docs.aws.amazon.com/efs/latest/ug/whatisefs.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-efs/\"}",
    "{\"question\":\"An AWS Lambda function is being developed in a VPC. When a file is added to an Amazon S3 bucket, this Lambda function is triggered, processes the file, and logs the results into a file. These result and log files need to be accessible by other AWS services and on-premises resources. What should the developer use to meet these requirements?\",\"choices\":[\"Store the result and log files in Amazon S3 and append the new log entries to existing objects. \",\"Use Amazon DynamoDB to store the files and enable DynamoDB Streams to send notifications of changes. \",\"Keep the result and log files in Amazon Elastic File System (EFS) accessible by Lambda functions. \",\"Use AWS Glue to consolidate and catalog all result and log files and append log entries. \"],\"answer\":\"Keep the result and log files in Amazon Elastic File System (EFS) accessible by Lambda functions. \",\"reason\":\"Explanation: The correct solution is Amazon EFS as it provides a scalable, fully managed elastic NFS file system for AWS Cloud and on-premises resources. EFS can scale to petabytes without disrupting applications, making it suitable for Lambda functions to read/write large amounts of data. Furthermore, AWS Lambda can now mount an Amazon EFS file system, making it easy for Lambda functions to read and write large amounts of data or to share data across a fleet of functions. \\n CORRECT : \\\"Keep the result and log files in Amazon Elastic File System (EFS) accessible by Lambda functions\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Store the result and log files in Amazon S3 and append the new log entries to existing objects\\\" is incorrect. Amazon S3 does not support appending data to existing S3 objects. \\n INCORRECT : \\\"Use Amazon DynamoDB to store the files and enable DynamoDB Streams to send notifications of changes\\\" is incorrect. While Amazon DynamoDB can store data and Streams can notify services of changes, it is not designed for the type of file storage and sharing described in this scenario. \\n INCORRECT : \\\"Use AWS Glue to consolidate and catalog all result and log files and append log entries\\\" is incorrect. AWS Glue is a fully managed extract, transform, and load (ETL) service that makes it easy for users to prepare and load their data for analytics, but it is not used for direct file storage and sharing. References: https://docs.aws.amazon.com/lambda/latest/dg/services-efs.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-efs/\"}",
    "{\"question\":\"A developer is configuring health checks using Amazon Route 53 and needs to set values to determine the health of critical endpoints. What is the parameter that Amazon Route 53 reviews before deciding if an endpoint is unhealthy?\",\"choices\":[\"failure threshold. \",\"latency. \",\"fault tolerance. \",\"network response. \"],\"answer\":\"failure threshold. \",\"reason\":\"Explanation: The failure threshold is specified by the AWS customer. A failure is when the endpoint does not respond to a request. \\n CORRECT : \\\"failure threshold\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"latency\\\" is incorrect. Latency is not a parameter that Route 53 uses to determine if an endpoint is healthy. \\n INCORRECT : \\\"fault tolerance\\\" is incorrect. Fault tolerance is not a parameter that Route 53 uses to determine if an endpoint is healthy. \\n INCORRECT : \\\"network response\\\" is incorrect. Network response is not a parameter that Route 53 uses to determine if an endpoint is healthy. References: https://docs.aws.amazon.com/Route53/latest/DeveloperGuide/welcome-health-checks.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-route-53/\"}",
    "{\"question\":\"A development team is working with Amazon MemoryDB for Redis. The team lead wants to limit the default access of the service-linked role and implement a custom-managed policy instead. What needs to be done prior to making this change?\",\"choices\":[\"The team lead would have to login as the root user to change the permission setting. \",\"The team lead would need permission from an AWS Technical Account Manager. \",\"The team lead would need to have permission to call `iam:createServiceLinkedRole`. \",\"The team lead would edit the default policy named `AmazonMemoryDBFullAccess`. \"],\"answer\":\"The team lead would need to have permission to call `iam:createServiceLinkedRole`. \",\"reason\":\"Explanation: Service-linked roles are tied to the resource with predefined permissions. The default policy associated Amazon MemoryDB for Redis is “AmazonMemoryDBFullAccess”. This comes pre-provisioned with permission that the service requires to create a service link that is used to create resources and access other AWS resources and services. You might decide not to use the default policy and instead to use a custom-managed policy. In this case, make sure that you have either permissions to call iam:createServiceLinkedRole or that you have created the MemoryDB service-linked role. \\n CORRECT : \\\"The team lead would need to have permission to call `iam:createServiceLinkedRole`\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"The team lead would have to login as the root user to change the permission setting\\\" is incorrect. The team lead may not be the root user. Logging in and accessing the root user should be limited to the tasks that are necessary and limited to the root user. A Cloud Administrator, for example, could update the IAM roles needed for the team lead to make the appropriate service-linked policy changes. \\n INCORRECT : \\\"The team lead would need permission from an AWS Technical Account Manager\\\" is incorrect. There are some changes to an account or services that an organization would need to contact their technical account manager to change. An update to the IAM policy or changing a service-linked role is not one of them. \\n INCORRECT : \\\"The team lead would edit the default policy named `AmazonMemoryDBFullAccess`\\\" is incorrect. AWS managed policies cannot be edited. A custom service policy would need to create and attached with the appropriate IAM role. References: https://docs.aws.amazon.com/memorydb/latest/devguide/set-up.html\"}",
    "{\"question\":\"A developer is using AWS AppConfig to deploy an application. The application's data needs to be encrypted when at rest. Which statement describes how this requirement is met?\",\"choices\":[\"Data is automatically encrypted using AWS owned keys. This cannot be disabled. The developer can add an additional layer of encryption using customer managed keys. \",\"Data is automatically encrypted using AWS owned keys and AWS Key Management Service (KMS). The developer can disable this layer and instead add their own customer managed key. \",\"Data is stored in S3 buckets unencrypted by default. The developer can use AWS Key Management Service (KMS) to apply encryption keys to the data at rest. \",\"Data is unencrypted by default. The developer can choose to use AWS managed keys or customer managed keys to encrypt the data. \"],\"answer\":\"Data is automatically encrypted using AWS owned keys. This cannot be disabled. The developer can add an additional layer of encryption using customer managed keys. \",\"reason\":\"Explanation: AWS AppConfig will automatically encrypt data at rest using AWS owned keys and AWS Key Management Service (KMS). This layer cannot be disabled or altered by the customer. The customer can add a second layer of encryption protection that they can control and manage using customer managed keys. \\n CORRECT : \\\"Data is automatically encrypted using AWS owned keys. This cannot be disabled. The developer can add an additional layer of encryption using customer managed keys\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Data is automatically encrypted using AWS owned keys and AWS Key Management Service (KMS). The developer can disable this layer and instead add their own customer managed key\\\" is incorrect. The default layer of encryption cannot be disabled. The developer can add a second layer of a customer managed key. \\n INCORRECT : \\\"Data is stored in S3 buckets unencrypted by default. The developer can use AWS Key Management Service (KMS) to apply encryption keys to the data at rest\\\" is incorrect. Data is encrypted by default using AWS owned keys. The developer can use customer managed keys to add a second layer of encryption. \\n INCORRECT : \\\"Data is unencrypted by default. The developer can choose to use AWS managed keys or customer managed keys to encrypt the data\\\" is incorrect. Data is encrypted by default using AWS managed keys and AWS Key Management Service (KMS). A second layer can be added using customer managed keys. References: https://docs.aws.amazon.com/appconfig/latest/userguide/appconfig-security.html\"}",
    "{\"question\":\"A junior developer has been assigned the project of making updates to a current application using AWS Cloud Development Kit (CDK). How can the developer revert to a previous version if there is an error?\",\"choices\":[\"Only make changes in the deployment phase, and never during synthesis. \",\"Once CloudFormation has been installed, it can be used to revert to previous versions. \",\"Once CloudFormation has been added to the application, it can be used to manage the scope. \",\"Once CloudFormation has been installed, it can be used to track changes in application stacks. \"],\"answer\":\"Only make changes in the deployment phase, and never during synthesis. \",\"reason\":\"Explanation: Synthesis is the process of converting AWS CDK stacks to AWS CloudFormation templates and assets. Changes should only be during the deployment phase and after AWS CloudFormation template has been created. This will allow it to roll back the change if there are any problems. \\n CORRECT : \\\"Only make changes in the deployment phase, and never during synthesis\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Once CloudFormation has been installed, it can be used to revert to previous versions\\\" is incorrect. The CDK Toolkit already provides the ability to convert CDK stacks into CloudFormation templates. There is no need to install CloudFormation. \\n INCORRECT : \\\"Once CloudFormation has been added to the application, it can be used to manage the scope\\\" is incorrect. The CDK Toolkit already provides the ability to convert CDK stacks into CloudFormation templates.  It is not used to manage the scope. \\n INCORRECT : \\\"Once synthesis is complete, it can be used to track drift changes\\\" is incorrect. CloudFormation can detect drift changes if changes are made outside of CloudFormation management. Synthesis is the process of converting AWS CDK stacks to AWS CloudFormation templates. References: https://docs.aws.amazon.com/cdk/v2/guide/best-practices.html\"}",
    "{\"question\":\"A developer received the following error message during an AWS CloudFormation deployment:DELETE_FAILED (The following resource(s) failed to delete: (sg-11223344). )Which action should the developer take to resolve this error?\",\"choices\":[\"Add a DependsOn attribute to the sg-11223344 resource in the CloudFormation template. Then delete the stack. \",\"Manually delete the security group. Then execute a change set to force deletion of the CloudFormation stack. \",\"Update the logical ID of the security group resource with the security groups ARN. Then delete the stack. \",\"Modify the CloudFormation template to retain the security group resource. Then manually delete the resource after deployment. \"],\"answer\":\"Modify the CloudFormation template to retain the security group resource. Then manually delete the resource after deployment. \",\"reason\":\"Explanation: The stack may be stuck in the DELETE_FAILED state because the dependent object (security group), can't be deleted. This can be for many reasons, for example, the security group could have an ENI attached that’s not part of the CloudFormation stack. To delete the stack you must choose to delete the stack in the console and then select to retain the resource(s) that failed to delete.  This can also be achieved from the AWS CLI:\\n CORRECT : \\\"Modify the CloudFormation template to retain the security group resource. Then manually delete the resource after deployment\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Add a DependsOn attribute to the sg-11223344 resource in the CloudFormation template. Then delete the stack\\\" is incorrect. This creates a dependency for stack creation. It does not assist with resolving the issue that is preventing the stack from deleting successfully. \\n INCORRECT : \\\"Manually delete the security group. Then execute a change set to force deletion of the CloudFormation stack\\\" is incorrect. You can manually delete the security group. However, you would not then use a change set to continue with the deletion. You would instead simply choose to delete the stack from the console or the CLI. \\n INCORRECT : \\\"Update the logical ID of the security group resource with the security groups ARN. Then delete the stack\\\" is incorrect. The issue has nothing to do with logical IDs or ARNs. The resource cannot be deleted by CloudFormation so the developer simply needs to choose to retain the resource before continuing with the stack deletion process. References: https://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/using-cfn-updating-stacks-changesets.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-cloudformation/\"}",
    "{\"question\":\"A company has an application that provides access to objects in Amazon S3 based on the type of user. The user types are registered user and guest user. The company has 30,000 users. Information is read from an S3 bucket depending on the user type. Which approaches are recommended to provide access to both user types MOST efficiently? (Select TWO. )\",\"choices\":[\"Store separate access keys in the application code for registered users and guest users to provide access to the objects. \",\"Use S3 bucket policies to restrict read access to specific IAM users. \",\"Use Amazon Cognito to provide access using authenticated and unauthenticated roles. \",\"Create a new IAM user for each user and grant access to the S3 objects. \",\"Use the AWS IAM service and let the application assume different roles depending on the type of user. \"],\"answer\":[\"Use Amazon Cognito to provide access using authenticated and unauthenticated roles. \",\"Use the AWS IAM service and let the application assume different roles depending on the type of user. \"],\"reason\":\"Explanation: Amazon Cognito can be used with identity pools. A Cognito identity pool supports both authenticated and unauthenticated identities. Authenticated identities belong to users who are authenticated by any supported identity provider. Unauthenticated identities typically belong to guest users. The most secure way of using the IAM service for this solution would be to use separate roles. IAM roles can be securely assumed based on the type of user. Each role can be configured with different permission sets as applicable to registered and guest users. \\n CORRECT : \\\"Use Amazon Cognito to provide access using authenticated and unauthenticated roles\\\" is a correct answer (as explained above. )\\n CORRECT : \\\"Use the AWS IAM service and let the application assume different roles depending on the type of user\\\" is also a correct answer (as explained above. )\\n INCORRECT : \\\"Store separate access keys in the application code for registered users and guest users to provide access to the objects\\\" is incorrect. This is highly insecure. You should avoid embedding access keys in application code and use IAM roles instead. \\n INCORRECT : \\\"Create a new IAM user for each user and grant access to the S3 objects\\\" is incorrect. This would be a lot of users and is an inefficient solution. \\n INCORRECT : \\\"Use S3 bucket policies to restrict read access to specific IAM users\\\" is incorrect. This would also be highly complex with so many users and would need constant updating when users need to be added or removed. References: https://docs.aws.amazon.com/cognito/latest/developerguide/identity-pools.htmlhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-iam/https://digitalcloud.training/amazon-cognito/\"}",
    "{\"question\":\"A company has deployed a new web application that uses Amazon Cognito for authentication. The company wants to allow sign-in from any source but wants to automatically block all sign-in attempts if the risk level is elevated. Which Amazon Cognito feature will meet these requirements?\",\"choices\":[\"Advanced security metrics. \",\"Multi-factor authentication (MFA). \",\"Adaptive authentication. \",\"Case sensitive user pools. \"],\"answer\":\"Adaptive authentication. \",\"reason\":\"Explanation: With adaptive authentication, you can configure your user pool to block suspicious sign-ins or add second factor authentication in response to an increased risk level. For each sign-in attempt, Amazon Cognito generates a risk score for how likely the sign-in request is to be from a compromised source. This risk score is based on many factors, including whether it detects a new device, user location, or IP address. For each risk level, you can choose from the following options:Allow - Users can sign in without an additional factor. Optional MFA - Users who have a second factor configured must complete a second factor challenge to sign in. Require MFA - Users who have a second factor configured must complete a second factor challenge to sign in. Amazon Cognito blocks sign-in for users who don't have a second factor configured. Block - Amazon Cognito blocks all sign-in attempts at the designated risk level. In this case the company should use adaptive authentication and configure Cognito to block sign-in attempts at the specific risk level they feel is appropriate. \\n CORRECT : \\\"Adaptive authentication\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Advanced security metrics\\\" is incorrect. Amazon Cognito publishes sign-in attempts, their risk levels, and failed challenges to Amazon CloudWatch. These are known as advanced security metrics. This information is useful for analysis, but adaptive authentication is required to automatically block sign-in attempts. \\n INCORRECT : \\\"Multi-factor authentication (MFA)\\\" is incorrect. This is not a method of blocking. In this case adaptive authentication with a block response should be configured. \\n INCORRECT : \\\"Case sensitive user pools\\\" is incorrect. This has nothing to do with responding to security threats. This is a configuration that determines whether Cognito considers the case of email addresses and usernames. References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-pool-settings-adaptive-authentication.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cognito/\"}",
    "{\"question\":\"A start-up organization imported their X. 509 certificate from another issuer into AWS Certificate Manager (ACM) approximately 11 months ago. What needs to be done to ensure that visitors will continue to have secure access to the website? (Select TWO. )\",\"choices\":[\"A new certificate will need to be requested from ACM. \",\"A new certificate will need to be imported into ACM. \",\"A new certificate will automatically be created prior to the certificate expiring at 12 months. \",\"A new certificate will automatically be created prior to the certificate expiring at 13 months. \",\"A new certificate will be automatically imported into ACM. \"],\"answer\":[\"A new certificate will need to be requested from ACM. \",\"A new certificate will need to be imported into ACM. \"],\"reason\":\"Explanation: AWS ACM issued certificates are valid for 13 months. They are also renewed automatically. Imported certificates are not automatically renewed and would need to be imported once created from the third party. \\n CORRECT : \\\"A new certificate will need to be requested from ACM\\\" is the correct answer (as explained above. )\\n CORRECT : \\\"A new certificate will need to be imported into ACM” is also a correct answer (as explained above. )\\n INCORRECT : \\\"A new certificate will automatically be created prior to the certificate expiring at 12 months\\\" is incorrect. Imported certificates are not automatically renewed by AWS ACM. \\n INCORRECT : \\\"A new certificate will automatically be created prior to the certificate expiring at 13 months\\\" is incorrect. Imported certificates are not automatically renewed by AWS ACM. \\n INCORRECT : \\\"A new certificate will be automatically imported into ACM\\\" is incorrect. A new certificate issued by a third-party can be imported but it is not automatically done. References: https://docs.aws.amazon.com/acm/latest/userguide/managed-renewal.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-certificate-manager/\"}",
    "{\"question\":\"A developer is planning the deployment of a new version of an application to AWS Elastic Beanstalk. The new version of the application should be deployed only to new EC2 instances. Which deployment methods will meet these requirements? (Select TWO. )\",\"choices\":[\"Rolling with additional batch\",\"Immutable\",\"Rolling\",\"All at once\",\"Blue/green\"],\"answer\":[\"Immutable\",\"Blue/green\"],\"reason\":\"Explanation: AWS Elastic Beanstalk provides several options for how deployments are processed, including deployment policies and options that let you configure batch size and health check behavior during deployments. All at once:Deploys the new version to all instances simultaneously. Rolling:Update a few instances at a time (bucket), and then move onto the next bucket once the first bucket is healthy (downtime for 1 bucket at a time). Rolling with additional batch:Like Rolling but launches new instances in a batch ensuring  that there is full availability. Immutable:Launches new instances in a new ASG and deploys the version update to these instances before swapping traffic to these instances once healthy. Zero downtime. Blue / Green deployment:Zero downtime and release facility. Create a new “stage” environment and deploy updates there. The immutable and blue/green options both provide zero downtime as they will deploy the new version to a new version of the application. These are also the only two options that will ONLY deploy the updates to new EC2 instances. \\n CORRECT : \\\"Immutable\\\" is the correct answer. \\n CORRECT : \\\"Blue/green\\\" is the correct answer. \\n INCORRECT : \\\"All-at-once\\\" is incorrect as this will deploy the updates to existing instances. \\n INCORRECT : \\\"Rolling\\\" is incorrect as this will deploy the updates to existing instances. \\n INCORRECT : \\\"Rolling with additional batch\\\" is incorrect as this will launch new instances but will also update the existing instances as well (which is not allowed according to the requirements). References: https://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features. rolling-version-deploy.htmlhttps://docs.aws.amazon.com/elasticbeanstalk/latest/dg/using-features. deploy-existing-version.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-elastic-beanstalk/\"}",
    "{\"question\":\"A Developer is creating an AWS Lambda function that will process data from an Amazon Kinesis data stream. The function is expected to be invoked 50 times per second and take 100 seconds to complete each request. What MUST the Developer do to ensure the functions runs without errors?\",\"choices\":[\"Contact AWS and request to increase the limit for concurrent executions\",\"No action is required as AWS Lambda can easily accommodate this requirement\",\"Increase the concurrency limit for the function\",\"Implement exponential backoff in the function code\"],\"answer\":\"Contact AWS and request to increase the limit for concurrent executions\",\"reason\":\"Explanation: Concurrency is the number of requests that your function is serving at any given time. When your function is invoked, Lambda allocates an instance of it to process the event. When the function code finishes running, it can handle another request. If the function is invoked again while a request is still being processed, another instance is allocated, which increases the function's concurrency. Concurrency is subject to a Regional limit that is shared by all functions in a Region. For an initial burst of traffic, your functions' cumulative concurrency in a Region can reach an initial level of between 500 and 3000, which varies per Region:3000 – US West (Oregon), US East (N. Virginia), Europe (Ireland)1000 – Asia Pacific (Tokyo), Europe (Frankfurt)500 – Other RegionsAfter the initial burst, your functions' concurrency can scale by an additional 500 instances each minute. This continues until there are enough instances to serve all requests, or until a concurrency limit is reached. When requests come in faster than your function can scale, or when your function is at maximum concurrency, additional requests fail with a throttling error (429 status code). The function continues to scale until the account's concurrency limit for the function's Region is reached. The function catches up to demand, requests subside, and unused instances of the function are stopped after being idle for some time. Unused instances are frozen while they're waiting for requests and don't incur any charges. The regional concurrency limit starts at 1,000. You can increase the limit by submitting a request in the Support Center console. Calculating concurrency requirements for this scenarioTo calculate the concurrency requirements for this scenario, simply multiply the invocation requests per second (50) with the average execution time in seconds (100). This calculation is 50 x 100 = 5,000. Therefore, 5,000 concurrent executions is over the default limit and the Developer will need to request in the AWS Support Center console. \\n CORRECT : \\\"Contact AWS and request to increase the limit for concurrent executions\\\" is the correct answer. \\n INCORRECT : \\\"No action is required as AWS Lambda can easily accommodate this requirement\\\" is incorrect as by default the AWS account will be limited. Lambda can easily scale to this level of demand however the account limits must first be increased. \\n INCORRECT : \\\"Increase the concurrency limit for the function\\\" is incorrect as the default account limit of 1,000 concurrent executions will mean you can only assign up to 900 executions to the function (100 must be left unreserved). This is insufficient for this requirement to the account limit must be increased. \\n INCORRECT : \\\"Implement exponential backoff in the function code\\\" is incorrect. Exponential backoff means configuring the application to wait longer between API calls, slowing the demand. However, this is not a good resolution to this issue as it will have negative effects on the application. The correct choice is to raise the account limits so the function can concurrently execute according to its requirements. References: https://docs.aws.amazon.com/lambda/latest/dg/invocation-scaling.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-lambda/\"}",
    "{\"question\":\"An application is being migrated into the cloud. The application is stateless and will run on a fleet of Amazon EC2 instances. The application should scale elastically. How can a Developer ensure that the number of instances available is sufficient for current demand?\",\"choices\":[\"Create a launch configuration and use Amazon CodeDeploy\",\"Create a task definition and use an Amazon ECS cluster\",\"Create a launch configuration and use Amazon EC2 Auto Scaling\",\"Create a task definition and use an AWS Fargate cluster\"],\"answer\":\"Create a launch configuration and use Amazon EC2 Auto Scaling\",\"reason\":\"Explanation: Amazon EC2 Auto Scaling helps you maintain application availability and allows you to automatically add or remove EC2 instances according to conditions you define. You can use the fleet management features of EC2 Auto Scaling to maintain the health and availability of your fleet. You can also use the dynamic and predictive scaling features of EC2 Auto Scaling to add or remove EC2 instances. Dynamic scaling responds to changing demand and predictive scaling automatically schedules the right number of EC2 instances based on predicted demand. Dynamic scaling and predictive scaling can be used together to scale faster. A launch configuration is an instance configuration template that an Auto Scaling group uses to launch EC2 instances. When you create a launch configuration, you specify information for the instances. Include the ID of the Amazon Machine Image (AMI), the instance type, a key pair, one or more security groups, and a block device mapping. If you've launched an EC2 instance before, you specified the same information in order to launch the instance. You can specify your launch configuration with multiple Auto Scaling groups. However, you can only specify one launch configuration for an Auto Scaling group at a time, and you can't modify a launch configuration after you've created it. To change the launch configuration for an Auto Scaling group, you must create a launch configuration and then update your Auto Scaling group with it. Therefore, the Developer should create a launch configuration and use Amazon EC2 Auto Scaling. \\n CORRECT : \\\"Create a launch configuration and use Amazon EC2 Auto Scaling\\\" is the correct answer. \\n INCORRECT : \\\"Create a launch configuration and use Amazon CodeDeploy\\\" is incorrect as CodeDeploy is not used for auto scaling of Amazon EC2 instances. \\n INCORRECT : \\\"Create a task definition and use an Amazon ECS cluster\\\" is incorrect as the migrated application will be running on Amazon EC2 instances, not containers. \\n INCORRECT : \\\"Create a task definition and use an AWS Fargate cluster\\\" is incorrect as the migrated application will be running on Amazon EC2 instances, not containers. References: https://docs.aws.amazon.com/autoscaling/ec2/userguide/what-is-amazon-ec2-auto-scaling.htmlhttps://docs.aws.amazon.com/autoscaling/ec2/userguide/LaunchConfiguration.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-ec2-auto-scaling/\"}",
    "{\"question\":\"AWS CodeBuild builds code for an application, creates a Docker image, pushes the image to Amazon Elastic Container Registry (ECR), and tags the image with a unique identifier. If the Developers already have AWS CLI configured on their workstations, how can the Docker images be pulled to the workstations?\",\"choices\":[\"Run the following: docker pull REPOSITORY URI : TAG\",\"Run the output of the following: aws ecr get-login-password, and then run docker pull REPOSITORY URI : TAG\",\"Run the following: aws ecr get-login-password, and then run: docker pull REPOSITORY URI : TAG\",\"Run the output of the following: aws ecr get-download-url-for-layer, and then run docker pull REPOSITORY URI : TAG\"],\"answer\":\"Run the output of the following: aws ecr get-login-password, and then run docker pull REPOSITORY URI : TAG\",\"reason\":\"Explanation: If you would like to run a Docker image that is available in Amazon ECR, you can pull it to your local environment with the docker pull command. You can do this from either your default registry or from a registry associated with another AWS account. Docker CLI does not support standard AWS authentication methods, so client authentication must be handled so that ECR knows who is requesting to push or pull an image. To do this you can issue the aws ecr get-login-password AWS CLI command and then use the output to login using docker login and then issue a docker pull command specifying the image name using registry/repository[:tag]. \\n CORRECT : \\\"Run the output of the following: aws ecr get-login-password, and then run docker pull REPOSITORY URI : TAG\\\" is the correct answer. \\n INCORRECT : \\\"Run the following: docker pull REPOSITORY URI : TAG\\\" is incorrect as the Developers first need to authenticate before they can pull the image. \\n INCORRECT : \\\"Run the following: aws ecr get-login-password, and then run: docker pull REPOSITORY URI : TAG\\\" is incorrect. The Developers need to not just run the login command but run the output of the login command which contains the authentication token required to log in. \\n INCORRECT : \\\"Run the output of the following: aws ecr get-download-url-for-layer, and then run docker pull REPOSITORY URI : TAG\\\" is incorrect as this command retrieves a pre-signed Amazon S3 download URL corresponding to an image layer. References: https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html#registry_authhttps://docs.aws.amazon.com/AmazonECR/latest/userguide/docker-pull-ecr-image.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-ecs-and-eks/\"}",
    "{\"question\":\"A Developer is creating a DynamoDB table for storing application logs. The table has 5 write capacity units (WCUs). The Developer needs to configure the read capacity units (RCUs) for the table. Which of the following configurations represents the most efficient use of throughput?\",\"choices\":[\"Eventually consistent reads of 5 RCUs reading items that are 4 KB in size\",\"Strongly consistent reads of 5 RCUs reading items that are 4 KB in size\",\"Eventually consistent reads of 15 RCUs reading items that are 1 KB in size\",\"Strongly consistent reads of 15 RCUs reading items that are 1KB in size\"],\"answer\":\"Eventually consistent reads of 15 RCUs reading items that are 1 KB in size\",\"reason\":\"Explanation: \\n\\nAmong these\\noptions, the most cost-efficient in terms of maximizing the number of read\\noperations per RCU is:\\n\\n·       \\nEventually consistent reads\\nof 15 RCUs reading items that are 1 KB in size. \\n\\nThis choice\\nutilizes the RCU allocation most effectively because:\\n\\n·       \\nIt\\nleverages the lower RCU consumption of eventually consistent reads. \\n\\n·       \\nIt\\ntakes advantage of the smaller item size (1 KB), which requires fewer RCUs for\\nthe same number of operations compared to items up to 4 KB. \\n\\nCORRECT: \\\"Strongly consistent reads of 15 RCUs reading items that are 1KB in\\nsize\\\" is correct as described above. \\n\\nINCORRECT: \\\"Eventually consistent reads of 15 RCUs reading items that are 1\\nKB in size\\\" is incorrect as described above. \\n\\nINCORRECT: \\\"Strongly consistent reads of 5 RCUs reading items that are 4 KB\\nin size\\\" is incorrect as described above. \\n\\nINCORRECT: \\\"Eventually consistent reads of 5 RCUs reading items that are 4 KB\\nin size\\\" is incorrect as described above. \\n\\nReferences:\\n\\nhttps://docs. amazonaws. cn/en_us/amazondynamodb/latest/developerguide/ProvisionedThroughput.html\\n\\nSave\\ntime with our AWS cheat sheets:\\n\\nhttps://digitalcloud.training/amazon-dynamodb/\"}",
    "{\"question\":\"An application has been instrumented to use the AWS X-Ray SDK to collect data about the requests the application serves. The Developer has set the user field on segments to a string that identifies the user who sent the request. How can the Developer search for segments associated with specific users?\",\"choices\":[\"Use a filter expression to search for the user field in the segment metadata\",\"By using the GetTraceSummaries API with a filter expression\",\"By using the GetTraceGraph API with a filter expression\",\"Use a filter expression to search for the user field in the segment annotations\"],\"answer\":\"By using the GetTraceSummaries API with a filter expression\",\"reason\":\"Explanation: A segment document conveys information about a segment to X-Ray. A segment document can be up to 64 kB and contain a whole segment with subsegments, a fragment of a segment that indicates that a request is in progress, or a single subsegment that is sent separately. You can send segment documents directly to X-Ray by using the PutTraceSegments API. Example minimally complete segment:{  \\\"name\\\" : \\\"example. com\\\",  \\\"id\\\" : \\\"70de5b6f19ff9a0a\\\",  \\\"start_time\\\" : 1. 478293361271E9,  \\\"trace_id\\\" : \\\"1-581cf771-a006649127e371903a2de979\\\",  \\\"end_time\\\" : 1. 478293361449E9} A subset of segment fields are indexed by X-Ray for use with filter expressions. For example, if you set the user field on a segment to a unique identifier, you can search for segments associated with specific users in the X-Ray console or by using the GetTraceSummaries API. \\n CORRECT : \\\"By using the GetTraceSummaries API with a filter expression\\\" is the correct answer. \\n INCORRECT : \\\"By using the GetTraceGraph API with a filter expression\\\" is incorrect as this API action retrieves a service graph for one or more specific trace IDs. \\n INCORRECT : \\\"Use a filter expression to search for the user field in the segment metadata\\\" is incorrect as the user field is not part of the segment metadata and metadata is not is not indexed for search. \\n INCORRECT : \\\"Use a filter expression to search for the user field in the segment annotations\\\" is incorrect as the user field is not part of the segment annotations. References: https://docs.aws.amazon.com/xray/latest/devguide/xray-api-segmentdocuments.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-developer-tools/\"}",
    "{\"question\":\"A Development team has deployed several applications running on an Auto Scaling fleet of Amazon EC2 instances. The Operations team have asked for a display that shows a key performance metric for each application on a single screen for monitoring purposes. What steps should a Developer take to deliver this capability using Amazon CloudWatch?\",\"choices\":[\"Create a custom namespace with a unique metric name for each application\",\"Create a custom dimension with a unique metric name for each application\",\"Create a custom event with a unique metric name for each application\",\"Create a custom alarm with a unique metric name for each application\"],\"answer\":\"Create a custom namespace with a unique metric name for each application\",\"reason\":\"Explanation: A namespace is a container for CloudWatch metrics. Metrics in different namespaces are isolated from each other, so that metrics from different applications are not mistakenly aggregated into the same statistics. Therefore, the Developer should create a custom namespace with a unique metric name for each application. This namespace will then allow the metrics for each individual application to be shown in a single view through CloudWatch. \\n CORRECT : \\\"Create a custom namespace with a unique metric name for each application\\\" is the correct answer. \\n INCORRECT : \\\"Create a custom dimension with a unique metric name for each application\\\" is incorrect as a dimension further clarifies what a metric is and what data it stores. \\n INCORRECT : \\\"Create a custom event with a unique metric name for each application\\\" is incorrect as an event is not used to organize metrics for display. \\n INCORRECT : \\\"Create a custom alarm with a unique metric name for each application\\\" is incorrect as alarms are used to trigger actions when a threshold is reached, this is not relevant to organizing metrics for display. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/cloudwatch_concepts.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cloudwatch/\"}",
    "{\"question\":\"An engineer is constructing an AWS Lambda function and intends to log specific key events that transpire during the function's execution. To correlate the events with a particular function invocation, the engineer is looking to incorporate a unique identifier. The following code segment has been added to the Lambda function:function handler (event, context) {}\",\"choices\":[\"Use context. awsRequestId within the function to fetch the unique identifier associated with each invocation. \",\"Use event. requestId to obtain the unique identifier for each function execution. \",\"Use context. invocationId to extract the unique identifier tied to each function run. \",\"Use context. lambdaId to get the unique identifier corresponding to each function invocation. \"],\"answer\":\"Use context. awsRequestId within the function to fetch the unique identifier associated with each invocation. \",\"reason\":\"Explanation: The context object in a Lambda function provides metadata about the function and the current invocation, including a unique identifier for the request, awsRequestId, which can be used to correlate logs from a specific invocation. \\n CORRECT : \\\"Use context. awsRequestId within the function to fetch the unique identifier associated with each invocation\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Use event. requestId to obtain the unique identifier for each function execution\\\" is incorrect. event. requestId is incorrect as the event object does not contain a property called requestId. \\n INCORRECT : \\\"Use context. invocationId to extract the unique identifier tied to each function run\\\" is incorrect. context. invocationId is not valid as there is no such property in the context object of a Lambda function. \\n INCORRECT : \\\"Use context. lambdaId to get the unique identifier corresponding to each function invocation\\\" is incorrect. context. lambdaId is not a valid property within the context object for a Lambda function. References: https://docs.aws.amazon.com/lambda/latest/dg/nodejs-context.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-lambda/\"}",
    "{\"question\":\"A developer is creating an AWS Serverless Application Model (AWS SAM) template. It includes several AWS Lambda functions, an Amazon S3 bucket, and an Amazon CloudFront distribution. One Lambda function, running on Lambda@Edge, is integrated with the CloudFront distribution, while the S3 bucket serves as an origin for the distribution. However, upon deploying the AWS SAM blueprint in the us-west-1 Region, the stack's creation fails. What could be the possible reason for this failure?\",\"choices\":[\"Lambda@Edge functions can only be deployed in the us-east-1 Region. \",\"AWS SAM templates are not supported in the us-west-1 Region. \",\"Amazon S3 buckets serving as origins for CloudFront must be created in a separate Region from the CloudFront distribution. \",\"AWS Lambda functions integrated with CloudFront cannot be deployed using AWS SAM templates. \"],\"answer\":\"Lambda@Edge functions can only be deployed in the us-east-1 Region. \",\"reason\":\"Explanation: Lambda@Edge functions can only be created in the us-east-1 Region. If you want to deploy such functions, they must be done in this specific region. \\n CORRECT : \\\"Lambda@Edge functions can only be deployed in the us-east-1 Region\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"AWS SAM templates are not supported in the us-west-1 Region\\\" is incorrect. This is not a restriction, and hence not a reason for failure. \\n INCORRECT : \\\"Amazon S3 buckets serving as origins for CloudFront must be created in a separate Region from the CloudFront distribution\\\" is incorrect. Amazon S3 buckets serving as origins for CloudFront distributions can be in the same region as the distribution. Therefore, this would not cause a failure. \\n INCORRECT : \\\"AWS Lambda functions integrated with CloudFront cannot be deployed using AWS SAM templates\\\" is incorrect. AWS Lambda functions can be integrated with Amazon CloudFront and deployed using AWS SAM templates. This statement is incorrect and would not lead to a stack creation failure. References: https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/lambda-edge-how-it-works.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-sam/\"}",
    "{\"question\":\"A company uses an Amazon S3 bucket to store a large number of sensitive files relating to eCommerce transactions. The company has a policy that states that all data written to the S3 bucket must be encrypted. How can a Developer ensure compliance with this policy?\",\"choices\":[\"Create a bucket policy that denies the S3 PutObject request with the attribute x-amz-acl having values public-read, public-read-write, or authenticated-read\",\"Create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption\",\"Enable Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) on the Amazon S3 bucket\",\"Create an Amazon CloudWatch alarm that notifies an administrator if unencrypted objects are uploaded to the S3 bucket\"],\"answer\":\"Create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption\",\"reason\":\"Explanation: To encrypt an object at the time of upload, you need to add a header called x-amz-server-side-encryption to the request to tell S3 to encrypt the object using SSE-C, SSE-S3, or SSE-KMS. The following code example shows a Put request using SSE-S3. Enabling encryption on an S3 bucket does not enforce encryption however, so it is still necessary to take extra steps to force compliance with the policy. As the message in the image below states, bucket policies are applied before encryption settings so PUT requests without encryption information can be rejected by a bucket policy:Therefore, we need to create an S3 bucket policy that denies any S3 Put request that do not include the x-amz-server-side-encryption header. There are two possible values for the x-amz-server-side-encryption header: AES256, which tells S3 to use S3-managed keys, and aws:kms, which tells S3 to use AWS KMS–managed keys. \\n CORRECT : \\\"Create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption\\\" is the correct answer. \\n INCORRECT : \\\"Create a bucket policy that denies the S3 PutObject request with the attribute x-amz-acl having values public-read, public-read-write, or authenticated-read\\\" is incorrect. This policy means that authenticated users cannot upload objects to the bucket if the objects have public permissions. \\n INCORRECT : \\\"Enable Server-Side Encryption with Amazon S3-Managed Keys (SSE-S3) on the Amazon S3 bucket\\\" is incorrect as this will enable default encryption but will not enforce encryption on the S3 bucket. You do still need to enable default encryption on the bucket, but this alone will not enforce encryption. \\n INCORRECT : \\\"Create an Amazon CloudWatch alarm that notifies an administrator if unencrypted objects are uploaded to the S3 bucket\\\" is incorrect. This is operationally difficult to manage and only notifies, it does not prevent. References: https://aws. amazon. com/blogs/security/how-to-prevent-uploads-of-unencrypted-objects-to-amazon-s3/Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-s3-and-glacier/\"}",
    "{\"question\":\"An application reads data from Amazon S3 and makes 55,000 read requests per second. A Developer must design the storage solution to ensure the performance requirements are met cost-effectively. How can the storage be optimized to meet these requirements?\",\"choices\":[\"Create at least 10 prefixes and split the files across the prefixes. \",\"Create at least 10 S3 buckets and split the files across the buckets. \",\"Move the files to Amazon EFS. Index the files with S3 metadata. \",\"Move the files to Amazon DynamoDB. Index the files with S3 metadata. \"],\"answer\":\"Create at least 10 prefixes and split the files across the prefixes. \",\"reason\":\"Explanation: To avoid throttling in Amazon S3 you must ensure you do not exceed certain limits on a per-prefix basis. You can send 3,500 PUT/COPY/POST/DELETE or 5,500 GET/HEAD requests per second per prefix in an Amazon S3 bucket. There are no limits to the number of prefixes that you can have in your bucket. In this case the Developer would need to split the files across at least 10 prefixes in a single Amazon S3 bucket. The application should then read the files across the prefixes in parallel. \\n CORRECT : \\\"Create at least 10 prefixes and split the files across the prefixes\\\" is the correct answer. \\n INCORRECT : \\\"Create at least 10 S3 buckets and split the files across the buckets\\\" is incorrect. Performance is improved based on splitting reads across prefixes, not buckets. \\n INCORRECT : \\\"Move the files to Amazon EFS. Index the files with S3 metadata\\\" is incorrect. This is not cost-effective. \\n INCORRECT : \\\"Move the files to Amazon DynamoDB. Index the files with S3 metadata\\\" is incorrect. This is not cost-effective. References: https://aws. amazon. com/premiumsupport/knowledge-center/s3-request-limit-avoid-throttling/Save time with our AWS cheat sheets:https://digitalcloud.training/certification-training/aws-Developer-associate/aws-storage/amazon-s3/\"}",
    "{\"question\":\"A Developer has added a Global Secondary Index (GSI) to an existing Amazon DynamoDB table. The GSI is used mainly for read operations whereas the primary table is extremely write-intensive. Recently, the Developer has noticed throttling occurring under heavy write activity on the primary table. However, the write capacity units on the primary table are not fully utilized. What is the best explanation for why the writes are being throttled on the primary table?\",\"choices\":[\"There are insufficient read capacity units on the primary table\",\"The Developer should have added an LSI instead of a GSI\",\"There are insufficient write capacity units on the primary table\",\"The write capacity units on the GSI are under provisioned\"],\"answer\":\"The write capacity units on the GSI are under provisioned\",\"reason\":\"Explanation: Some applications might need to perform many kinds of queries, using a variety of different attributes as query criteria. To support these requirements, you can create one or more global secondary indexes and issue Query requests against these indexes in Amazon DynamoDB. When items from a primary table are written to the GSI they consume write capacity units. It is essential to ensure the GSI has sufficient WCUs (typically, at least as many as the primary table). If writes are throttled on the GSI, the main table will be throttled (even if there’s enough WCUs on the main table). LSIs do not cause any special throttling considerations. In this scenario, it is likely that the Developer assumed that the GSI would need fewer WCUs as it is more read-intensive and neglected to factor in the WCUs required for writing data into the GSI. Therefore, the most likely explanation is that the write capacity units on the GSI are under provisionedCORRECT: \\\"The write capacity units on the GSI are under provisioned\\\" is the correct answer. \\n INCORRECT : \\\"There are insufficient read capacity units on the primary table\\\" is incorrect as the table is being throttled due to writes, not reads. \\n INCORRECT : \\\"The Developer should have added an LSI instead of a GSI\\\" is incorrect as a GSI has specific advantages and there was likely good reason for adding a GSI. Also, you cannot add an LSI to an existing table. \\n INCORRECT : \\\"There are insufficient write capacity units on the primary table\\\" is incorrect as the question states that the WCUs are underutilized. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/GSI.html\"}",
    "{\"question\":\"An application uses an Auto Scaling group of Amazon EC2 instances, an Application Load Balancer (ALB), and an Amazon Simple Queue Service (SQS) queue. An Amazon CloudFront distribution caches content for global users. A Developer needs to add in-transit encryption to the data by configuring end-to-end SSL between the CloudFront Origin and the end users. How can the Developer meet this requirement? (Select TWO. )\",\"choices\":[\"Configure the Origin Protocol Policy\",\"Create an Origin Access Identity (OAI)\",\"Add a certificate to the Auto Scaling Group\",\"Configure the Viewer Protocol Policy\",\"Create an encrypted distribution\"],\"answer\":[\"Configure the Origin Protocol Policy\",\"Configure the Viewer Protocol Policy\"],\"reason\":\"Explanation: For web distributions, you can configure CloudFront to require that viewers use HTTPS to request your objects, so that connections are encrypted when CloudFront communicates with viewers. You also can configure CloudFront to use HTTPS to get objects from your origin, so that connections are encrypted when CloudFront communicates with your origin. If you configure CloudFront to require HTTPS both to communicate with viewers and to communicate with your origin, here's what happens when CloudFront receives a request for an object:A viewer submits an HTTPS request to CloudFront. There's some SSL/TLS negotiation here between the viewer and CloudFront. In the end, the viewer submits the request in an encrypted format. If the object is in the CloudFront edge cache, CloudFront encrypts the response and returns it to the viewer, and the viewer decrypts it. If the object is not in the CloudFront cache, CloudFront performs SSL/TLS negotiation with your origin and, when the negotiation is complete, forwards the request to your origin in an encrypted format. Your origin decrypts the request, encrypts the requested object, and returns the object to CloudFront. CloudFront decrypts the response, re-encrypts it, and forwards the object to the viewer. CloudFront also saves the object in the edge cache so that the object is available the next time it's requested. The viewer decrypts the response. To enable SSL between the origin and the distribution the Developer can configure the Origin Protocol Policy. Depending on the domain name used (CloudFront default or custom), the steps are different. To enable SSL between the end-user and CloudFront the Viewer Protocol Policy should be configured. \\n CORRECT : \\\"Configure the Origin Protocol Policy\\\" is a correct answer. \\n CORRECT : \\\"Configure the Viewer Protocol Policy\\\" is also a correct answer. \\n INCORRECT : \\\"Create an Origin Access Identity (OAI)\\\" is incorrect as this is a special user used for securing objects in Amazon S3 origins. \\n INCORRECT : \\\"Add a certificate to the Auto Scaling Group\\\" is incorrect as you do not add certificates to an ASG. The certificate should be located on the ALB listener in this scenario. \\n INCORRECT : \\\"Create an encrypted distribution\\\" is incorrect as there’s no such thing as an encrypted distributionReferences:https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-viewers-to-cloudfront.htmlhttps://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/using-https-cloudfront-to-custom-origin.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cloudfront/\"}",
    "{\"question\":\"A website is running on a single Amazon EC2 instance. A Developer wants to publish the website on the Internet and is creating an A record on Amazon Route 53 for the website’s public DNS name. What type of IP address MUST be assigned to the EC2 instance and used in the A record to ensure ongoing connectivity?\",\"choices\":[\"Public IP address\",\"Dynamic IP address\",\"Elastic IP address\",\"Private IP address\"],\"answer\":\"Elastic IP address\",\"reason\":\"Explanation: In Amazon Route 53 when you create an A record you must supply an IP address for the resource to connect to. For a public hosted zone this must be a public IP address. There are three types of IP address that can be assigned to an Amazon EC2 instance:Public – public address that is assigned automatically to instances in public subnets and reassigned if instance is stopped/started. Private – private address assigned automatically to all instances. Elastic IP – public address that is static. To ensure ongoing connectivity the Developer needs to use an Elastic IP address for the EC2 instance and DNS A record as this is the only type of static, public IP address you can assign to an Amazon EC2 instance. \\n CORRECT : \\\"Elastic IP address\\\" is the correct answer. \\n INCORRECT : \\\"Public IP address\\\" is incorrect as though this is a public IP address, it is not static and will change every time the EC2 instance restarts. Therefore, connectivity would be lost until you update the Route 53 A record. \\n INCORRECT : \\\"Dynamic IP address\\\" is incorrect as a dynamic IP address is an IP address that will change over time. For this scenario a static, public address is required. \\n INCORRECT : \\\"Private IP address\\\" is incorrect as a public IP address is required for the public DNS A record. References: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/using-instance-addressing.htmlhttps://docs.aws.amazon.com/Route53/latest/DeveloperGuide/ResourceRecordTypes.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-ec2/https://digitalcloud.training/amazon-route-53/\"}",
    "{\"question\":\"An Amazon API Gateway API developer aims to integrate request validation in a production setting but wants to test it before deployment. Which of the following methods offers the least operational overhead for testing via a tool by sending test requests?\",\"choices\":[\"Clone the existing API, add the request validation, run the tests, then modify the original API to include request validation before deploying to production. \",\"Modify the existing API to include request validation, deploy this to a new API Gateway stage, test it, then deploy it to the production stage. \",\"First export the current API to an OpenAPI file, create and modify a new API by importing the OpenAPI file and adding request validation, test it, then modify and deploy the original API. \",\"Create a new API from scratch with the necessary resources, methods, and request validation, run the tests, then modify and deploy the original API. \"],\"answer\":\"Modify the existing API to include request validation, deploy this to a new API Gateway stage, test it, then deploy it to the production stage. \",\"reason\":\"Explanation: The primary concern here is reducing operational overhead. The correct answer is the most straightforward and efficient because it leverages the functionality of Amazon API Gateway stages. By modifying the existing API to add request validation and deploying the updated API to a new API Gateway stage, it allows for testing in a controlled environment before deploying to production. It provides the least operational overhead because it does not involve creating a new API or exporting/importing an OpenAPI file. \\n CORRECT : \\\"Modify the existing API to include request validation, deploy this to a new API Gateway stage, test it, then deploy it to the production stage\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Clone the existing API, add the request validation, run the tests, then modify the original API to include request validation before deploying to production\\\" is incorrect. Cloning the API or exporting it to an OpenAPI file and importing into a new API both involve unnecessary steps that increase operational overhead. \\n INCORRECT : \\\"First export the current API to an OpenAPI file, create and modify a new API by importing the OpenAPI file and adding request validation, test it, then modify and deploy the original API\\\" is incorrect. Cloning the API or exporting it to an OpenAPI file and importing into a new API both involve unnecessary steps that increase operational overhead. \\n INCORRECT : \\\"Create a new API from scratch with the necessary resources, methods and request validation, run the tests, then modify and deploy the original API\\\" is incorrect. Creating a new API from scratch can be time-consuming and requires extra effort which again adds to operational overhead. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/stages.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-api-gateway/\"}",
    "{\"question\":\"An eCommerce application uses an Amazon RDS database with Amazon ElastiCache in front. Stock volume data is updated dynamically in listings as sales are made. Customers have complained that occasionally the stock volume data is incorrect, and they end up purchasing items that are out of stock. A Developer has checked the front end and indeed some items display the incorrect stock count. What could be causing this issue?\",\"choices\":[\"The Amazon RDS database is deployed as Multi-AZ and the standby is inconsistent. \",\"The Amazon RDS database has insufficient IOPS provisioned for its EBS volumes. \",\"The stock volume data is being retrieved using a write-through ElastiCache cluster. \",\"The cache is not being invalidated when the stock volume data is changed. \"],\"answer\":\"The cache is not being invalidated when the stock volume data is changed. \",\"reason\":\"Explanation: Amazon ElastiCache is being used to cache data from the Amazon RDS database to improve performance when performing queries. In this case the cache has stale stock volume data stored and is returning this information when customers are purchasing items. The resolution is to ensure that the cache is invalidated whenever the stock volume data is changed. This can be done in the application layer. \\n CORRECT : \\\"The cache is not being invalidated when the stock volume data is changed\\\" is the correct answer. \\n INCORRECT : \\\"The stock volume data is being retrieved using a write-through ElastiCache cluster\\\" is incorrect. If this was the case the data would not be stale. \\n INCORRECT : \\\"The Amazon RDS database is deployed as Multi-AZ and the standby is inconsistent\\\" is incorrect. Multi-AZ standbys are not used for reading data and the replication is synchronous so it would not be inconsistent. \\n INCORRECT : \\\"The Amazon RDS database has insufficient IOPS provisioned for its EBS volumes\\\" is incorrect. This is not the issue here; the stale data is being retrieved from the ElastiCache database. References: https://docs.aws.amazon.com/AmazonElastiCache/latest/red-ug/Strategies.html Save time with our AWS cheat sheets:https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-associate/aws-database-dva/amazon-elasticache/\"}",
    "{\"question\":\"A company wants a serverless solution for phased release of static websites hosted on various version control systems. Deployments should be triggered by Git branch merges and all data exchange should be over HTTPS. Which option offers the LOWEST operational overhead?\",\"choices\":[\"Deploy websites on separate Amazon EC2 instances for each environment, use AWS CodeDeploy for automation, and link it with the version control systems. \",\"Use AWS Amplify for hosting, connect corresponding repository branches, and initiate deployments by merging changes to the needed branch. \",\"Use Amazon S3 to host the websites, create a manual script to deploy changes when there are code merges in the version control systems. \",\"Use AWS Elastic Beanstalk for hosting and use AWS CodeStar to manage deployments and workflows. \"],\"answer\":\"Use AWS Amplify for hosting, connect corresponding repository branches, and initiate deployments by merging changes to the needed branch. \",\"reason\":\"Explanation: AWS Amplify is designed to host static websites with continuous deployment linked to Git repositories. It allows you to connect branches in your repository with environments in Amplify, and to automatically deploy changes when you merge code to those branches. This solution requires the least operational overhead among the options. \\n CORRECT : \\\"Use AWS Amplify for hosting, connect corresponding repository branches, and initiate deployments by merging changes to the needed branch\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Deploy websites on separate Amazon EC2 instances for each environment, use AWS CodeDeploy for automation, and link it with the version control systems\\\" is incorrect. While Amazon EC2 could host the websites and AWS CodeDeploy could manage deployments, this is not a serverless solution as requested. The EC2 instances would need to run continuously, creating more operational overhead. \\n INCORRECT : \\\"Use Amazon S3 to host the websites, create a manual script to deploy changes when there are code merges in the version control systems\\\" is incorrect. While Amazon S3 can host static websites, creating a manual script to deploy changes adds unnecessary complexity and operational overhead compared to a service like Amplify that automates this process. \\n INCORRECT : \\\"Use AWS Elastic Beanstalk for hosting and use AWS CodeStar to manage deployments and workflows\\\" is incorrect. AWS Elastic Beanstalk is typically used for dynamic, multi-tier web applications, and AWS CodeStar provides a unified user interface, enabling you to easily manage your software development activities in one place. However, this combination could be more complex and require more operational overhead than AWS Amplify for a static website. References: https://docs.aws.amazon.com/amplify/latest/userguide/welcome.html\"}",
    "{\"question\":\"An application uses both Amazon EC2 instances and on-premises servers. The on-premises servers are a critical component of the application, and a developer wants to collect metrics and logs from these servers. The developer would like to use Amazon CloudWatch. How can the developer accomplish this?\",\"choices\":[\"Install an AWS SDK on the on-premises servers that automatically sends logs to CloudWatch. \",\"Install the CloudWatch agent on the on-premises servers and specify IAM credentials with permissions to CloudWatch. \",\"Write a batch script that uses system utilities to collect performance metrics and application logs. Upload the metrics and logs to CloudWatch. \",\"Install the CloudWatch agent on the on-premises servers and specify an IAM role with permissions to CloudWatch. \"],\"answer\":\"Install the CloudWatch agent on the on-premises servers and specify IAM credentials with permissions to CloudWatch. \",\"reason\":\"Explanation: You can download the CloudWatch agent package using either Systems Manager Run Command or an Amazon S3 download link. You then install the agent and specify the IAM credentials to use. The IAM credentials are an access key and secret access key of an IAM user that has permissions to Amazon CloudWatch. Once this has been completed the on-premises servers will automatically send metrics and log files to Amazon CloudWatch and can be centrally monitored along with AWS services. \\n CORRECT :  Install the CloudWatch agent on the on-premises servers and specify IAM credentials with permissions to CloudWatch\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Install the CloudWatch agent on the on-premises servers and specify an IAM role with permissions to CloudWatch\\\" is incorrect. You cannot specify a role with an on-premises server so you must use access keys instead. \\n INCORRECT : \\\"Write a batch script that uses system utilities to collect performance metrics and application logs. Upload the metrics and logs to CloudWatch\\\" is incorrect. The CloudWatch agent would be a better solution and you must have permissions to send this information to CloudWatch. \\n INCORRECT : \\\"Install an AWS SDK on the on-premises servers that automatically sends logs to CloudWatch\\\" is incorrect. The CloudWatch agent would be a better solution and you must have permissions to send this information to CloudWatch. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/install-CloudWatch-Agent-on-premise.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cloudwatch/\"}",
    "{\"question\":\"An application is running on an Amazon EC2 Linux instance. The instance needs to make AWS API calls to several AWS services. What is the MOST secure way to provide access to the AWS services with MINIMAL management overhead?\",\"choices\":[\"Use AWS KMS to store and retrieve credentials\",\"Use EC2 instance profiles\",\"Store the credentials in AWS CloudHSM\",\"Store the credentials in the ~/. aws/credentials file\"],\"answer\":\"Use EC2 instance profiles\",\"reason\":\"Explanation: An instance profile is a container for an IAM role that you can use to pass role information to an EC2 instance when the instance starts. Using an instance profile you can attach an IAM Role to an EC2 instance that the instance can then assume in order to gain access to AWS services. \\n CORRECT : \\\"Use EC2 instance profiles\\\" is the correct answer. \\n INCORRECT : \\\"Use AWS KMS to store and retrieve credentials\\\" is incorrect as KMS is used to manage encryption keys. \\n INCORRECT : \\\"Store the credentials in AWS CloudHSM\\\" is incorrect as CloudHSM is also used to manage encryption keys. It is similar to KMS but uses a dedicated hardware device that is not multi-tenant. \\n INCORRECT : \\\"Store the credentials in the ~/. aws/credentials file\\\" is incorrect as this is not the most secure option. The credentials file is associated with the AWS CLI and used for passing credentials in the form of an access key ID and secret access key when making programmatic requests from the command line. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles_use_switch-role-ec2_instance-profiles.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-ec2/\"}",
    "{\"question\":\"An application that processes financial transactions receives thousands of transactions each second. The transactions require end-to-end encryption, and the application implements this by using the AWS KMS GenerateDataKey operation. During operation the application receives the following error message:“You have exceeded the rate at which you may call KMS. Reduce the frequency of your calls. (Service: AWSKMS; Status Code: 400; Error Code: ThrottlingException; Request ID: ”Which actions are best practices to resolve this error? (Select TWO. )\",\"choices\":[\"Create a local cache using the AWS Encryption SDK and the LocalCryptoMaterialsCache feature. \",\"Call the AWS KMS Encrypt operation directly to allow AWS KMS to encrypt the data. \",\"Create a case in the AWS Support Center to increase the quota for the account. \",\"Use Amazon SQS to queue the requests and configure AWS KMS to poll the queue. \",\"Create an AWS KMS custom key store and generate data keys through AWS CloudHSM. \"],\"answer\":[\"Create a local cache using the AWS Encryption SDK and the LocalCryptoMaterialsCache feature. \",\"Create a case in the AWS Support Center to increase the quota for the account. \"],\"reason\":\"Explanation: To ensure that AWS KMS can provide fast and reliable responses to API requests from all customers, it throttles API requests that exceed certain boundaries. Throttling occurs when AWS KMS rejects an otherwise valid request and returns a ThrottlingException error. Data key caching stores data keys and related cryptographic material in a cache. When you encrypt or decrypt data, the AWS Encryption SDK looks for a matching data key in the cache. If it finds a match, it uses the cached data key rather than generating a new one. Data key caching can improve performance, reduce cost, and help you stay within service limits as your application scales. Your application can benefit from data key caching if:It can reuse data keys. It generates numerous data keys. Your cryptographic operations are unacceptably slow, expensive, limited, or resource-intensive. To create an instance of the local cache, use the LocalCryptoMaterialsCache constructor in Java and Python, the getLocalCryptographicMaterialsCache function in JavaScript, or the aws_cryptosdk_materials_cache_local_new constructor in C. Additionally, the developer can request an increase in the quota for AWS KMS which will provide the ability to submit more API calls the AWS KMS. \\n CORRECT : \\\"Create a local cache using the AWS Encryption SDK and the LocalCryptoMaterialsCache feature\\\" is a correct answer (as explained above. )\\n CORRECT : \\\"Create a case in the AWS Support Center to increase the quota for the account\\\" is also a correct answer (as explained above. )\\n INCORRECT : \\\"Call the AWS KMS Encrypt operation directly to allow AWS KMS to encrypt the data\\\" is incorrect. This will not reduce API calls to AWS KMS. Additionally, there are limits to the maximum size of the data that can be encrypted using this method. The max is 4096 bytes. \\n INCORRECT : \\\"Use Amazon SQS to queue the requests and configure AWS KMS to poll the queue\\\" is incorrect. KMS cannot be configured to poll and SQS queue. \\n INCORRECT : \\\"Create an AWS KMS custom key store and generate data keys through AWS CloudHSM\\\" is incorrect. This is an unnecessary step and would incur additional cost. CloudHSM is not beneficial for this specific situation. References: https://docs.aws.amazon.com/encryption-sdk/latest/developer-guide/data-key-caching.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-kms/\"}",
    "{\"question\":\"An application runs on a fleet of Amazon EC2 instances and stores data in a Microsoft SQL Server database hosted on Amazon RDS. The developer wants to avoid storing database connection credentials the application code. The developer would also like a solution that automatically rotates the credentials. What is the MOST secure way to store and access the database credentials?\",\"choices\":[\"Use AWS Secrets Manager to store the credentials. Retrieve the credentials from Secrets Manager as needed. \",\"Create an IAM role that has permissions to access the database. Attach the role to the EC2 instance. \",\"Use AWS Systems Manager Parameter store to store the credentials. Enable automatic rotation of the credentials. \",\"Store the credentials in an encrypted source code repository. Retrieve the credentials from AWS CodeCommit as needed. \"],\"answer\":\"Use AWS Secrets Manager to store the credentials. Retrieve the credentials from Secrets Manager as needed. \",\"reason\":\"Explanation: AWS Secrets Manager can be used for secure storage of secrets such as database connection credentials. Automatic rotation is supported for several RDS database types including Microsoft SQL Server. This is the most secure solution for storing and retrieving the credentials. \\n CORRECT : \\\"Use AWS Secrets Manager to store the credentials. Retrieve the credentials from Secrets Manager as needed\\\" is the correct answer (as explained above. )\\n INCORRECT : \\\"Use AWS Systems Manager Parameter store to store the credentials. Enable automatic rotation of the credentials\\\" is incorrect. With SSM Parameter Store you cannot enable automatic rotation. You can rotate the credentials but you would need to configure your own Lambda function. \\n INCORRECT : \\\"Create an IAM role that has permissions to access the database. Attach the role to the EC2 instance\\\" is incorrect. RDS for SQL Server does support windows authentication using a managed Microsoft AD with IAM roles for permissions to the AD service, but this is not described in the solution. \\n INCORRECT : \\\"Store the credentials in an encrypted source code repository. Retrieve the credentials from AWS CodeCommit as needed\\\" is incorrect. This is not a solution that is suitable for retrieving database connection credentials and it does not support automatic rotation. References: https://docs.aws.amazon.com/secretsmanager/latest/userguide/rotating-secrets.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-secrets-manager/\"}",
    "{\"question\":\"A programmer is creating an application that requires signed requests (Signature Version 4) for invoking other AWS services. Having constructed a canonical request, created the string to sign, and calculated the signing information, which strategies can the programmer apply to finalize a signed request? (Select TWO. )\",\"choices\":[\"Incorporate the signature into an HTTP header called \\\"Authorization\\\". \",\"Append the signature to a query string parameter known as \\\"X-Amz-Credentials\\\". \",\"Embed the signature in an HTTP header labelled \\\"Authorization-Key\\\". \",\"Insert the signature into a query string parameter referred to as \\\"X-Amz-Signature\\\". \",\"Add the signature to a query string parameter named \\\"Signature-Token\\\". \"],\"answer\":[\"Incorporate the signature into an HTTP header called \\\"Authorization\\\". \",\"Insert the signature into a query string parameter referred to as \\\"X-Amz-Signature\\\". \"],\"reason\":\"Explanation: When sending a request to AWS services using Signature Version 4, the signature should be included in the \\\"Authorization\\\" HTTP header or as a parameter \\\"X-Amz-Signature\\\" in the query string. \\n CORRECT : \\\"Incorporate the signature into an HTTP header called \\\"Authorization\\\"\\\" is a correct answer (as explained above. )\\n CORRECT : \\\"Insert the signature into a query string parameter referred to as \\\"X-Amz-Signature\\\"\\\" is also a correct answer (as explained above. )\\n INCORRECT : \\\"Append the signature to a query string parameter known as \\\"X-Amz-Credentials\\\"\\\" is incorrect. \\\"X-Amz-Credentials\\\" is a query string parameter, but it's not meant for adding signatures. It's used to include access key ID and scoped credential details. \\n INCORRECT : \\\"Embed the signature in an HTTP header labelled \\\"Authorization-Key\\\"\\\" is incorrect. \\\"Authorization-Key\\\" is not a recognized HTTP header for adding AWS Signature Version 4. \\n INCORRECT : \\\"Add the signature to a query string parameter named \\\"Signature-Token\\\"\\\" is incorrect. \\\"Signature-Token\\\" is not a recognized query string parameter for adding AWS Signature Version 4. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_aws-signing.html\"}",
    "{\"question\":\"An application running on a fleet of EC2 instances use the AWS SDK for Java to copy files into several AWS buckets using access keys stored in environment variables. A Developer has modified the instances to use an assumed IAM role with a more restrictive policy that allows access to only one bucket. However, after applying the change the Developer logs into one of the instances and is still able to write to all buckets. What is the MOST likely explanation for this situation?\",\"choices\":[\"An IAM inline policy is being used on the IAM role\",\"An IAM managed policy is being used on the IAM role\",\"The AWS CLI is corrupt and needs to be reinstalled\",\"The AWS credential provider looks for instance profile credentials last\"],\"answer\":\"The AWS credential provider looks for instance profile credentials last\",\"reason\":\"Explanation: When you initialize a new service client without supplying any arguments, the AWS SDK for Java attempts to find AWS credentials by using the default credential provider chain implemented by the DefaultAWSCredentialsProviderChain class. The default credential provider chain looks for credentials in this order:Environment variables–AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. The AWS SDK for Java uses the EnvironmentVariableCredentialsProvider class to load these credentials. Java system properties–aws. accessKeyId and aws. secretKey. The AWS SDK for Java uses the SystemPropertiesCredentialsProvider to load these credentials. The default credential profiles file– typically located at ~/. aws/credentials (location can vary per platform) and shared by many of the AWS SDKs and by the AWS CLI. The AWS SDK for Java uses the ProfileCredentialsProvider to load these credentials. Amazon ECS container credentials– loaded from the Amazon ECS if the environment variable AWS_CONTAINER_CREDENTIALS_RELATIVE_URI is set. The AWS SDK for Java uses the ContainerCredentialsProvider to load these credentials. You can specify the IP address for this value. Instance profile credentials– used on EC2 instances and delivered through the Amazon EC2 metadata service. The AWS SDK for Java uses the InstanceProfileCredentialsProvider to load these credentials. You can specify the IP address for this value. Therefore, the AWS SDK for Java will find the credentials stored in environment variables before it checks for instance provide credentials and will allow access to the extra S3 buckets. NOTE: The Default Credential Provider Chain is very similar for other SDKs and the CLI as well. Check the references below for an article showing the steps for the AWS CLI. \\n CORRECT : \\\"The AWS credential provider looks for instance profile credentials last\\\" is the correct answer. \\n INCORRECT : \\\"An IAM inline policy is being used on the IAM role\\\" is incorrect. If an inline policy was also applied to the role with a less restrictive policy it wouldn’t matter, as the most restrictive policy would be applied. \\n INCORRECT : \\\"An IAM managed policy is being used on the IAM role\\\" is incorrect. Though the managed policies are less restrictive by default (read-only or full access), this is not the most likely cause of the situation as we were told the policy is more restrictive and we know the environments variables have access keys in them which will be used before the policy is checked. \\n INCORRECT : \\\"The AWS CLI is corrupt and needs to be reinstalled\\\" is incorrect. There is a plausible explanation for this situation so no reason to suspect a software bug is to blame. References: https://docs.aws.amazon.com/sdk-for-java/v1/developer-guide/credentials.htmlhttps://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html\"}",
    "{\"question\":\"A company is migrating a stateful web service into the AWS cloud. The objective is to refactor the application to realize the benefits of cloud computing. How can the Developer leading the project refactor the application to enable more elasticity? (Select TWO. )\",\"choices\":[\"Use Amazon CloudFormation and the Serverless Application Model\",\"Use Amazon CloudFront with a Web Application Firewall\",\"Store the session state in an Amazon RDS database\",\"Use an Elastic Load Balancer and Auto Scaling Group\",\"Store the session state in an Amazon DynamoDB table\"],\"answer\":[\"Use an Elastic Load Balancer and Auto Scaling Group\",\"Store the session state in an Amazon DynamoDB table\"],\"reason\":\"Explanation: As this is a stateful application the session data needs to be stored somewhere. Amazon DynamoDB is designed to be used for storing session data and it highly scalable. To add elasticity to the architecture an Amazon Elastic Load Balancer (ELB) and Amazon EC2 Auto Scaling group (ASG) can be used. With this architecture the web service can scale elastically using the ASG and the ELB will distribute traffic to all new instances that the ASG launches. This is a good example of utilizing some of the key benefits of refactoring applications into the AWS cloud. \\n CORRECT : \\\"Use an Elastic Load Balancer and Auto Scaling Group\\\" is a correct answer. \\n CORRECT : \\\"Store the session state in an Amazon DynamoDB table\\\" is also a correct answer. \\n INCORRECT : \\\"Use Amazon CloudFormation and the Serverless Application Model\\\" is incorrect. AWS SAM is used in CloudFormation templates for expressing serverless applications using a simplified syntax. This application is not a serverless application. \\n INCORRECT : \\\"Use Amazon CloudFront with a Web Application Firewall\\\" is incorrect neither protection from web exploits nor improved performance for content delivery are requirements in this scenario. \\n INCORRECT : \\\"Store the session state in an Amazon RDS database\\\" is incorrect as RDS is not suitable for storing session state data. DynamoDB is a better fit for this use case. References: https://docs.aws.amazon.com/aws-sdk-php/v2/guide/feature-dynamodb-session-handler.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/https://digitalcloud.training/amazon-ec2-auto-scaling/https://digitalcloud.training/amazon-dynamodb/\"}",
    "{\"question\":\"A Developer is creating a service on Amazon ECS and needs to ensure that each task is placed on a different container instance. How can this be achieved?\",\"choices\":[\"Use a task placement strategy\",\"Use a task placement constraint\",\"Create a service on Fargate\",\"Create a cluster with multiple container instances\"],\"answer\":\"Use a task placement constraint\",\"reason\":\"Explanation: A task placement constraint is a rule that is considered during task placement. Task placement constraints can be specified when either running a task or creating a new service. Amazon ECS supports the following types of task placement constraints:distinctInstancePlace each task on a different container instance. This task placement constraint can be specified when either running a task or creating a new service. memberOfPlace tasks on container instances that satisfy an expression. For more information about the expression syntax for constraints, see Cluster Query Language. The memberOf task placement constraint can be specified with the following actions:Running a taskCreating a new serviceCreating a new task definitionCreating a new revision of an existing task definitionThe following code can be used in a task definition to specify a task placement constraint that ensures that each task will run on a distinct instance:\\\"placementConstraints\\\": [    {        \\\"type\\\": \\\"distinctInstance\\\"    }]\\n CORRECT : \\\"Use a task placement constraint\\\" is the correct answer. \\n INCORRECT : \\\"Use a task placement strategy\\\" is incorrect as this is used to select instances for task placement using the binpack, random and spread algorithms. \\n INCORRECT : \\\"Create a service on Fargate\\\" is incorrect as Fargate spreads tasks across AZs but not instances. \\n INCORRECT : \\\"Create a cluster with multiple container instances\\\" is incorrect as this will not guarantee that each task runs on a different container instance. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-constraints.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-ecs-and-eks/\"}",
    "{\"question\":\"An AWS Lambda function requires several environment variables with secret values. The secret values should be obscured in the Lambda console and API output even for users who have permission to use the key. What is the best way to achieve this outcome and MINIMIZE complexity and latency?\",\"choices\":[\"Encrypt the secret values client-side using encryption helpers\",\"Encrypt the secret values with a customer-managed CMK\",\"Store the encrypted values in an encrypted Amazon S3 bucket and reference them from within the code\",\"Use an external encryption infrastructure to encrypt the values and add them as environment variables\"],\"answer\":\"Encrypt the secret values client-side using encryption helpers\",\"reason\":\"Explanation: You can use environment variables to store secrets securely for use with Lambda functions. Lambda always encrypts environment variables at rest. Additionally, you can use the following features to customize how environment variables are encrypted. Key configuration – On a per-function basis, you can configure Lambda to use an encryption key that you create and manage in AWS Key Management Service. These are referred to as customer managed customer master keys (CMKs) or customer managed keys. If you don't configure a customer managed key, Lambda uses an AWS managed CMK named aws/lambda, which Lambda creates in your account. Encryption helpers – The Lambda console lets you encrypt environment variable values client side, before sending them to Lambda. This enhances security further by preventing secrets from being displayed unencrypted in the Lambda console, or in function configuration that's returned by the Lambda API. The console also provides sample code that you can adapt to decrypt the values in your function handler. The configuration for using encryption helps to encrypt data client-side looks like this:This is the best way to achieve this outcome and minimizes complexity as the encryption infrastructure will still use AWS KMS and be able to decrypt the values during function execution. \\n CORRECT : \\\"Encrypt the secret values client-side using encryption helpers\\\" is the correct answer. \\n INCORRECT : \\\"Encrypt the secret values with a customer-managed CMK\\\" is incorrect as this alone will not achieve the desired outcome as the environment variables should be encrypted client-side with the encryption helper to ensure users cannot see the secret values. \\n INCORRECT : \\\"Store the encrypted values in an encrypted Amazon S3 bucket and reference them from within the code\\\" is incorrect as this would introduce complexity and latency. \\n INCORRECT : \\\"Use an external encryption infrastructure to encrypt the values and add them as environment variables\\\" is incorrect as this would introduce complexity and latency. References: https://docs.aws.amazon.com/lambda/latest/dg/security-dataprotection.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-lambda/\"}",
    "{\"question\":\"A company uses Amazon SQS to decouple an online application that generates memes. The SQS consumers poll the queue regularly to keep throughput high and this is proving to be costly and resource intensive. A Developer has been asked to review the system and propose changes that can reduce costs and the number of empty responses. What would be the BEST approach to MINIMIZING cost?\",\"choices\":[\"Set the imaging queue visibility Timeout attribute to 20 seconds\",\"Set the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds\",\"Set the imaging queue MessageRetentionPeriod attribute to 20 seconds\",\"Set the DelaySeconds parameter of a message to 20 seconds\"],\"answer\":\"Set the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds\",\"reason\":\"Explanation: The process of consuming messages from a queue depends on whether you use short or long polling. By default, Amazon SQS uses short polling, querying only a subset of its servers (based on a weighted random distribution) to determine whether any messages are available for a response. You can use long polling to reduce your costs while allowing your consumers to receive messages as soon as they arrive in the queue. When the wait time for the ReceiveMessage API action is greater than 0, long polling is in effect. The maximum long polling wait time is 20 seconds. Long polling helps reduce the cost of using Amazon SQS by eliminating the number of empty responses (when there are no messages available for a ReceiveMessage request) and false empty responses (when messages are available but aren't included in a response). Therefore, the best way to optimize resource usage and reduce the number of empty responses (and cost) is to configure long polling by setting the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds. \\n CORRECT : \\\"Set the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds\\\" is the correct answer. \\n INCORRECT : \\\"Set the imaging queue visibility Timeout attribute to 20 seconds\\\" is incorrect. This attribute configures message visibility which will not reduce empty responses. \\n INCORRECT : \\\"Set the imaging queue MessageRetentionPeriod attribute to 20 seconds\\\" is incorrect. This attribute sets the length of time, in seconds, for which Amazon SQS retains a message. \\n INCORRECT : \\\"Set the DelaySeconds parameter of a message to 20 seconds\\\" is incorrect. This attribute sets the length of time, in seconds, for which the delivery of all messages in the queue is delayed. References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-short-and-long-polling.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-application-integration-services/\"}",
    "{\"question\":\"A web application is using Amazon Kinesis Data Streams for ingesting IoT data that is then stored before processing for up to 24 hours. How can the Developer implement encryption at rest for data stored in Amazon Kinesis Data Streams?\",\"choices\":[\"Add a certificate and enable SSL/TLS connections to Kinesis Data Streams\",\"Use the Amazon Kinesis Consumer Library (KCL) to encrypt the data\",\"Encrypt the data once it is at rest with an AWS Lambda function\",\"Enable server-side encryption on Kinesis Data Streams with an AWS KMS CMK\"],\"answer\":\"Enable server-side encryption on Kinesis Data Streams with an AWS KMS CMK\",\"reason\":\"Explanation: Amazon Kinesis Data Streams (KDS) is a massively scalable and durable real-time data streaming service. KDS can continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, financial transactions, social media feeds, IT logs, and location-tracking events. Server-side encryption is a feature in Amazon Kinesis Data Streams that automatically encrypts data before it's at rest by using an AWS KMS customer master key (CMK) you specify. Data is encrypted before it's written to the Kinesis stream storage layer and decrypted after it’s retrieved from storage. As a result, your data is encrypted at rest within the Kinesis Data Streams service. This allows you to meet strict regulatory requirements and enhance the security of your data. With server-side encryption, your Kinesis stream producers and consumers don't need to manage master keys or cryptographic operations. Your data is automatically encrypted as it enters and leaves the Kinesis Data Streams service, so your data at rest is encrypted. AWS KMS provides all the master keys that are used by the server-side encryption feature. AWS KMS makes it easy to use a CMK for Kinesis that is managed by AWS, a user-specified AWS KMS CMK, or a master key imported into the AWS KMS service. Therefore, in this scenario the Developer can enable server-side encryption on Kinesis Data Streams with an AWS KMS CMKCORRECT: \\\"Enable server-side encryption on Kinesis Data Streams with an AWS KMS CMK\\\" is the correct answer. \\n INCORRECT : \\\"Add a certificate and enable SSL/TLS connections to Kinesis Data Streams\\\" is incorrect as SSL/TLS is already used with Kinesis (you don’t need to add a certificate) and this only provides encryption in-transit, not encryption at rest. \\n INCORRECT : \\\"Use the Amazon Kinesis Consumer Library (KCL) to encrypt the data\\\" is incorrect. The KCL provides design patterns and code for Amazon Kinesis Data Streams consumer applications. The KCL is not used for adding encryption to the data in a stream. \\n INCORRECT : \\\"Encrypt the data once it is at rest with an AWS Lambda function\\\" is incorrect as this is unnecessary when Kinesis natively supports server-side encryption. References: https://docs.aws.amazon.com/streams/latest/dev/what-is-sse.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-kinesis/\"}",
    "{\"question\":\"A Developer is troubleshooting an issue with a DynamoDB table. The table is used to store order information for a busy online store and uses the order date as the partition key. During busy periods writes to the table are being throttled despite the consumed throughput being well below the provisioned throughput. According to AWS best practices, how can the Developer resolve the issue at the LOWEST cost?\",\"choices\":[\"Increase the read and write capacity units for the table\",\"Add a random number suffix to the partition key values\",\"Add a global secondary index to the table\",\"Use an Amazon SQS queue to buffer the incoming writes\"],\"answer\":\"Add a random number suffix to the partition key values\",\"reason\":\"Explanation: DynamoDB stores data as groups of attributes, known as items. Items are similar to rows or records in other database systems. DynamoDB stores and retrieves each item based on the primary key value, which must be unique. Items are distributed across 10-GB storage units, called partitions (physical storage internal to DynamoDB). Each table has one or more partitions, as shown in the following illustration. DynamoDB uses the partition key’s value as an input to an internal hash function. The output from the hash function determines the partition in which the item is stored. Each item’s location is determined by the hash value of its partition key. All items with the same partition key are stored together, and for composite partition keys, are ordered by the sort key value. DynamoDB splits partitions by sort key if the collection size grows bigger than 10 GB. DynamoDB evenly distributes provisioned throughput—read capacity units (RCUs) and write capacity units (WCUs)—among partitions and automatically supports your access patterns using the throughput you have provisioned. However, if your access pattern  exceeds 3000 RCU or 1000 WCU for a single partition key value, your requests might be throttled with a ProvisionedThroughputExceededException error. To avoid request throttling, design your DynamoDB table with the right partition key to meet your access requirements and provide even distribution of data. Recommendations for doing this include the following:Use high cardinality attributes (e. g. email_id, employee_no, customer_id etc. )Use composite attributesCache popular itemsAdd random numbers or digits from a pre-determined range for write-heavy use casesIn this case there is a hot partition due to the order date being used as the partition key and this is causing writes to be throttled. Therefore, the best solution to ensure the writes are more evenly distributed in this scenario is to add a random number suffix to the partition key values. \\n CORRECT : \\\"Add a random number suffix to the partition key values\\\" is the correct answer. \\n INCORRECT : \\\"Increase the read and write capacity units for the table\\\" is incorrect as this will not solve the hot partition issue and we know that the consumed throughput is lower than provisioned throughput. \\n INCORRECT : \\\"Add a global secondary index to the table\\\" is incorrect as a GSI is used for querying data more efficiently, it will not solve the problem of write performance due to a hot partition. \\n INCORRECT : \\\"Use an Amazon SQS queue to buffer the incoming writes\\\" is incorrect as this is not the lowest cost option. You would need to have producers and consumers of the queue as well as paying for the queue itself. References: https://aws. amazon. com/blogs/database/choosing-the-right-dynamodb-partition-key/Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-dynamodb/\"}",
    "{\"question\":\"An Amazon DynamoDB table will store authentication credentials for a mobile app. The table must be secured so only a small group of Developers are able to access it. How can table access be secured according to this requirement and following AWS best practice?\",\"choices\":[\"Attach a permissions policy to an IAM group containing the Developer’s IAM user accounts that grants access to the table\",\"Attach a resource-based policy to the table and add an IAM group containing the Developer’s IAM user accounts as a Principal in the policy\",\"Create an AWS KMS resource-based policy to a CMK and grant the developer’s user accounts the permissions to decrypt data in the table using the CMK\",\"Create a shared user account and attach a permissions policy granting access to the table. Instruct the Developer’s to login with the user account\"],\"answer\":\"Attach a permissions policy to an IAM group containing the Developer’s IAM user accounts that grants access to the table\",\"reason\":\"Explanation: Amazon DynamoDB supports identity-based policies only. The best practice method to assign permissions to the table is to create a permissions policy that grants access to the table and assigning that policy to an IAM group that contains the Developer’s user accounts. This will provide all users with accounts in the IAM group with the access required to access the DynamoDB table. \\n CORRECT : \\\"Attach a permissions policy to an IAM group containing the Developer’s IAM user accounts that grants access to the table\\\" is the correct answer. \\n INCORRECT : \\\"Attach a resource-based policy to the table and add an IAM group containing the Developer’s IAM user accounts as a Principal in the policy\\\" is incorrect as you cannot assign resource-based policies to DynamoDB tables. \\n INCORRECT : \\\"Create an AWS KMS resource-based policy to a CMK and grant the developer’s user accounts the permissions to decrypt data in the table using the CMK\\\" is incorrect as the questions requires that the Developers can access the table, not to be able to decrypt data. \\n INCORRECT : \\\"Create a shared user account and attach a permissions policy granting access to the table. Instruct the Developer’s to login with the user account\\\" is incorrect as this is against AWS best practice. You should never share user accounts. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/using-identity-based-policies.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-dynamodb/\"}",
    "{\"question\":\"A company uses continuous integration and continuous delivery (CI/CD) systems. A Developer needs to automate the deployment of a software package to Amazon EC2 instances as well as to on-premises virtual servers. Which AWS service can be used for the software deployment?\",\"choices\":[\"AWS CodePipeline\",\"AWS CloudBuild\",\"AWS Elastic Beanstalk\",\"AWS CodeDeploy\"],\"answer\":\"AWS CodeDeploy\",\"reason\":\"Explanation: CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises instances, serverless Lambda functions, or Amazon ECS services. CodeDeploy can deploy application content that runs on a server and is stored in Amazon S3 buckets, GitHub repositories, or Bitbucket repositories. CodeDeploy can also deploy a serverless Lambda function. You do not need to make changes to your existing code before you can use CodeDeploy. The image below shows the flow of a typical CodeDeploy in-place deployment. The above deployment could also be directed at on-premises servers. Therefore, the best answer is to use AWS CodeDeploy to deploy the software package to both EC2 instances and on-premises virtual servers. \\n CORRECT : \\\"AWS CodeDeploy\\\" is the correct answer. \\n INCORRECT : \\\"AWS CodePipeline\\\" is incorrect. AWS CodePipeline is a fully managed continuous delivery service that helps you automate your release pipelines for fast and reliable application and infrastructure updates. You can use CodeDeploy in a CodePipeline pipeline however it is actually CodeDeploy that deploys the software packages. \\n INCORRECT : \\\"AWS CloudBuild\\\" is incorrect as this is a build tool, not a deployment tool. \\n INCORRECT : \\\"AWS Elastic Beanstalk\\\" is incorrect as you cannot deploy software packages to on-premise virtual servers using Elastic BeanstalkReferences:https://docs.aws.amazon.com/codedeploy/latest/userguide/welcome.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-developer-tools/\"}",
    "{\"question\":\"A Developer is deploying an update to a serverless application that includes AWS Lambda using the AWS Serverless Application Model (SAM). The traffic needs to move from the old Lambda version to the new Lambda version gradually, within the shortest period of time. Which deployment configuration is MOST suitable for these requirements?\",\"choices\":[\"CodeDeployDefault. LambdaCanary10Percent5Minutes\",\"CodeDeployDefault. HalfAtATime\",\"CodeDeployDefault. LambdaLinear10PercentEvery1Minute\",\"CodeDeployDefault. LambdaLinear10PercentEvery2Minutes\"],\"answer\":\"CodeDeployDefault. LambdaCanary10Percent5Minutes\",\"reason\":\"Explanation: If you use AWS SAM to create your serverless application, it comes built-in with CodeDeploy to provide gradual Lambda deployments. With just a few lines of configuration, AWS SAM does the following for you:Deploys new versions of your Lambda function, and automatically creates aliases that point to the new version. Gradually shifts customer traffic to the new version until you're satisfied that it's working as expected, or you roll back the update. Defines pre-traffic and post-traffic test functions to verify that the newly deployed code is configured correctly and your application operates as expected. Rolls back the deployment if CloudWatch alarms are triggered. There are several options for how CodeDeploy shifts traffic to the new Lambda version. You can choose from the following:Canary: Traffic is shifted in two increments. You can choose from predefined canary options. The options specify the percentage of traffic that's shifted to your updated Lambda function version in the first increment, and the interval, in minutes, before the remaining traffic is shifted in the second increment. Linear: Traffic is shifted in equal increments with an equal number of minutes between each increment. You can choose from predefined linear options that specify the percentage of traffic that's shifted in each increment and the number of minutes between each increment. All-at-once: All traffic is shifted from the original Lambda function to the updated Lambda function version at once. Therefore CodeDeployDefault. LambdaCanary10Percent5Minutes is the best answer as this will shift 10 percent of the traffic and then after 5 minutes shift the remainder of the traffic. The entire deployment will take 5 minutes to cut over. \\n CORRECT : \\\"CodeDeployDefault. LambdaCanary10Percent5Minutes\\\" is the correct answer. \\n INCORRECT : \\\"CodeDeployDefault. HalfAtATime\\\" is incorrect as this is a CodeDeploy traffic shifting strategy that is not applicable to AWS Lambda. You can use Half at a Time with EC2 and on-premises instances. \\n INCORRECT : \\\"CodeDeployDefault. LambdaLinear10PercentEvery1Minute\\\" is incorrect as this option will take longer. CodeDeploy will shift 10 percent every 1 minute and therefore the deployment time will be 10 minutes. \\n INCORRECT : \\\"CodeDeployDefault. LambdaLinear10PercentEvery2Minutes\\\" is incorrect as this option will take longer. CodeDeploy will shift 10 percent every 2 minutes and therefore the deployment time will be 20 minutes. References: https://docs.aws.amazon.com/serverless-application-model/latest/developerguide/automating-updates-to-serverless-apps.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-sam/https://digitalcloud.training/aws-developer-tools/\"}",
    "{\"question\":\"A static website is hosted on Amazon S3 using the bucket name of dctlabs. com. Somehtml pages on the site use JavaScript to download images that are located in the bucket https://dctlabsimages. s3. amazonaws. com/ Users have reported that the images are not being displayed. What is the MOST likely cause?\",\"choices\":[\"Cross Origin Resource Sharing is not enabled on the dctlabsimages bucket\",\"The dctlabsimages bucket is not in the same region as the dctlabs. com bucket\",\"Amazon S3 Transfer Acceleration should be enabled on the dctlabs. com bucket\",\"Cross Origin Resource Sharing is not enabled on the dctlabs. com bucket\"],\"answer\":\"Cross Origin Resource Sharing is not enabled on the dctlabsimages bucket\",\"reason\":\"Explanation: Cross-origin resource sharing (CORS) defines a way for client web applications that are loaded in one domain to interact with resources in a different domain. With CORS support, you can build rich client-side web applications with Amazon S3 and selectively allow cross-origin access to your Amazon S3 resources. To configure your bucket to allow cross-origin requests, you create a CORS configuration, which is an XML document with rules that identify the origins that you will allow to access your bucket, the operations (HTTP methods) that will support for each origin, and other operation-specific information. In this case, you would apply the CORS configuration to the dctlabsimages bucket so that it will allow GET requests from the dctlabs. com origin. \\n CORRECT : \\\"Cross Origin Resource Sharing is not enabled on the dctlabsimages bucket\\\" is the correct answer. \\n INCORRECT : \\\"Cross Origin Resource Sharing is not enabled on the dctlabs. com bucket\\\" is incorrect as in this case the images that are being blocked are located in the dctlabsimages bucket. You need to apply the CORS configuration to the dctlabsimages bucket so it allows requests from the dctlabs. com origin. \\n INCORRECT : \\\"The dctlabsimages bucket is not in the same region as the dctlabs. com bucket\\\" is incorrect as it doesn’t matter what regions the buckets are in. \\n INCORRECT : \\\"Amazon S3 Transfer Acceleration should be enabled on the dctlabs. com bucket\\\" is incorrect as this feature of Amazon S3 is used to speed uploads to S3. References: https://docs.aws.amazon.com/AmazonS3/latest/dev/cors.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-s3-and-glacier/\"}",
    "{\"question\":\"A Developer has deployed an AWS Lambda function and an Amazon DynamoDB table. The function code returns data from the DynamoDB table when it receives a request. The Developer needs to implement a front end that can receive HTTP GET requests and proxy the request information to the Lambda function. What is the SIMPLEST and most COST-EFFECTIVE solution?\",\"choices\":[\"Implement an API Gateway API with Lambda proxy integration\",\"Implement an API Gateway API with a POST method\",\"Implement an Elastic Load Balancer with a Lambda function target\",\"Implement an Amazon Cognito User Pool with a Lambda proxy integration\"],\"answer\":\"Implement an API Gateway API with Lambda proxy integration\",\"reason\":\"Explanation: Amazon API Gateway Lambda proxy integration is a simple, powerful, and nimble mechanism to build an API with a setup of a single API method. The Lambda proxy integration allows the client to call a single Lambda function in the backend. The function accesses many resources or features of other AWS services, including calling other Lambda functions. In Lambda proxy integration, when a client submits an API request, API Gateway passes to the integrated Lambda function the raw request as-is, except that the order of the request parameters is not preserved. This request data includes the request headers, query string parameters, URL path variables, payload, and API configuration data. This solution provides a front end that can listen for HTTP GET requests and then proxy them to the Lambda function and is the simplest option to implement and also the most cost-effective. \\n CORRECT : \\\"Implement an API Gateway API with Lambda proxy integration\\\" is the correct answer. \\n INCORRECT : \\\"Implement an API Gateway API with a POST method\\\" is incorrect as a GET method should be implemented. A GET method is a request for data whereas a POST method is a request to upload data. \\n INCORRECT : \\\"Implement an Elastic Load Balancer with a Lambda function target\\\" is incorrect as though you can do this it is not the simplest or most cost-effective solution. \\n INCORRECT : \\\"Implement an Amazon Cognito User Pool with a Lambda proxy integration\\\" is incorrect as you cannot create Lambda proxy integrations with Cognito. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/set-up-lambda-proxy-integrations.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-api-gateway/\"}",
    "{\"question\":\"A Developer has created the code for a Lambda function saved the code in a file named lambda_function. py. He has also created a template that named template. yaml. The following code is included in the template file:AWSTemplateFormatVersion: '2010-09-09'Transform: 'AWS::Serverless-2016-10-31'Resources:  microservicehttpendpointpython3:    Type: 'AWS::Serverless::Function'    Properties:      Handler: lambda_function. lambda_handler      CodeUri: . What commands can the Developer use to prepare and then deploy this template? (Select TWO. )\",\"choices\":[\"Run aws cloudformation package and then aws cloudformation deploy\",\"Run sam package and then sam deploy\",\"Run aws cloudformation compile and then aws cloudformation deploy\",\"Run sam build and then sam package\",\"Run aws serverless package and then aws serverless deploy\"],\"answer\":[\"Run aws cloudformation package and then aws cloudformation deploy\",\"Run sam package and then sam deploy\"],\"reason\":\"Explanation: The template shown is an AWS SAM template for deploying a serverless application. This can be identified by the template header: Transform: 'AWS::Serverless-2016-10-31'The Developer will need to package and then deploy the template. To do this the source code must be available in the same directory or referenced using the “codeuri” parameter. Then, the Developer can use the “aws cloudformation package” or “sam package” commands to prepare the local artifacts (local paths) that your AWS CloudFormation template references. The command uploads local artifacts, such as source code for an AWS Lambda function or a Swagger file for an AWS API Gateway REST API, to an S3 bucket. The command returns a copy of your template, replacing references to local artifacts with the S3 location where the command uploaded the artifacts. Once that is complete the template can be deployed using the “aws cloudformation deploy” or “sam deploy” commands. Therefore, the developer has two options to prepare and then deploy this package:Run aws cloudformation package and then aws cloudformation deployRun sam package and then sam deployCORRECT: \\\"Run aws cloudformation package and then aws cloudformation deploy\\\" is a correct answer. \\n INCORRECT : \\\"Run sam package and then sam deploy\\\" is also a correct answer. \\n INCORRECT : \\\"Run aws cloudformation compile and then aws cloudformation deploy\\\" is incorrect as the “compile” command should be replaced with the “package” command. \\n INCORRECT : \\\"Run sam build and then sam package\\\" is incorrect as the Developer needs to run the “package” command first and then the “deploy” command to actually deploy the function. \\n INCORRECT : \\\"Run aws serverless package and then aws serverless deploy\\\" is incorrect as there is no AWS CLI command named “serverless”. References: https://docs.aws.amazon.com/cli/latest/reference/cloudformation/package.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-sam/\"}",
    "{\"question\":\"A development team have deployed a new application and users have reported some performance issues. The developers need to enable monitoring for specific metrics with a data granularity of one second. How can this be achieved?\",\"choices\":[\"Do nothing, CloudWatch uses standard resolution metrics by default\",\"Create custom metrics and configure them as standard resolution\",\"Create custom metrics and enable detailed monitoring\",\"Create custom metrics and configure them as high resolution\"],\"answer\":\"Create custom metrics and configure them as high resolution\",\"reason\":\"Explanation: You can publish your own metrics to CloudWatch using the AWS CLI or an API. You can view statistical graphs of your published metrics with the AWS Management Console. CloudWatch stores data about a metric as a series of data points. Each data point has an associated time stamp. You can even publish an aggregated set of data points called a statistic set. Each metric is one of the following:Standard resolution, with data having a one-minute granularityHigh resolution, with data at a granularity of one secondMetrics produced by AWS services are standard resolution by default. When you publish a custom metric, you can define it as either standard resolution or high resolution. When you publish a high-resolution metric, CloudWatch stores it with a resolution of 1 second, and you can read and retrieve it with a period of 1 second, 5 seconds, 10 seconds, 30 seconds, or any multiple of 60 seconds. High-resolution metrics can give you more immediate insight into your application's sub-minute activity. Keep in mind that every PutMetricData call for a custom metric is charged, so calling PutMetricData more often on a high-resolution metric can lead to higher charges. Therefore, the best action to take is to Create custom metrics and configure them as high resolution. This will ensure that granularity can be down to 1 second. \\n CORRECT : \\\"Create custom metrics and configure them as high resolution\\\" is the correct answer. \\n INCORRECT : \\\"Do nothing, CloudWatch uses standard resolution metrics by default\\\" is incorrect as standard resolution has a granularity of one-minute. \\n INCORRECT : \\\"Create custom metrics and configure them as standard resolution\\\" is incorrect as standard resolution has a granularity of one-minute. \\n INCORRECT : \\\"Create custom metrics and enable detailed monitoring\\\" is incorrect as detailed monitoring has a granularity of one-minute. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/monitoring/publishingMetrics.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cloudwatch/\"}",
    "{\"question\":\"A company is developing a game for the Android and iOS platforms. The mobile game will securely store user game history and other data locally on the device. The company would like users to be able to use multiple mobile devices and synchronize data between devices. Which service can be used to synchronize the data across mobile devices without the need to create a backend application?\",\"choices\":[\"AWS Lambda\",\"Amazon API Gateway\",\"Amazon DynamoDB\",\"Amazon Cognito\"],\"answer\":\"Amazon Cognito\",\"reason\":\"Explanation: Amazon Cognito lets you save end user data in datasets containing key-value pairs. This data is associated with an Amazon Cognito identity, so that it can be accessed across logins and devices. To sync this data between the Amazon Cognito service and an end user’s devices, invoke the synchronize method. Each dataset can have a maximum size of 1 MB. You can associate up to 20 datasets with an identity. The Amazon Cognito Sync client creates a local cache for the identity data. Your app talks to this local cache when it reads and writes keys. This guarantees that all of your changes made on the device are immediately available on the device, even when you are offline. When the synchronize method is called, changes from the service are pulled to the device, and any local changes are pushed to the service. At this point the changes are available to other devices to synchronize. \\n CORRECT : \\\"Amazon Cognito\\\" is the correct answer. \\n INCORRECT : \\\"AWS Lambda\\\" is incorrect. AWS Lambda provides serverless functions that run your code, it is not used for mobile client data synchronization. \\n INCORRECT : \\\"Amazon API Gateway\\\" is incorrect as API Gateway provides APIs for traffic coming into AWS. It is not used for mobile client data synchronization. \\n INCORRECT : \\\"Amazon DynamoDB\\\" is incorrect as DynamoDB is a NoSQL database. It is not used for mobile client data synchronization. References: https://docs.aws.amazon.com/cognito/latest/developerguide/synchronizing-data.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cognito/\"}",
    "{\"question\":\"A serverless application is used to process customer information and outputs a JSON file to an Amazon S3 bucket. AWS Lambda is used for processing the data. The data is sensitive and should be encrypted. How can a Developer modify the Lambda function to ensure the data is encrypted before it is uploaded to the S3 bucket?\",\"choices\":[\"Use the GenerateDataKey API, then use the data key to encrypt the file using the Lambda code\",\"Enable server-side encryption on the S3 bucket and create a policy to enforce encryption\",\"Use the S3 managed key and call the GenerateDataKey API to encrypt the file\",\"Use the default KMS key for S3 and encrypt the file using the Lambda code\"],\"answer\":\"Use the GenerateDataKey API, then use the data key to encrypt the file using the Lambda code\",\"reason\":\"Explanation: The GenerateDataKey API is used with the AWS KMS services and generates a unique symmetric data key. This operation returns a plaintext copy of the data key and a copy that is encrypted under a customer master key (CMK) that you specify. You can use the plaintext key to encrypt your data outside of AWS KMS and store the encrypted data key with the encrypted data. For this scenario we can use GenerateDataKey to obtain an encryption key from KMS that we can then use within the function code to encrypt the file. This ensures that the file is encrypted BEFORE it is uploaded to Amazon S3. \\n CORRECT : \\\"Use the GenerateDataKey API, then use the data key to encrypt the file using the Lambda code\\\" is the correct answer. \\n INCORRECT : \\\"Enable server-side encryption on the S3 bucket and create a policy to enforce encryption\\\" is incorrect. This would not encrypt data before it is uploaded as S3 would only encrypt the data as it is written to storage. \\n INCORRECT : \\\"Use the S3 managed key and call the GenerateDataKey API to encrypt the file\\\" is incorrect as you do not use an encryption key to call KMS. You call KMS with the GenerateDataKey API to obtain an encryption key. Also, the S3 managed key can only be used within the S3 service. \\n INCORRECT : \\\"Use the default KMS key for S3 and encrypt the file using the Lambda code\\\" is incorrect. You cannot use the default KMS key for S3 within the Lambda code as it can only be used within the S3 service. References: https://docs.aws.amazon.com/kms/latest/APIReference/API_GenerateDataKey.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-kms/\"}",
    "{\"question\":\"A Developer received the following error when attempting to launch an Amazon EC2 instance using the AWS CLI. An error occurred (UnauthorizedOperation) when calling the RunInstances operation: You are not authorized to perform this operation. Encoded authorization failure message: VNVaHFdCohROkbyT_rIXoRyNTp7vXFJCqnGiwPuyKnsSVf-WSSGK_06H3vKnrkUa3qx5D40hqj9HEG8kznr04Acmi6lvc8m51tfqtsomFSDylK15x96ZrxMW7MjDJLrMkM0BasPvy8ixo1wi6X2b0C-J1ThyWU9IcrGd7WbaRDOiGbBhJtKs1z01WSn2rVa5_7sr5PwEK-ARrC9y5Pl54pmeF6wh7QhSv2pFO0y39WVBajL2GmByFmQ4p8s-6Lcgxy23b4NJdJwWOF4QGxK9HcKof1VTVZ2oIpsI-dH6_0t2DI0BTwaIgmaT7ldontI1p7OGz-3wPgXm67x2NVNgaK63zPxjYNbpl32QuXLKUKNlB9DdkSdoLvsuFIvf-lQOXLPHnZKCWMqrkI87eqKHYpYKyV5c11TIZTAJ3MntTGO_TJ4U9ySYvTzU2LgswYOtKF_O76-13fryGG5dhgOW5NxwCWBj6WT2NSJvqOeLykAFjR_ET4lM6Dl1XYfQITWCqIzlvlQdLmHJ1jqjp4gW56VcQCdqozLv2UAg8IdrZIXd0OJ047RQcvvN1IyZN0ElL7dR6RzAAQrftoKMRhZQng6THZs8PZM6wep6-yInzwfg8J5_FW6G_PwYqO-4VunVtJSTzM_F_8kojGlRmzqy7eCk5or__bIisUoslwWhat action should the Developer perform to make this error more human-readable?\",\"choices\":[\"Make a call to AWS KMS to decode the message\",\"Use the AWS STS decode-authorization-message API to decode the message\",\"Use an open source decoding library to decode the message\",\"Use the AWS IAM decode-authorization-message API to decode this message\"],\"answer\":\"Use the AWS STS decode-authorization-message API to decode the message\",\"reason\":\"Explanation: The AWS STS decode-authorization-message API decodes additional information about the authorization status of a request from an encoded message returned in response to an AWS request. The output is then decoded into a more human-readable output that can be viewed in a JSON editor. The following example is the decoded output from the error shown in the question:{    \\\"DecodedMessage\\\": \\\"{\\\\\\\"allowed\\\\\\\":false,\\\\\\\"explicitDeny\\\\\\\":false,\\\\\\\"matchedStatements\\\\\\\":{\\\\\\\"items\\\\\\\":[]},\\\\\\\"failures\\\\\\\":{\\\\\\\"items\\\\\\\":[]},\\\\\\\"context\\\\\\\":{\\\\\\\"principal\\\\\\\":{\\\\\\\"id\\\\\\\":\\\\\\\"AIDAXP4J2EKU7YXXG3EJ4\\\\\\\",\\\\\\\"name\\\\\\\":\\\\\\\"Paul\\\\\\\",\\\\\\\"arn\\\\\\\":\\\\\\\"arn:aws:iam::515148227241:user/Paul\\\\\\\"},\\\\\\\"action\\\\\\\":\\\\\\\"ec2:RunInstances\\\\\\\",\\\\\\\"resource\\\\\\\":\\\\\\\"arn:aws:ec2:ap-southeast-2:515148227241:instance/*\\\\\\\",\\\\\\\"conditions\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"key\\\\\\\":\\\\\\\"ec2:InstanceMarketType\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"on-demand\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"aws:Resource\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"instance/*\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"aws:Account\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"515148227241\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"ec2:AvailabilityZone\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"ap-southeast-2a\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"ec2:ebsOptimized\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"false\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"ec2:IsLaunchTemplateResource\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"false\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"ec2:InstanceType\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"t2. micro\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"ec2:RootDeviceType\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"ebs\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"aws:Region\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"ap-southeast-2\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"aws:Service\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"ec2\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"ec2:InstanceID\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"*\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"aws:Type\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"instance\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"ec2:Tenancy\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"default\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"ec2:Region\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"ap-southeast-2\\\\\\\"}]}},{\\\\\\\"key\\\\\\\":\\\\\\\"aws:ARN\\\\\\\",\\\\\\\"values\\\\\\\":{\\\\\\\"items\\\\\\\":[{\\\\\\\"value\\\\\\\":\\\\\\\"arn:aws:ec2:ap-southeast-2:515148227241:instance/*\\\\\\\"}]}}]}}}\\\"}Therefore, the best answer is to use the AWS STS decode-authorization-message API to decode the message. \\n CORRECT : \\\"Use the AWS STS decode-authorization-message API to decode the message\\\" is the correct answer. \\n INCORRECT : \\\"Make a call to AWS KMS to decode the message\\\" is incorrect as the message is not encrypted, it is base64 encoded. \\n INCORRECT : \\\"Use an open source decoding library to decode the message\\\" is incorrect as you can use the AWS STS decode-authorization-message API. \\n INCORRECT : \\\"Use the AWS IAM decode-authorization-message API to decode this message\\\" is incorrect as the decode-authorization-message API is associated with STS, not IAM. References: https://docs.aws.amazon.com/cli/latest/reference/sts/decode-authorization-message.html\"}",
    "{\"question\":\"A Developer is trying to make API calls using AWS SDK. The IAM user credentials used by the application require multi-factor authentication for all API calls. Which method should the Developer use to access the multi-factor authentication protected API?\",\"choices\":[\"GetFederationToken\",\"GetCallerIdentity\",\"GetSessionToken\",\"DecodeAuthorizationMessage\"],\"answer\":\"GetSessionToken\",\"reason\":\"Explanation: The GetSessionToken API call returns a set of temporary credentials for an AWS account or IAM user. The credentials consist of an access key ID, a secret access key, and a security token. Typically, you use GetSessionToken if you want to use MFA to protect programmatic calls to specific AWS API operationsTherefore, the Developer can use GetSessionToken with an MFA device to make secure API calls using the AWS SDK. \\n CORRECT : \\\"GetSessionToken\\\" is the correct answer. \\n INCORRECT : \\\"GetFederationToken\\\" is incorrect as this is used with federated users to return a set of temporary security credentials (consisting of an access key ID, a secret access key, and a security token). \\n INCORRECT : \\\"GetCallerIdentity\\\" is incorrect as this API action returns details about the IAM user or role whose credentials are used to call the operation. \\n INCORRECT : \\\"DecodeAuthorizationMessage\\\" is incorrect as this API action decodes additional information about the authorization status of a request from an encoded message returned in response to an AWS request. References: https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-iam/\"}",
    "{\"question\":\"A mobile application has hundreds of users. Each user may use multiple devices to access the application. The Developer wants to assign unique identifiers to these users regardless of the device they use. Which of the following methods should be used to obtain unique identifiers?\",\"choices\":[\"Create a user table in Amazon DynamoDB as key-value pairs of users and their devices. Use these keys as unique identifiers\",\"Use IAM-generated access key IDs for the users as the unique identifier, but do not store secret keys\",\"Implement developer-authenticated identities by using Amazon Cognito, and get credentials for these identities\",\"Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier\"],\"answer\":\"Implement developer-authenticated identities by using Amazon Cognito, and get credentials for these identities\",\"reason\":\"Explanation: Amazon Cognito supports developer authenticated identities, in addition to web identity federation through Facebook (Identity Pools), Google (Identity Pools), Login with Amazon (Identity Pools), and Sign in with Apple (identity Pools). With developer authenticated identities, you can register and authenticate users via your own existing authentication process, while still using Amazon Cognito to synchronize user data and access AWS resources. Using developer authenticated identities involves interaction between the end user device, your backend for authentication, and Amazon Cognito. Therefore, the Developer can implement developer-authenticated identities by using Amazon Cognito, and get credentials for these identities. \\n CORRECT : \\\"Implement developer-authenticated identities by using Amazon Cognito, and get credentials for these identities\\\" is the correct answer. \\n INCORRECT : \\\"Create a user table in Amazon DynamoDB as key-value pairs of users and their devices. Use these keys as unique identifiers\\\" is incorrect as this solution would require additional application logic and would be more complex. \\n INCORRECT : \\\"Use IAM-generated access key IDs for the users as the unique identifier, but do not store secret keys\\\" is incorrect as it is not a good practice to provide end users of mobile applications with IAM user accounts and access keys. Cognito is a better solution for this use case. \\n INCORRECT : \\\"Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier\\\" is incorrect. AWS Cognito is better suited to mobile users and with developer authenticated identities the users can be assigned unique identities. References: https://docs.aws.amazon.com/cognito/latest/developerguide/developer-authenticated-identities.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cognito/\"}",
    "{\"question\":\"A small team of Developers require access to an Amazon S3 bucket. An admin has created a resource-based policy. Which element of the policy should be used to specify the ARNs of the user accounts that will be granted access?\",\"choices\":[\"Sid\",\"Condition\",\"Principal\",\"Id\"],\"answer\":\"Principal\",\"reason\":\"Explanation: Use the Principal element in a policy to specify the principal that is allowed or denied access to a resource. You cannot use the Principal element in an IAM identity-based policy. You can use it in the trust policies for IAM roles and in resource-based policies. Resource-based policies are policies that you embed directly in an IAM resource. \\n CORRECT : \\\"Principal\\\" is the correct answer. \\n INCORRECT : \\\"Condition\\\" is incorrect. The Condition element (or Condition block) lets you specify conditions for when a policy is in effect. \\n INCORRECT : \\\"Sid\\\" is incorrect. The Sid (statement ID) is an optional identifier that you provide for the policy statement. \\n INCORRECT : \\\"Id\\\" is incorrect. The Id element specifies an optional identifier for the policy. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_elements_principal.html Save time with our AWS cheat sheets:https://digitalcloud.training/certification-training/aws-developer-associate/aws-security-identity-and-compliance/\"}",
    "{\"question\":\"A software firm is introducing a multimedia application that allows guest users to sample content before deciding to fully register. The company requires a mechanism that can identify users who have already created an account and to monitor the quantity of guest users who ultimately register. What two actions would best satisfy these needs? (Select TWO. )\",\"choices\":[\"Use Amazon Cognito User Pools for managing user registration and authentication. \",\"Implement AWS IAM roles to provide distinct permissions for guest users and registered users. \",\"Use AWS Glue to oversee user registration and account conversion. \",\"Deploy Amazon S3 for managing user data and monitoring account conversions. \",\"Use AWS Lambda to track the transitions from guest to full accounts. \"],\"answer\":[\"Use Amazon Cognito User Pools for managing user registration and authentication. \",\"Implement AWS IAM roles to provide distinct permissions for guest users and registered users. \"],\"reason\":\"Explanation: Amazon Cognito User Pools serve as a user directory that offers the backend capabilities required for user registration, authentication, and account recovery. This makes it the right solution for identifying users who have created accounts. AWS IAM roles can be used to provide distinct permissions for guest users and registered users. While they do not directly track account conversions, they can help delineate between different user states. \\n CORRECT : \\\"Implement AWS IAM roles to provide distinct permissions for guest users and registered users\\\" is a correct answer (as explained above. )\\n CORRECT : \\\"Use Amazon Cognito User Pools for managing user registration and authentication\\\" is also a correct answer (as explained above. )\\n INCORRECT : \\\"Use AWS Glue to oversee user registration and account conversion\\\" is incorrect. AWS Glue is an ETL service designed for easy data preparation and loading for analytics, not for managing user registration or authentication. \\n INCORRECT : \\\"Deploy Amazon S3 for managing user data and monitoring account conversions\\\" is incorrect. Amazon S3 is a storage service and is not designed for managing user registration or tracking account conversions. \\n INCORRECT : \\\"Use AWS Lambda to track the transitions from guest to full accounts\\\" is incorrect. AWS Lambda is a serverless compute service and is not typically used for managing user registration or tracking conversions. References: https://docs.aws.amazon.com/cognito/latest/developerguide/cognito-user-identity-pools.htmlhttps://docs.aws.amazon.com/IAM/latest/UserGuide/id_roles.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cognito/\"}",
    "{\"question\":\"A Developer is deploying an Amazon ECS update using AWS CodeDeploy. In the appspec. yaml file, which of the following is a valid structure for the order of hooks that should be specified?\",\"choices\":[\"BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic\",\"BeforeInstall > AfterInstall > ApplicationStart > ValidateService\",\"BeforeAllowTraffic > AfterAllowTraffic\",\"BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic\"],\"answer\":\"BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic\",\"reason\":\"Explanation: The content in the 'hooks' section of the AppSpec file varies, depending on the compute platform for your deployment. The 'hooks' section for an EC2/On-Premises deployment contains mappings that link deployment lifecycle event hooks to one or more scripts. The 'hooks' section for a Lambda or an Amazon ECS deployment specifies Lambda validation functions to run during a deployment lifecycle event. If an event hook is not present, no operation is executed for that event. This section is required only if you are running scripts or Lambda validation functions as part of the deployment. The following code snippet shows a valid example of the structure of hooks for an Amazon ECS deployment:Therefore, in this scenario a valid structure for the order of hooks that should be specified in the appspec. yml file is: BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTrafficCORRECT: \\\"BeforeInstall > AfterInstall > AfterAllowTestTraffic > BeforeAllowTraffic > AfterAllowTraffic\\\" is the correct answer. \\n INCORRECT : \\\"BeforeInstall > AfterInstall > ApplicationStart > ValidateService\\\" is incorrect as this would be valid for Amazon EC2. \\n INCORRECT : \\\"BeforeAllowTraffic > AfterAllowTraffic\\\" is incorrect as this would be valid for AWS Lambda. \\n INCORRECT : \\\"BeforeBlockTraffic > AfterBlockTraffic > BeforeAllowTraffic > AfterAllowTraffic\\\" is incorrect as this is a partial listing of hooks for Amazon EC2 but is incomplete. References: https://docs.aws.amazon.com/codedeploy/latest/userguide/reference-appspec-file-structure-hooks.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-developer-tools/\"}",
    "{\"question\":\"A Developer is migrating Docker containers to Amazon ECS. A large number of containers will be deployed across some newly deployed ECS containers instances using the same instance type. High availability is provided within the microservices architecture. Which task placement strategy requires the LEAST configuration for this scenario?\",\"choices\":[\"binpack\",\"random\",\"spread\",\"Fargate\"],\"answer\":\"random\",\"reason\":\"Explanation: When a task that uses the EC2 launch type is launched, Amazon ECS must determine where to place the task based on the requirements specified in the task definition, such as CPU and memory. Similarly, when you scale down the task count, Amazon ECS must determine which tasks to terminate. You can apply task placement strategies and constraints to customize how Amazon ECS places and terminates tasks. Task placement strategies and constraints are not supported for tasks using the Fargate launch type. By default, Fargate tasks are spread across Availability Zones. A task placement strategy is an algorithm for selecting instances for task placement or tasks for termination. For example, Amazon ECS can select instances at random, or it can select instances such that tasks are distributed evenly across a group of instances. Amazon ECS supports the following task placement strategies:binpackPlace tasks based on the least available amount of CPU or memory. This minimizes the number of instances in use. randomPlace tasks randomly. spreadPlace tasks evenly based on the specified value. Accepted values are instanceId (or host, which has the same effect), or any platform or custom attribute that is applied to a container instance, such as attribute:ecs. availability-zone. Service tasks are spread based on the tasks from that service. Standalone tasks are spread based on the tasks from the same task group. Therefore, for this scenario the random task placement strategy is most suitable as it requires the least configuration. \\n CORRECT : \\\"random\\\" is the correct answer. \\n INCORRECT : \\\"spread\\\" is incorrect. As high availability is taken care of within the containers there is no need to use a spread strategy to ensure HA. \\n INCORRECT : \\\"binpack\\\" is incorrect as there is no need to pack the containers onto the fewest instances based on CPU or memory. \\n INCORRECT : \\\"Fargate\\\" is incorrect as this is not a task placement strategy, it is a serverless service for running containers. References: https://docs.aws.amazon.com/AmazonECS/latest/developerguide/task-placement-strategies.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-ecs-and-eks/\"}",
    "{\"question\":\"A three-tier application is being migrated from an on-premises data center. The application includes an Apache Tomcat web tier, an application tier running on Linux, and a MySQL back end. A Developer must refactor the application to run on the AWS cloud. The cloud-based application must be fault tolerant and elastic. How can the Developer refactor the web tier and application tier? (Select TWO. )\",\"choices\":[\"Create an Amazon CloudFront distribution for the web tier\",\"Create an Auto Scaling group of EC2 instances for both the web tier and application tier\",\"Use a multi-AZ Amazon RDS database for the back end using the MySQL engine\",\"Implement an Elastic Load Balancer for the application tier\",\"Implement an Elastic Load Balancer for both the web tier and the application tier\"],\"answer\":[\"Create an Auto Scaling group of EC2 instances for both the web tier and application tier\",\"Implement an Elastic Load Balancer for both the web tier and the application tier\"],\"reason\":\"Explanation: The key requirements in this scenario are to add fault tolerances and elasticity to the web tier and application tier. Note that no specific requirements for the back end have been included. To add elasticity to the web and application tiers the Developer should create Auto Scaling groups of EC2 instances. We know that the application tier runs on Linux and the web tier runs on Apache Tomcat (which could be on Linux or Windows). Therefore, these workloads are suitable for an ASG and this will ensure the number of instances dynamically scales out and in based on actual usage. To add fault tolerance to the web and application tiers the Developer should add an Elastic Load Balancer. This will ensure that if the number of EC2 instances are changed by the ASG, the load balancer is able to distribute traffic to them. This also assists with elasticity. \\n CORRECT : \\\"Create an Auto Scaling group of EC2 instances for both the web tier and application tier\\\" is a correct answer. \\n CORRECT : \\\"Implement an Elastic Load Balancer for both the web tier and the application tier\\\" is also a correct answer. \\n INCORRECT : \\\"Create an Amazon CloudFront distribution for the web tier\\\" is incorrect as CloudFront is used for performance reasons, not elasticity or fault tolerance. You would use CloudFront to get content closer to end users around the world. \\n INCORRECT : \\\"Use a multi-AZ Amazon RDS database for the back end using the MySQL engine\\\" is incorrect as the question does not ask for fault tolerance of the back end, only the web tier and the application tier. \\n INCORRECT : \\\"Implement an Elastic Load Balancer for the application tier\\\" is incorrect. An Elastic Load Balancer should be implemented for both the web tier and the application tier as that is how we ensure fault tolerance and elasticity for both of those tiers. References: https://docs.aws.amazon.com/elasticloadbalancing/latest/userguide/what-is-load-balancing.htmlhttps://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-auto-scaling.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/https://digitalcloud.training/amazon-ec2-auto-scaling/\"}",
    "{\"question\":\"A Developer needs to run some code using Lambda in response to an event and forward the execution result to another application using a pub/sub notification. How can the Developer accomplish this?\",\"choices\":[\"Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SNS\",\"Configure a Lambda “on success” destination and route the execution results to Amazon SNS\",\"Configure a Lambda “on success” destination and route the execution results to Amazon SQS\",\"Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SQS\"],\"answer\":\"Configure a Lambda “on success” destination and route the execution results to Amazon SNS\",\"reason\":\"Explanation: With Destinations, you  can send asynchronous function execution results to a destination resource without writing code. A function execution result includes version, timestamp, request context, request payload, response context, and response payload. For each execution status (i. e. Success and Failure), you can choose one destination from four options: another Lambda function, an SNS topic, an SQS standard queue, or EventBridge. For this scenario, the code will be run by Lambda and the execution result will then be sent to the SNS topic. The application that is subscribed to the SNS topics will then receive the notification. \\n CORRECT : \\\"Configure a Lambda “on success” destination and route the execution results to Amazon SNS\\\" is the correct answer. \\n INCORRECT : \\\"Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SNS\\\" is incorrect as CloudWatch Events is used to track changes in the state of AWS resources. To forward execution results from Lambda a destination should be used. \\n INCORRECT : \\\"Configure a Lambda “on success” destination and route the execution results to Amazon SQS\\\" is incorrect as SQS is a message queue not a pub/sub notification service. \\n INCORRECT : \\\"Configure a CloudWatch Events alarm the triggers based on Lambda execution success and route the execution results to Amazon SQS\\\" is incorrect as CloudWatch Events is used to track changes in the state of AWS resources. To forward execution results from Lambda a destination should be used (with an SNS topic). References: https://aws. amazon. com/about-aws/whats-new/2019/11/aws-lambda-supports-destinations-for-asynchronous-invocations/Save time with our AWS cheat sheets:https://digitalcloud.training/aws-lambda/\"}",
    "{\"question\":\"A company currently runs a number of legacy automated batch processes for system update management and operational activities. The company are looking to refactor these processes and require a service that can coordinate multiple AWS services into serverless workflows. What is the MOST suitable service for this requirement?\",\"choices\":[\"Amazon SWF\",\"AWS Batch\",\"AWS Step Functions\",\"AWS Lambda\"],\"answer\":\"AWS Step Functions\",\"reason\":\"Explanation: AWS Step Functions is a web service that enables you to coordinate the components of distributed applications and microservices using visual workflows. You build applications from individual components that each perform a discrete function, or task, allowing you to scale and change applications quickly. Step Functions provides a reliable way to coordinate components and step through the functions of your application. Step Functions offers a graphical console to visualize the components of your application as a series of steps. It automatically triggers and tracks each step, and retries when there are errors, so your application executes in order and as expected, every time. Step Functions logs the state of each step, so when things go wrong, you can diagnose and debug problems quickly. \\n CORRECT : \\\"AWS Step Functions\\\" is the correct answer. \\n INCORRECT : \\\"Amazon SWF\\\" is incorrect. You can think of Amazon SWF as a fully-managed state tracker and task coordinator in the Cloud. It does not coordinate serverless workflows. \\n INCORRECT : \\\"AWS Batch\\\" is incorrect as this is used to run batch computing jobs on Amazon EC2 and is therefore not serverless. \\n INCORRECT : \\\"AWS Lambda\\\" is incorrect as though it is serverless, it does not provide a native capability to coordinate multiple AWS services. References: https://docs.aws.amazon.com/step-functions/latest/dg/welcome.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-application-integration-services/\"}",
    "{\"question\":\"What does an Amazon SQS delay queue accomplish?\",\"choices\":[\"Messages are hidden for a configurable amount of time when they are first added to the queue\",\"Messages are hidden for a configurable amount of time after they are consumed from the queue\",\"The consumer can poll the queue for a configurable amount of time before retrieving a message\",\"Message cannot be deleted for a configurable amount of time after they are consumed from the queue\"],\"answer\":\"Messages are hidden for a configurable amount of time when they are first added to the queue\",\"reason\":\"Explanation: Delay queues let you postpone the delivery of new messages to a queue for a number of seconds, for example, when your consumer application needs additional time to process messages. If you create a delay queue, any messages that you send to the queue remain invisible to consumers for the duration of the delay period. The default (minimum) delay for a queue is 0 seconds. The maximum is 15 minutes. Therefore, the correct explanation is that with an Amazon SQS Delay Queue messages are hidden for a configurable amount of time when they are first added to the queueCORRECT: \\\"Messages are hidden for a configurable amount of time when they are first added to the queue\\\" is the correct answer. \\n INCORRECT : \\\"Messages are hidden for a configurable amount of time after they are consumed from the queue\\\" is incorrect. They are hidden when they are added to the queue. \\n INCORRECT : \\\"The consumer can poll the queue for a configurable amount of time before retrieving a message\\\" is incorrect. A delay queue simply delays visibility of the message, it does not affect polling behavior. \\n INCORRECT : \\\"Message cannot be deleted for a configurable amount of time after they are consumed from the queue\\\" is incorrect. That is what a visibility timeout achieves. References: https://docs.aws.amazon.com/AWSSimpleQueueService/latest/SQSDeveloperGuide/sqs-delay-queues.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-application-integration-services/\"}",
    "{\"question\":\"An application scans an Amazon DynamoDB table once per day to produce a report. The scan is performed in non-peak hours when production usage uses around 50% of the provisioned throughput. How can you MINIMIZE the time it takes to produce the report without affecting production workloads? (Select TWO. )\",\"choices\":[\"Use a Parallel Scan API operation\",\"Use a Sequential Scan API operation\",\"Increase read capacity units during the scan operation\",\"Use the Limit parameter\",\"Use pagination to divide results into 1 MB pages\"],\"answer\":[\"Use a Parallel Scan API operation\",\"Use the Limit parameter\"],\"reason\":\"Explanation: By default, the Scan operation processes data sequentially. Amazon DynamoDB returns data to the application in 1 MB increments, and an application performs additional Scan operations to retrieve the next 1 MB of data. The larger the table or index being scanned, the more time the Scan takes to complete. In addition, a sequential Scan might not always be able to fully use the provisioned read throughput capacity: Even though DynamoDB distributes a large table's data across multiple physical partitions, a Scan operation can only read one partition at a time. For this reason, the throughput of a Scan is constrained by the maximum throughput of a single partition. To address these issues, the Scan operation can logically divide a table or secondary index into multiple segments, with multiple application workers scanning the segments in parallel. Each worker can be a thread (in programming languages that support multithreading) or an operating system process. To perform a parallel scan, each worker issues its own Scan request with the following parameters:Segment — A segment to be scanned by a particular worker. Each worker should use a different value for Segment. TotalSegments — The total number of segments for the parallel scan. This value must be the same as the number of workers that your application will use. The following diagram shows how a multithreaded application performs a parallel Scan with three degrees of parallelism. To make the most of your table’s provisioned throughput, you’ll want to use the Parallel Scan API operation so that your scan is distributed across your table’s partitions. However, you also need to ensure the scan doesn’t consume your table’s provisioned throughput and cause the critical parts of your application to be throttled. To control the amount of data returned per request, use the Limit parameter. This can help prevent situations where one worker consumes all of the provisioned throughput, at the expense of all other workers. Therefore, the best solution to this problem is to use a parallel scan API operation with the Limit parameter. \\n CORRECT : \\\"Use a Parallel Scan API operation \\\" is the correct answer. \\n CORRECT : \\\"Use the Limit parameter\\\" is also a correct answer. \\n INCORRECT : \\\"Use a Sequential Scan API operation\\\" is incorrect as this would take more time and the question requests that we minimize the time it takes to complete the scan. \\n INCORRECT : \\\"Increase read capacity units during the scan operation\\\" is incorrect as this would increase cost and we still need a solution to ensure we maximize usage of available throughput without affecting production workloads. \\n INCORRECT : \\\"Use pagination to divide results into 1 MB pages\\\" is incorrect as this does only divides the results into pages, it does not segment and limit the amount of throughput used. References: https://aws. amazon. com/blogs/developer/rate-limited-scans-in-amazon-dynamodb/https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Scan.html#Scan. ParallelScanSave time with our AWS cheat sheets:https://digitalcloud.training/category/aws-cheat-sheets/aws-developer-associate/aws-database-dva/\"}",
    "{\"question\":\"A company manages an application that stores data in an Amazon DynamoDB table. The company need to keep a record of all new changes made to the DynamoDB table in another table within the same AWS region. What is the MOST suitable way to deliver this requirement?\",\"choices\":[\"Use Amazon DynamoDB streams\",\"Use CloudWatch events\",\"Use Amazon CloudTrail\",\"Use Amazon DynamoDB snapshots\"],\"answer\":\"Use Amazon DynamoDB streams\",\"reason\":\"Explanation: A DynamoDB stream is an ordered flow of information about changes to items in a DynamoDB table. When you enable a stream on a table, DynamoDB captures information about every modification to data items in the table. Whenever an application creates, updates, or deletes items in the table, DynamoDB Streams writes a stream record with the primary key attributes of the items that were modified. A stream record contains information about a data modification to a single item in a DynamoDB table. You can configure the stream so that the stream records capture additional information, such as the \\\"before\\\" and \\\"after\\\" images of modified items. This is the best way to capture a record of new changes made to the DynamoDB table. Another table can then be populated with this data so the data is stored persistently. \\n CORRECT : \\\"Use Amazon DynamoDB streams\\\" is the correct answer. \\n INCORRECT : \\\"Use CloudWatch events\\\" is incorrect. CloudWatch Events delivers a near real-time stream of system events that describe changes in Amazon Web Services (AWS) resources. However, it does not capture the information that changes in a DynamoDB table so is unsuitable for this purpose. \\n INCORRECT : \\\"Use Amazon CloudTrail\\\" is incorrect as CloudTrail records a history of API calls on your account. It is used for creating an audit trail of events. \\n INCORRECT : \\\"Use Amazon DynamoDB snapshots\\\" is incorrect as snapshots only capture a point in time, they are not used for recording item-level changes. References: https://docs.aws.amazon.com/amazondynamodb/latest/developerguide/Streams.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-dynamodb/\"}",
    "{\"question\":\"An application needs to read up to 100 items at a time from an Amazon DynamoDB. Each item is up to 100 KB in size and all attributes must be retrieved. What is the BEST way to minimize latency?\",\"choices\":[\"Use GetItem and use a projection expression\",\"Use BatchGetItem\",\"Use a Scan operation with pagination\",\"Use a Query operation with a FilterExpression\"],\"answer\":\"Use BatchGetItem\",\"reason\":\"Explanation: The BatchGetItem operation returns the attributes of one or more items from one or more tables. You identify requested items by primary key. A single operation can retrieve up to 16 MB of data, which can contain as many as 100 items. In order to minimize response latency, BatchGetItem retrieves items in parallel. By default, BatchGetItem performs eventually consistent reads on every table in the request. If you want strongly consistent reads instead, you can set ConsistentRead to true for any or all tables. \\n CORRECT : \\\"Use BatchGetItem\\\" is the correct answer. \\n INCORRECT : \\\"Use GetItem and use a projection expression\\\" is incorrect as this will limit the attributes returned and will retrieve the items sequentially which results in more latency. \\n INCORRECT : \\\"Use a Scan operation with pagination\\\" is incorrect as a Scan operation is the least efficient way to retrieve the data as all items in the table are returned and then filtered. Pagination just breaks the results into pages. \\n INCORRECT : \\\"Use a Query operation with a FilterExpression\\\" is incorrect as this would limit the results that are returned. References: https://docs.aws.amazon.com/amazondynamodb/latest/APIReference/API_BatchGetItem.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-dynamodb/\"}",
    "{\"question\":\"A company needs a fully-managed source control service that will work in AWS. The service must ensure that revision control synchronizes multiple distributed repositories by exchanging sets of changes peer-to-peer. All users need to work productively even when not connected to a network. Which source control service should be used?\",\"choices\":[\"Subversion\",\"AWS CodeBuild\",\"AWS CodeCommit\",\"AWS CodeStar\"],\"answer\":\"AWS CodeCommit\",\"reason\":\"Explanation: AWS CodeCommit is a version control service hosted by Amazon Web Services that you can use to privately store and manage assets (such as documents, source code, and binary files) in the cloud. A repository is the fundamental version control object in CodeCommit. It's where you securely store code and files for your project. It also stores your project history, from the first commit through the latest changes. You can share your repository with other users so you can work together on a project. If you add AWS tags to repositories, you can set up notifications so that repository users receive email about events (for example, another user commenting on code). You can also change the default settings for your repository, browse its contents, and more. You can create triggers for your repository so that code pushes or other events trigger actions, such as emails or code functions. You can even configure a repository on your local computer (a local repo) to push your changes to more than one repository. \\n CORRECT : \\\"AWS CodeCommit\\\" is the correct answer. \\n INCORRECT : \\\"Subversion\\\" is incorrect as this is not a fully managed source control systemINCORRECT: \\\"AWS CodeBuild\\\" is incorrect as this is a service used for building and testing code. \\n INCORRECT : \\\"AWS CodeStar\\\" is incorrect as this is not a source control system; it integrates with source control systems such as CodeCommit. References: https://aws. amazon. com/codecommit/Save time with our AWS cheat sheets:https://digitalcloud.training/aws-developer-tools/\"}",
    "{\"question\":\"An organization needs to add encryption in-transit to an existing website running behind an Elastic Load Balancer. The website’s Amazon EC2 instances are CPU-constrained and therefore load on their CPUs should not be increased. What should be done to secure the website? (Select TWO. )\",\"choices\":[\"Configure an Elastic Load Balancer with SSL pass-through\",\"Configure SSL certificates on an Elastic Load Balancer\",\"Configure an Elastic Load Balancer with a KMS CMK\",\"Install SSL certificates on the EC2 instances\",\"Configure an Elastic Load Balancer with SSL termination\"],\"answer\":[\"Configure SSL certificates on an Elastic Load Balancer\",\"Configure an Elastic Load Balancer with SSL termination\"],\"reason\":\"Explanation: The company need to add security to their website by encrypting traffic in-transit using HTTPS. This requires adding SSL/TLS certificates to enable the encryption. The process of encrypting and decrypting data is CPU intensive and therefore the company need to avoid adding certificates to the EC2 instances as that will place further load on their CPUs. Therefore, the solution is to configure SSL certificates on the Elastic Load Balancer and then configure SSL termination. This can be done by adding a certificate to a HTTPS listener on the load balancer. \\n CORRECT : \\\"Configure SSL certificates on an Elastic Load Balancer\\\" is a correct answer. \\n CORRECT : \\\"Configure an Elastic Load Balancer with SSL termination\\\" is a correct answer. \\n INCORRECT : \\\"Configure an Elastic Load Balancer with SSL pass-through\\\" is incorrect as with pass-through the SSL session must be terminated on the EC2 instances which should be avoided as they are CPU-constrained. \\n INCORRECT : \\\"Configure an Elastic Load Balancer with a KMS CMK\\\" is incorrect as a KMS CMK is used to encrypt data at rest, it is not used for in-transit encryption. \\n INCORRECT : \\\"Install SSL certificates on the EC2 instances\\\" is incorrect as this would increase the load on the CPUsReferences:https://docs.aws.amazon.com/elasticloadbalancing/latest/application/create-https-listener.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-elastic-load-balancing-aws-elb/\"}",
    "{\"question\":\"An online multiplayer game employs Amazon API Gateway WebSocket APIs with an HTTP backend. The game developer needs to add a feature that identifies players with unstable connections who repeatedly join and leave the game. The developer also wants the ability to disconnect such players from the game. What two modifications should the developer implement in the game to fulfill these requirements? (Select TWO. )\",\"choices\":[\"Implement $connect and $disconnect routes in the backend service. \",\"Add logic to track the player's connection status using Amazon DynamoDB in the backend service. \",\"Switch to REST APIs in the backend service. \",\"Implement AWS Cognito for player authentication in the backend service. \",\"Switch to AWS App Runner for the backend service. \"],\"answer\":[\"Implement $connect and $disconnect routes in the backend service. \",\"Add logic to track the player's connection status using Amazon DynamoDB in the backend service. \"],\"reason\":\"Explanation: By implementing $connect and $disconnect routes in the backend service, you can capture events when a player connects and disconnects from the WebSocket API. This enables tracking and managing of players' connection statuses. Using Amazon DynamoDB (or another database service) allows you to persist and track the connection status of each player in real-time. This enables you to identify players who connect and disconnect frequently. \\n CORRECT : \\\"Implement $connect and $disconnect routes in the backend service\\\" is a correct answer (as explained above. )\\n CORRECT : \\\"Add logic to track the player's connection status using Amazon DynamoDB in the backend service\\\" is also a correct answer (as explained above. )\\n INCORRECT : \\\"Switch to REST APIs in the backend service\\\" is incorrect. Switching to REST APIs won't inherently provide the ability to manage or track unstable connections. WebSocket APIs are more suited for applications requiring real-time, two-way communication. \\n INCORRECT : \\\"Implement AWS Cognito for player authentication in the backend service\\\" is incorrect. While AWS Cognito provides authentication and user management, it doesn't offer features for tracking and managing unstable connections. \\n INCORRECT : \\\"Switch to AWS App Runner for the backend service\\\" is incorrect. AWS App Runner is a service that makes it easy to build, deploy, and scale containerized applications quickly, but it doesn't directly address the specific requirement of tracking and managing unstable connections. References: https://docs.aws.amazon.com/apigateway/latest/developerguide/apigateway-websocket-api-route-keys-connect-disconnect.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-api-gateway/\"}",
    "{\"question\":\"An application runs on Amazon EC2 and generates log files. A Developer needs to centralize the log files so they can be queried and retained. What is the EASIEST way for the Developer to centralize the log files?\",\"choices\":[\"Install the Amazon CloudWatch Logs agent and collect the logs from the instances\",\"Create a script that copies the log files to Amazon S3 and use a cron job to run the script on a recurring schedule\",\"Create a script that uses the AWS SDK to collect and send the log files to Amazon CloudWatch Logs\",\"Setup a CloudWatch Events rule to trigger an SNS topic when an application log file is generated\"],\"answer\":\"Install the Amazon CloudWatch Logs agent and collect the logs from the instances\",\"reason\":\"Explanation: You can use Amazon CloudWatch Logs to monitor, store, and access your log files from Amazon Elastic Compute Cloud (Amazon EC2) instances, AWS CloudTrail, Route 53, and other sources. CloudWatch Logs enables you to centralize the logs from all of your systems, applications, and AWS services that you use, in a single, highly scalable service. You can then easily view them, search them for specific error codes or patterns, filter them based on specific fields, or archive them securely for future analysis. To collect logs from Amazon EC2 and on-premises instances it is necessary to install an agent. There are two options: the unified CloudWatch Agent which collects logs and advanced metrics (such as memory usage), or the older CloudWatch Logs agent which only collects logs from Linux servers. \\n CORRECT : \\\"Install the Amazon CloudWatch Logs agent and collect the logs from the instances\\\" is the correct answer. \\n INCORRECT : \\\"Create a script that copies the log files to Amazon S3 and use a cron job to run the script on a recurring schedule\\\" is incorrect as the best place to move the log files to for querying and long term retention would be CloudWatch Logs. It is also easier to use the agent than to create and maintain a script. \\n INCORRECT : \\\"Create a script that uses the AWS SDK to collect and send the log files to Amazon CloudWatch Logs\\\" is incorrect as this is not the easiest way to achieve this outcome. It will be easier to use the CloudWatch Logs agent. \\n INCORRECT : \\\"Setup a CloudWatch Events rule to trigger an SNS topic when an application log file is generated\\\" is incorrect as CloudWatch Events does not collect log files, it monitors state changes in resources. References: https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/WhatIsCloudWatchLogs.html Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-cloudwatch/\"}",
    "{\"question\":\"An application is running on a fleet of EC2 instances running behind an Elastic Load Balancer (ELB). The EC2 instances session data in a shared Amazon S3 bucket. Security policy mandates that data must be encrypted in transit. How can the Developer ensure that all data that is sent to the S3 bucket is encrypted in transit?\",\"choices\":[\"Create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption\",\"Create an S3 bucket policy that denies traffic where SecureTransport is false\",\"Configure HTTP to HTTPS redirection on the Elastic Load Balancer\",\"Create an S3 bucket policy that denies traffic where SecureTransport is true\"],\"answer\":\"Create an S3 bucket policy that denies traffic where SecureTransport is false\",\"reason\":\"Explanation: At the Amazon S3 bucket level, you can configure permissions through a bucket policy. For example, you can limit access to the objects in a bucket by IP address range or specific IP addresses. Alternatively, you can make the objects accessible only through HTTPS. The following bucket policy allows access to Amazon S3 objects only through HTTPS (the policy was generated with the AWS Policy Generator). Here the bucket policy explicitly denies (\\\"Effect\\\": \\\"Deny\\\") all read access (\\\"Action\\\": \\\"s3:GetObject\\\") from anybody who browses (\\\"Principal\\\": \\\"*\\\") to Amazon S3 objects within an Amazon S3 bucket if they are not accessed through HTTPS (\\\"aws:SecureTransport\\\": \\\"false\\\"). \\n CORRECT : \\\"Create an S3 bucket policy that denies traffic where SecureTransport is false\\\" is the correct answer. \\n INCORRECT : \\\"Create an S3 bucket policy that denies traffic where SecureTransport is true\\\" is incorrect. This will not work as it is denying traffic that IS encrypted in transit. \\n INCORRECT : \\\"Create an S3 bucket policy that denies any S3 Put request that does not include the x-amz-server-side-encryption\\\" is incorrect. This will ensure that the data is encrypted at rest, but not in-transit. \\n INCORRECT : \\\"Configure HTTP to HTTPS redirection on the Elastic Load Balancer\\\" is incorrect. This will ensure the client traffic reaching the ELB is encrypted however we need to ensure the traffic from the EC2 instances to S3 is encrypted and the ELB is not involved in this communication. References: https://aws. amazon. com/blogs/security/how-to-use-bucket-policies-and-apply-defense-in-depth-to-help-secure-your-amazon-s3-data/Save time with our AWS cheat sheets:https://digitalcloud.training/amazon-s3-and-glacier/\"}",
    "{\"question\":\"A Developer has noticed some suspicious activity in her AWS account and is concerned that the access keys associated with her IAM user account may have been compromised. What is the first thing the Developer do in should do in this situation?\",\"choices\":[\"Delete her IAM user account\",\"Delete the compromised access keys\",\"Report the incident to AWS Support\",\"Change her IAM User account password\"],\"answer\":\"Delete the compromised access keys\",\"reason\":\"Explanation: In this case the Developer’s access keys may have been compromised so the first step would be to invalidate the access keys by deleting them. The next step would then be to determine if any temporary security credentials have been issued an invalidating those too to prevent any further misuse. The user account and user account password have not been compromised so they do not need to be deleted / changed as a first step. However, changing the account password would typically be recommended as a best practice in this situation. \\n CORRECT : \\\"Delete the compromised access keys\\\" is the correct answer. \\n INCORRECT : \\\"Delete her IAM user account\\\" is incorrect. This user account has not been compromised based on the available information, just the access keys. Deleting the access keys will prevent further misuse of the AWS account. \\n INCORRECT : \\\"Report the incident to AWS Support\\\" is incorrect is a good practice but not the first step. The Developer should first attempt to mitigate any further misuse of the account by deleting the access keys. \\n INCORRECT : \\\"Change her IAM User account password\\\" is incorrect as she does not have any evidence that the account has been compromised, just the access keys. However, it would be a good practice to change the password, just not the first thing to do. References: https://aws. amazon. com/blogs/security/what-to-do-if-you-inadvertently-expose-an-aws-access-key/Save time with our AWS cheat sheets:https://digitalcloud.training/aws-iam/\"}",
    "{\"question\":\"A Developer has joined a team and needs to connect to the AWS CodeCommit repository using SSH. What should the Developer do to configure access using Git?\",\"choices\":[\"On the Developer’s IAM account, under security credentials, choose to create HTTPS Git credentials for AWS CodeCommit\",\"On the Developer’s IAM account, under security credentials, choose to create an access key and secret ID\",\"Create an account on Github and user those login credentials to login to AWS CodeCommit\",\"Generate an SSH public and private key. Upload the public key to the Developer’s IAM account\"],\"answer\":\"Generate an SSH public and private key. Upload the public key to the Developer’s IAM account\",\"reason\":\"Explanation: You need to configure your Git client to communicate with CodeCommit repositories. As part of this configuration, you provide IAM credentials that CodeCommit can use to authenticate you. IAM supports CodeCommit with three types of credentials:Git credentials, an IAM -generated user name and password pair you can use to communicate with CodeCommit repositories over HTTPS. SSH keys, a locally generated public-private key pair that you can associate with your IAM user to communicate with CodeCommit repositories over SSH. AWS access keys, which you can use with the credential helper included with the AWS CLI to communicate with CodeCommit repositories over HTTPS. As the Developer is going to use SSH, he first needs to generate an SSH private and public key. These can then be used for authentication. The method of creating these depends on the operating system the Developer is using. Then, the Developer can upload the public key (by copying the contents of the file) into his IAM account under security credentials. \\n CORRECT : \\\"Generate an SSH public and private key. Upload the public key to the Developer’s IAM account\\\" is the correct answer. \\n INCORRECT : \\\"On the Developer’s IAM account, under security credentials, choose to create HTTPS Git credentials for AWS CodeCommit\\\" is incorrect as this method is used for creating credentials when you want to connect to CodeCommit using HTTPS. \\n INCORRECT : \\\"Create an account on Github and user those login credentials to login to AWS CodeCommit\\\" is incorrect as you cannot login to AWS CodeCommit using credentials from Github. \\n INCORRECT : \\\"On the Developer’s IAM account, under security credentials, choose to create an access key and secret ID\\\" is incorrect as though you can use access keys to authenticated to CodeCommit, this requires the credential helper, and enables access over HTTPS. References: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_ssh-keys.htmlhttps://docs.aws.amazon.com/codecommit/latest/userguide/setting-up-ssh-unixes.html Save time with our AWS cheat sheets:https://digitalcloud.training/aws-developer-tools/\"}"
]